{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import os\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from libs.lizi.my_magi import MyMagiModel\n",
    "from libs.lizi.my_magi.config import MagiConfig\n",
    "from libs.lizi.my_magi.utils import UnionFind\n",
    "from libs.lizi.my_magi.utils import read_image_as_np_array as read_image\n",
    "from numpy.typing import NDArray\n",
    "from PIL import Image\n",
    "from rich.pretty import pprint as pp\n",
    "from torchmetrics.functional.pairwise import pairwise_cosine_similarity\n",
    "from transformers.modeling_utils import load_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyMagiModel(\n",
       "  (crop_embedding_model): ViTMAEModel(\n",
       "    (embeddings): ViTMAEEmbeddings(\n",
       "      (patch_embeddings): ViTMAEPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "    )\n",
       "    (encoder): ViTMAEEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTMAELayer(\n",
       "          (attention): ViTMAESdpaAttention(\n",
       "            (attention): ViTMAESdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTMAESelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTMAEIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTMAEOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (detection_transformer): ConditionalDetrModel(\n",
       "    (backbone): ConditionalDetrConvModel(\n",
       "      (conv_encoder): ConditionalDetrConvEncoder(\n",
       "        (model): FeatureListNet(\n",
       "          (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "          (bn1): ConditionalDetrFrozenBatchNorm2d()\n",
       "          (act1): ReLU(inplace=True)\n",
       "          (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "          (layer1): Sequential(\n",
       "            (0): Bottleneck(\n",
       "              (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "              (downsample): Sequential(\n",
       "                (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): ConditionalDetrFrozenBatchNorm2d()\n",
       "              )\n",
       "            )\n",
       "            (1): Bottleneck(\n",
       "              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): Bottleneck(\n",
       "              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (layer2): Sequential(\n",
       "            (0): Bottleneck(\n",
       "              (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "              (bn2): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "              (downsample): Sequential(\n",
       "                (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                (1): ConditionalDetrFrozenBatchNorm2d()\n",
       "              )\n",
       "            )\n",
       "            (1): Bottleneck(\n",
       "              (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): Bottleneck(\n",
       "              (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (3): Bottleneck(\n",
       "              (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (layer3): Sequential(\n",
       "            (0): Bottleneck(\n",
       "              (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "              (bn2): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "              (downsample): Sequential(\n",
       "                (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                (1): ConditionalDetrFrozenBatchNorm2d()\n",
       "              )\n",
       "            )\n",
       "            (1): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (3): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (4): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (5): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (layer4): Sequential(\n",
       "            (0): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "              (bn2): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "              (downsample): Sequential(\n",
       "                (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                (1): ConditionalDetrFrozenBatchNorm2d()\n",
       "              )\n",
       "            )\n",
       "            (1): Bottleneck(\n",
       "              (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): Bottleneck(\n",
       "              (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (position_embedding): ConditionalDetrSinePositionEmbedding()\n",
       "    )\n",
       "    (input_projection): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (query_position_embeddings): Embedding(305, 256)\n",
       "    (encoder): ConditionalDetrEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x ConditionalDetrEncoderLayer(\n",
       "          (self_attn): DetrAttention(\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): ReLU()\n",
       "          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): ConditionalDetrDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): ConditionalDetrDecoderLayer(\n",
       "          (sa_qcontent_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (sa_qpos_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (sa_kcontent_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (sa_kpos_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (sa_v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (self_attn): ConditionalDetrAttention(\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (ca_qcontent_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (ca_qpos_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (ca_kcontent_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (ca_kpos_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (ca_v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (ca_qpos_sine_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (encoder_attn): ConditionalDetrAttention(\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1-5): 5 x ConditionalDetrDecoderLayer(\n",
       "          (sa_qcontent_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (sa_qpos_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (sa_kcontent_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (sa_kpos_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (sa_v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (self_attn): ConditionalDetrAttention(\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (ca_qcontent_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (ca_qpos_proj): None\n",
       "          (ca_kcontent_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (ca_kpos_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (ca_v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (ca_qpos_sine_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (encoder_attn): ConditionalDetrAttention(\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (query_scale): MLP(\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ref_point_head): MLP(\n",
       "        (layers): ModuleList(\n",
       "          (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (1): Linear(in_features=256, out_features=2, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (bbox_predictor): ConditionalDetrMLPPredictionHead(\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "      (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (character_character_matching_head): ConditionalDetrMLPPredictionHead(\n",
       "    (layers): ModuleList(\n",
       "      (0): Linear(in_features=2304, out_features=256, bias=True)\n",
       "      (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (2): Linear(in_features=256, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (class_labels_classifier): Linear(in_features=256, out_features=3, bias=True)\n",
       "  (matcher): ConditionalDetrHungarianMatcher()\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загрузка модели/Model initialization\n",
    "state_dict = load_state_dict(str(Path(\"models/magi/pytorch_model.bin\").resolve()))\n",
    "state_dict.keys()\n",
    "config: MagiConfig = MagiConfig.from_json_file(Path(\"libs/lizi/my_magi/config.json\").resolve())  # type: ignore\n",
    "model = MyMagiModel(config)\n",
    "model.load_state_dict(state_dict, strict=False)\n",
    "model.cuda() # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Все dataclasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Хранение картинки страницы, как numpy массива и его названия\n",
    "@dataclass\n",
    "class ImageInfo:\n",
    "    image: np.ndarray\n",
    "    full_file_name: str\n",
    "\n",
    "    def get_image_array(self):\n",
    "        return self.image\n",
    "\n",
    "# Для cropBox   \n",
    "@dataclass\n",
    "class CropBbox:\n",
    "    id_crop_bbox: int\n",
    "    image_bbox: np.ndarray # картиночка самого bboxa\n",
    "    character_score: float # не нужен, но есть\n",
    "    embeddings_for_batch: torch.Tensor # эмбеддинг для сравнения\n",
    "    crop_bboxes_for: torch.Tensor # 4 координаты\n",
    "    file_name: str # имя страницы исходной, иначе хрен сравним с раскраской\n",
    "\n",
    "# Тут картинка с примером раскраски и названием всего файла с картинкой для раскраски\n",
    "@dataclass\n",
    "class SampleImage:\n",
    "    # sample_image: int\n",
    "    sample_image: np.ndarray # картиночка \n",
    "    # character_score: float # не нужен, но есть\n",
    "    # embeddings_for_batch: torch.Tensor # эмбеддинг для сравнения\n",
    "    # crop_bboxes_for: torch.Tensor # 4 координаты\n",
    "    full_file_name: str # имя страницы исходной для подачи потом в раскраску\n",
    "\n",
    "# Уже преобразованная картинка с эмбеддингом для сравнения\n",
    "@dataclass\n",
    "class AnalisysSampleImage:\n",
    "    # id_sample_image: int\n",
    "    sample_image: np.ndarray # картиночка \n",
    "    # character_score: float # не нужен, но есть\n",
    "    embeddings_for_batch: torch.Tensor # эмбеддинг для сравнения\n",
    "    # crop_bboxes_for: torch.Tensor # 4 координаты\n",
    "    full_file_name: str # имя страницы исходной для подачи потом в раскраску\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SampleImageConnection:\n",
    "    # id_sample_image: int\n",
    "    crop_image_bbox: np.ndarray\n",
    "    crop_bboxes_coordinates: torch.Tensor # 4 координаты\n",
    "    file_page_name: str # имя страницы исходной, иначе хрен сравним с раскраской\n",
    "    \n",
    "    sample_image: np.ndarray # картиночка с примером раскраски\n",
    "    full_sample_file_name: str # имя страницы исходной для подачи потом в раскраску"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Говнокод"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = \"data/x_manga\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Uploading images\n",
    "# Функция для чтания файлов из папки с целой главой\n",
    "def ubpload_pages(directory_path_uploading: str) -> list[ImageInfo]:\n",
    "    pages_images = []\n",
    "    for filename in os.listdir(directory_path_uploading):\n",
    "        if filename.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "            full_path = os.path.join(directory_path_uploading, filename)\n",
    "            try:\n",
    "                img = np.asarray(Image.open(full_path).convert(\"RGB\"))\n",
    "                pages_images.append(ImageInfo(image=img, full_file_name=full_path))\n",
    "            except Exception as e:\n",
    "                print(f\"Error when opening {full_path}: {e}\")\n",
    "        else:\n",
    "            print(f\"Incorrect file extension {directory_path_uploading}\")\n",
    "    return pages_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# получение списка с np.ndarray, где каждый элемент - экземпляр класса ImageInfo,\n",
    "# где лежит название файла страницы и сама страница в виде нампай массива\n",
    "my_pages = ubpload_pages(directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get embeddings\n",
    "def get_embeddings(images_pages: list[ImageInfo]) -> tuple[list[CropBbox], list[torch.Tensor]]:\n",
    "    images_for_everything = []\n",
    "    list_of_embedddings = []\n",
    "    id_number = 0\n",
    "    for batch in images_pages:\n",
    "        with torch.no_grad():\n",
    "            page_image = [batch.image]\n",
    "            page_name = batch.full_file_name\n",
    "\n",
    "            (\n",
    "                batch_crop_bboxes,\n",
    "                batch_crop_embeddings_for_batch,\n",
    "                batch_image_bboxes,\n",
    "                batch_character_scores,\n",
    "            ) = model.get_crops_and_embeddings(page_image)\n",
    "\n",
    "        num_rows = len(batch_crop_embeddings_for_batch[0])\n",
    "\n",
    "        for i in range(num_rows):\n",
    "            images_for_everything.append(\n",
    "                CropBbox(\n",
    "                    id_crop_bbox=i + id_number,\n",
    "                    image_bbox=batch_image_bboxes[0][i],\n",
    "                    character_score=batch_character_scores[0][i],\n",
    "                    embeddings_for_batch=batch_crop_embeddings_for_batch[0][i],\n",
    "                    crop_bboxes_for=batch_crop_bboxes[0][i],\n",
    "                    file_name=page_name,\n",
    "                )\n",
    "            )\n",
    "            list_of_embedddings.append(batch_crop_embeddings_for_batch[0][i])\n",
    "\n",
    "        id_number = id_number + num_rows\n",
    "    return images_for_everything, list_of_embedddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_bboxes, original_emb = get_embeddings(my_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(original_emb[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Разделим по 100 bboxes\n",
    "# Тут подсписки длиной в 100 экземпляров BBox\n",
    "# sublists_for_clasterization = []\n",
    "# Тут подсписки длиной в 100 тензоров каждый - тензоры для cosine_simularity\n",
    "def prepreparing_embeddings(list_of_embedddings: list[torch.Tensor]) -> list[torch.Tensor]:\n",
    "    sublists_for_embedddings = []\n",
    "    for i in range(0, len(list_of_embedddings), 100):\n",
    "        # sublist_for_bbox = images_bw_for_everything[i:i+100]\n",
    "        sublist_for_embedddings = list_of_embedddings[i : i + 100]\n",
    "        # sublists_for_clasterization.append(sublist_for_bbox)\n",
    "        sublists_for_embedddings.append(sublist_for_embedddings)\n",
    "\n",
    "    # Тут просто в тензоры объединим\n",
    "    list_for_analysis = []\n",
    "    for one_list in sublists_for_embedddings:\n",
    "        crop_embedddings = None\n",
    "\n",
    "        # Проходим по каждому элементу в подмножестве\n",
    "        for i in range(len(one_list)):\n",
    "            # Извлекаем эмбеддинги из текущего элемента\n",
    "            current_embeddings = one_list[i].unsqueeze(dim=0)\n",
    "\n",
    "            # Если crop_embeds еще не инициализирована, инициализируем ее текущими эмбеддингами\n",
    "            if crop_embedddings is None:\n",
    "                crop_embedddings = current_embeddings\n",
    "            else:\n",
    "                # Иначе объединяем текущие эмбеддинги с предыдущими\n",
    "                crop_embedddings = torch.cat((crop_embedddings, current_embeddings), dim=0)\n",
    "\n",
    "        list_for_analysis.append(crop_embedddings)\n",
    "    return list_for_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_original_embeddings = prepreparing_embeddings(original_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([86, 768])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(list_original_embeddings[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Проба с максимумом по главе\n",
    "# Список для поиска совпадений с раскрашенными персами\n",
    "def original_bboxes_compare(list_for_analysis: list[torch.Tensor]) -> list[torch.Tensor]:\n",
    "    compare_list = []\n",
    "    # Перебираем все подмножества в sublists_for_embedddings\n",
    "    for one_pack_for_analysis in list_for_analysis:\n",
    "        # Матрица с косинусными совпадениями все-на-все\n",
    "        pcs = pairwise_cosine_similarity(one_pack_for_analysis, one_pack_for_analysis)\n",
    "        # Меняем единицы в главной диагонали на нули\n",
    "        pcs = pcs.fill_diagonal_(0.0)\n",
    "        # Получаем индексы всех максимумов - лучшие совпадения\n",
    "        new_var = torch.argmax(pcs, dim=1)\n",
    "        # Объединяем индексы лучших совпадений друг с другом попарно\n",
    "        char_to = torch.cat(\n",
    "            (new_var.unsqueeze(1), torch.arange(len(new_var)).cuda().unsqueeze(1)), dim=1\n",
    "        )\n",
    "        # Делаем граф из совпадающих вершин\n",
    "        graphs_chapter_one_max = nx.Graph(char_to.tolist())\n",
    "        # Объединяем все совпавшие вершины друг с другом\n",
    "        indixes_per_chapter = [list(c_) for c_ in nx.connected_components(graphs_chapter_one_max)]\n",
    "        # Создаём список compare_list и добавляем внутри него тензоры\n",
    "        for c_k in indixes_per_chapter:\n",
    "            for character_index in range(len(c_k)):\n",
    "                num = int(c_k[character_index])\n",
    "                if character_index == 0:\n",
    "                    first_compare_batch = one_pack_for_analysis[num].unsqueeze(dim=0)\n",
    "                else:\n",
    "                    first_compare_batch = torch.cat(\n",
    "                        (first_compare_batch, one_pack_for_analysis[num].unsqueeze(dim=0)), dim=0\n",
    "                    )\n",
    "            compare_list.append(first_compare_batch)\n",
    "    return compare_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_comp_list = original_bboxes_compare(list_original_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Поиск samples\n",
    "directory_path_samples = \"data/ex_samples\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_img(\n",
    "    directory_path_samples: str,\n",
    ") -> tuple[torch.Tensor, list[AnalisysSampleImage]]:\n",
    "    # images_samples - список, содержащий в себе примеры для окраски в виде numpy arrays\n",
    "    images_samples = []\n",
    "    # Заглушка для чтения файлов из папки с примерами раскраски\n",
    "    for filename in os.listdir(directory_path_samples):\n",
    "        full_path = os.path.join(directory_path_samples, filename)\n",
    "        try:\n",
    "            img = np.asarray(Image.open(full_path).convert(\"RGB\"))\n",
    "            images_samples.append(SampleImage(sample_image=img, full_file_name=full_path))\n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка при открытии {full_path}: {e}\")\n",
    "\n",
    "    images_color_for_analysis = []\n",
    "    list_of_compare_embeddings = []\n",
    "\n",
    "    # записываем все картинки для раскраски в такую же структуру, что и bboxes\n",
    "    for batch in images_samples:\n",
    "        with torch.no_grad():\n",
    "            page_image = [batch.sample_image]\n",
    "            page_name = batch.full_file_name\n",
    "\n",
    "            (\n",
    "                batch_crop_bboxes,\n",
    "                batch_crop_embeddings_for_batch,\n",
    "                batch_image_bboxes,\n",
    "                batch_character_scores,\n",
    "            ) = model.get_crops_and_embeddings(page_image)\n",
    "\n",
    "        num_rows = len(batch_crop_embeddings_for_batch[0])\n",
    "\n",
    "        for i in range(num_rows):\n",
    "            images_color_for_analysis.append(\n",
    "                AnalisysSampleImage(\n",
    "                    sample_image=batch_image_bboxes[0][i],\n",
    "                    embeddings_for_batch=batch_crop_embeddings_for_batch[0][i],\n",
    "                    full_file_name=page_name,\n",
    "                )\n",
    "            )\n",
    "            list_of_compare_embeddings.append(batch_crop_embeddings_for_batch[0][i])\n",
    "\n",
    "    # Обрабатываем список с эмбеддингами - делаем из них единый тензор\n",
    "    crop_embedddings_sample = None\n",
    "    # Проходим по каждому элементу в подмножестве\n",
    "    for i in range(len(list_of_compare_embeddings)):\n",
    "        # Извлекаем эмбеддинги из текущего элемента\n",
    "        current_embeddings = list_of_compare_embeddings[i].unsqueeze(dim=0)\n",
    "\n",
    "        # Если crop_embeds_bw еще не инициализирована, инициализируем ее текущими эмбеддингами\n",
    "        if crop_embedddings_sample is None:\n",
    "            crop_embedddings_sample = current_embeddings\n",
    "        else:\n",
    "            # Иначе объединяем текущие эмбеддинги с предыдущими\n",
    "            crop_embedddings_sample = torch.cat(\n",
    "                (crop_embedddings_sample, current_embeddings), dim=0\n",
    "            )\n",
    "    return crop_embedddings_sample, images_color_for_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_crop_embedddings_sample, my_images_color_for_analysis = sample_img(directory_path_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/ex_samples/0.png\n"
     ]
    }
   ],
   "source": [
    "print(my_images_color_for_analysis[0].full_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Сопоставляем эмбеддинги цветных примеров и эмбеддинги bboxes\n",
    "def finding_samples(\n",
    "    crop_embedddings_sample: torch.Tensor, compare_list: list[torch.Tensor]\n",
    ") -> tuple[dict, dict]:\n",
    "    # result_dict - тут будет цифра, которая привязана только к sample_dict и подходящие эмбеддинги среди нераскрашенных кропов\n",
    "    result_dict = {}\n",
    "    # sample_dict - тут будет цифра, которая привязана связана с result_dict и эмбеддинг раскраски\n",
    "    sample_dict = {}\n",
    "\n",
    "    for one_tensor in compare_list:\n",
    "        # Составляем матрицу и сравниваем с самплами\n",
    "        pcs_samples = pairwise_cosine_similarity(one_tensor, crop_embedddings_sample)\n",
    "        # Ищем сумму по всем значениям\n",
    "        comp = torch.sum(pcs_samples, dim=0)\n",
    "        # Ищем индекс максимального совпадения\n",
    "        max_coincidence = int(torch.argmax(comp))\n",
    "        if result_dict.get(max_coincidence) is not None:\n",
    "            inter_res = torch.cat((result_dict[max_coincidence], one_tensor), dim=0)\n",
    "            result_dict[max_coincidence] = inter_res\n",
    "        else:\n",
    "            result_dict[max_coincidence] = one_tensor\n",
    "            # Запись под аналогичным номером эмбеддинга с примером раскраски\n",
    "            sample_dict[max_coincidence] = crop_embedddings_sample[max_coincidence]\n",
    "    return result_dict, sample_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_result_dict, my_sample_dict = finding_samples(my_crop_embedddings_sample, my_comp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(my_sample_dict[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(my_images_color_for_analysis[0].embeddings_for_batch.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2.9781e-01,  1.9670e-02, -1.7568e-01,  2.9379e-02, -4.2561e-02,\n",
       "        -9.6347e-01, -2.2351e-01, -3.4887e-01,  6.6167e-02,  2.2741e-01,\n",
       "         1.0703e-01, -3.2303e-02, -2.2454e-01,  3.6913e-02,  2.0199e-01,\n",
       "        -1.2524e-01,  5.2243e-02,  1.0523e-01, -1.1381e-01,  3.1429e-01,\n",
       "        -1.2751e-02,  1.0706e-01,  1.0423e-02,  1.9518e-01, -7.7103e-02,\n",
       "         1.4773e-01, -1.1976e-02, -1.3550e-02, -9.4123e-02, -2.7921e-02,\n",
       "        -3.9593e-01, -2.0264e-02, -7.1006e-02, -3.2286e-01, -1.6692e-01,\n",
       "         1.2103e-01,  5.8713e-02, -4.8541e-03, -7.1120e-02,  2.1507e-02,\n",
       "        -2.8327e-03, -1.1897e-02,  1.0950e-01,  2.4325e-02, -1.0657e-01,\n",
       "        -9.3130e-02,  1.3887e-01,  1.0261e-02, -1.2351e-01, -1.4700e-02,\n",
       "        -5.4408e-02,  1.3477e-02,  1.9875e-01, -2.2320e-02, -3.5938e-01,\n",
       "        -1.3636e-01,  7.1938e-02, -1.5723e-01, -7.5227e-01,  3.0485e-02,\n",
       "         8.0218e-02, -9.4945e-02,  2.3611e-02,  2.1705e-01, -1.0553e-01,\n",
       "         3.3739e-01,  5.6018e-02, -1.8904e-01,  1.1658e-01, -6.4142e-02,\n",
       "         7.8278e-02, -3.2701e-01,  8.1384e-02, -5.4913e-02, -1.9575e-01,\n",
       "        -1.2541e-01,  1.2935e-01,  1.9757e-01,  7.8093e-02, -7.5413e-02,\n",
       "         6.1149e-02,  2.0752e-02,  1.8560e-01, -1.3570e-01, -6.0092e-02,\n",
       "         6.6736e-02, -4.2078e-02,  1.0254e-01, -1.5251e-01,  2.0694e-02,\n",
       "         1.3565e-01, -9.3538e-02, -4.7955e-03, -1.6013e-02, -9.2001e-02,\n",
       "        -3.0840e-02, -1.2497e-01,  2.4146e-01, -1.5254e-01, -6.7673e-02,\n",
       "        -5.4034e-02, -1.4135e-01,  2.0874e-02,  3.7579e-01,  1.3318e-02,\n",
       "         1.1744e-01,  3.1412e-02,  1.8184e-01,  2.3486e-01, -1.7023e-01,\n",
       "         7.6227e-02, -3.9206e-02, -1.7097e-02,  1.2779e-01,  2.2205e-02,\n",
       "        -8.7323e-02, -1.4524e-01,  1.7743e-01, -8.9305e-02,  9.3465e-02,\n",
       "        -6.1310e-02, -4.4888e-02,  6.8407e-02,  8.7059e-02,  1.5729e-02,\n",
       "         5.2197e-02,  6.1053e-02, -1.0818e-01, -6.7967e-03, -1.0319e-01,\n",
       "         1.7213e-01,  1.1551e-01, -4.1758e-02,  2.4004e-02,  3.8220e-01,\n",
       "         1.4005e-01, -1.5065e-01,  1.4950e-01, -1.5141e-01,  1.1792e-01,\n",
       "         9.4235e-03, -1.9399e-01, -1.1350e-01, -4.9168e-02, -6.8575e-02,\n",
       "        -7.6312e-02, -1.0376e-01,  1.5529e-01, -1.5536e-01, -5.3969e-02,\n",
       "         1.0169e-01, -1.0797e-01, -1.8513e-02,  7.8461e-03,  3.3708e-02,\n",
       "        -5.5285e-02, -5.2419e-02,  1.0186e-01, -1.2062e-01,  4.1175e-02,\n",
       "        -9.1699e-02,  5.2497e-02, -1.3346e-01,  7.3877e-02, -1.3783e-01,\n",
       "         1.4057e-02, -6.6085e-02,  7.2853e-02, -1.4027e-02, -1.9685e-01,\n",
       "         1.3196e-01, -4.7361e-02,  1.9948e-02, -3.3814e-02,  1.0121e-02,\n",
       "         6.7517e-02,  1.8208e-01,  1.8710e-02, -1.5217e-02,  2.1875e-02,\n",
       "         1.0632e-01, -6.9022e-02, -1.3080e-01, -1.3064e-01,  7.2362e-02,\n",
       "         9.0285e-03,  3.0959e-01, -1.0287e-01,  3.8219e-01, -5.3967e-02,\n",
       "         1.3022e-01,  6.3776e-02,  2.7237e-01, -1.3864e-01, -4.1272e-01,\n",
       "         2.8994e-01, -5.4633e-01,  1.8528e-02, -1.5795e-01, -1.4179e-02,\n",
       "         1.0722e-01, -1.9826e-01,  2.1249e-01,  3.6491e-01, -1.1834e-01,\n",
       "        -2.1462e-01, -2.8246e-03, -1.7024e-01, -1.4224e-01,  1.2614e-01,\n",
       "         5.0821e-02,  1.5738e-01,  1.0428e-02,  3.8322e-02,  5.6417e-02,\n",
       "         6.2738e-03,  2.8616e-01,  2.8225e-01,  3.1476e-01, -2.4001e-02,\n",
       "        -1.8071e-01, -6.5578e-02, -9.2483e-02, -6.7356e-01, -8.9876e-02,\n",
       "        -2.1037e-01,  1.0514e-01,  5.9424e-02, -2.8740e-01, -3.9327e-02,\n",
       "        -6.6005e-02,  4.4670e-02, -7.9911e-02,  2.0431e-02, -5.4931e-02,\n",
       "        -1.5002e-01,  2.5551e-02, -2.0962e-02, -8.9942e-02, -3.1122e-01,\n",
       "         7.5836e-02, -2.5685e-02,  7.5882e-02, -2.0033e-01, -3.9265e-01,\n",
       "         7.7323e-02, -1.3177e-01,  4.0725e-02, -2.3450e-02, -1.9439e-01,\n",
       "         1.0696e-01, -5.2330e-02,  9.4092e-02,  6.6111e-02, -4.0100e-01,\n",
       "        -8.3552e-02,  5.6175e-02, -1.4645e-02, -1.9737e-01, -1.2085e-01,\n",
       "         5.0604e-01,  1.3711e-02,  2.8074e-01,  4.0030e-03,  3.7429e-02,\n",
       "         1.0347e-01, -7.8046e-02,  2.3143e-01, -8.7881e-02,  9.5312e-03,\n",
       "         1.4821e-01, -1.4172e-02, -6.4962e-01,  2.1166e-01,  1.0750e+00,\n",
       "        -1.7109e-01, -5.3397e-02,  1.6576e-01,  9.1181e-02, -1.3594e-02,\n",
       "         4.2716e-02,  9.0277e-02, -4.5145e-02,  5.4306e-02,  9.3947e-03,\n",
       "        -7.1404e-02, -2.2510e-01,  1.6645e-01,  7.1019e-02, -5.3695e-02,\n",
       "        -2.0698e-01,  7.4202e-02, -1.1589e-02, -9.7655e-03,  5.1906e-02,\n",
       "         2.8443e-01,  1.3301e-02,  2.6748e-01,  3.6933e-02,  1.0087e-01,\n",
       "        -1.2398e-01,  6.0761e-02, -2.9863e-02, -2.4158e-01,  1.9199e+00,\n",
       "        -5.0401e-02,  1.3358e-01, -4.1560e-01, -7.0065e-02,  1.1677e-02,\n",
       "         1.3184e-01,  3.9719e-02, -1.4992e-01, -2.8895e-02, -2.4523e-01,\n",
       "         7.4626e-02, -1.4466e-01,  1.0058e-01, -2.2438e-01, -2.7126e-02,\n",
       "         2.9899e-02, -3.7201e-01, -4.0477e-02, -6.4577e-02,  2.5632e-01,\n",
       "         1.2254e-03, -1.1546e-01,  1.2106e-01, -3.3440e-01, -2.4247e-01,\n",
       "         1.3504e-01,  3.5405e-01, -4.6707e-02, -1.3666e-01, -4.0071e-01,\n",
       "        -1.2355e-01,  4.0579e-02, -1.4027e-01, -2.5785e-01,  8.3174e-02,\n",
       "         6.4711e-02,  5.9991e-02,  3.9828e-02,  8.2532e-02, -5.0310e-01,\n",
       "         4.8933e-02,  2.3037e-02, -1.5729e-01,  1.1600e-01,  1.3577e-01,\n",
       "         1.4724e-01,  1.4550e-01,  8.3444e-03, -1.9623e-01,  1.3465e-01,\n",
       "         2.4636e-01,  1.0147e-01,  2.0289e-01,  1.7630e-03, -9.0455e-02,\n",
       "         4.3797e-02,  8.0015e-02, -8.9316e-02, -2.1146e-01, -2.3095e-01,\n",
       "        -1.8619e-02, -3.6310e-02, -7.1620e-02, -1.2913e-01,  1.0134e-01,\n",
       "        -5.1486e-02,  5.8892e-02,  4.8023e-02, -4.6968e-01, -2.1055e-01,\n",
       "        -1.2351e-01,  3.5224e-01, -3.0464e-01, -4.9622e-02, -1.2619e-01,\n",
       "        -9.1092e-02,  1.1960e-01,  1.1298e-01, -2.9930e-01, -1.2172e-01,\n",
       "         1.6177e-01,  3.9466e-01, -3.1546e-01, -1.2718e-02, -5.7530e-02,\n",
       "         1.8922e-01,  1.3871e-01, -8.5662e-02, -1.4588e-02, -3.6566e-01,\n",
       "         5.8932e-02,  2.3679e-01,  1.7245e-01, -4.2930e-03,  9.8729e-02,\n",
       "        -1.3976e-01, -2.6684e-01, -4.0556e-01,  3.2977e-02, -3.8784e-02,\n",
       "        -1.3716e-01, -9.1601e-02,  6.2922e-02, -7.0833e-02, -2.7188e-02,\n",
       "         2.5659e-01, -1.6000e-02,  1.6520e-02, -9.4555e-03,  4.3538e-05,\n",
       "        -1.4764e-01,  1.8487e-01,  1.1588e-01,  2.0922e-01, -2.0763e-01,\n",
       "         8.9102e-02, -3.5354e-02,  9.5917e-02, -7.8242e-02,  1.1208e-02,\n",
       "         7.3703e-02,  9.5913e-02, -9.2264e-02,  1.6807e-01,  1.2461e-01,\n",
       "        -2.7698e-01,  1.7627e-01,  1.6608e-02,  1.4886e-01, -1.1920e-01,\n",
       "        -1.8031e-01,  4.9004e-02,  2.3993e-01,  2.8173e-01,  1.1931e-01,\n",
       "        -1.3457e-02,  2.5030e-02, -1.8534e-02,  1.4651e-02,  1.1753e-01,\n",
       "         1.2342e-01, -6.9079e-02, -9.5170e-02,  9.6026e-02,  1.5538e-01,\n",
       "        -1.6795e-01, -4.8532e-02,  1.6099e-01,  2.2339e-01, -3.5154e-02,\n",
       "         2.2115e-01, -3.3150e-02,  9.1526e-01,  7.3276e-02,  1.2402e-01,\n",
       "         3.1351e-02,  2.3805e-03,  4.1525e-02,  7.3525e-02, -5.4256e-02,\n",
       "        -2.0709e-02,  4.5047e-02,  1.5200e-02,  4.4738e-02,  9.7863e-02,\n",
       "         5.2112e-02,  2.7709e-01,  7.0480e-02, -1.5260e-02, -3.3441e-02,\n",
       "        -8.8564e-03,  2.2015e-01,  6.9270e-04, -6.0237e-02, -2.8453e-01,\n",
       "        -4.5015e-02,  7.2290e-02,  4.7439e-02, -1.9407e-01, -1.1025e-01,\n",
       "         2.5901e-01,  7.5677e-02,  4.8613e-01, -7.6533e-02, -7.1506e-01,\n",
       "        -1.3875e-01,  1.1912e-01, -9.0702e-02, -8.9753e-02, -2.1769e-01,\n",
       "        -5.7930e-02, -1.5781e-01, -4.5091e-02, -1.0710e-02, -1.3947e-01,\n",
       "        -7.0219e-02,  6.1290e-02, -1.6545e-01,  2.0259e-01,  6.2111e-02,\n",
       "         6.6623e-02, -2.2571e-01,  3.9555e-03, -7.5322e-02,  2.4550e-01,\n",
       "        -2.1807e-01,  1.5765e-01,  2.1717e-01,  6.8546e-02,  1.0271e-01,\n",
       "         1.8861e-02, -1.0770e-01, -2.9589e-03, -3.1045e-01,  1.2881e-01,\n",
       "        -1.2354e-01, -3.5150e-02,  1.6061e-01, -4.1891e-01,  1.6915e-01,\n",
       "        -1.8760e-01,  1.8972e-01, -5.3828e-02,  1.2983e-01, -7.7003e-01,\n",
       "        -4.9988e-02,  5.3612e-02, -1.9608e-03,  5.8846e-02,  3.4316e-02,\n",
       "        -4.2463e-02,  3.0498e-01, -6.1457e-02, -3.0684e-02,  6.3112e-02,\n",
       "         3.4436e-01,  1.0629e-02,  4.8991e-02, -4.0380e-02,  1.2695e-01,\n",
       "         8.8042e-02,  1.2083e-01,  1.2148e-01,  1.4080e-01,  3.7066e-02,\n",
       "        -2.0164e-01,  9.7313e-02, -2.4988e-01, -6.5589e-03,  2.6625e-02,\n",
       "        -1.0213e-01, -4.6101e-02, -2.5975e-02, -1.9060e-01,  1.1128e-01,\n",
       "         4.6130e-02,  1.1217e-01, -1.7374e-01,  2.4125e-01,  1.0055e-01,\n",
       "        -2.6038e-02, -7.4419e-02, -1.1529e-01,  1.3070e-01, -7.8721e-02,\n",
       "        -3.6182e-01,  8.6037e-02,  1.3447e-01, -1.6663e-01,  1.5940e-01,\n",
       "         7.5715e-02,  1.5823e-01, -8.8065e-02, -1.1202e-01,  2.4068e-01,\n",
       "        -2.3168e-01, -1.7443e-01,  8.7756e-02, -1.7646e-01,  1.6159e+00,\n",
       "        -5.7433e-02, -3.2570e-02,  4.2517e-02, -5.0241e-02,  7.8149e-02,\n",
       "        -8.1220e-03, -3.4966e-02,  5.1007e-02,  3.0983e-01, -2.0283e-01,\n",
       "         4.0816e-02, -4.6848e-02, -9.8464e-02,  1.2002e-01,  5.7488e-02,\n",
       "        -1.5624e-01, -9.5499e-02, -1.2366e-01,  6.0294e-02, -5.0070e-02,\n",
       "        -5.7962e-02, -8.9587e-01,  1.9182e-01,  1.2160e-01,  1.1038e-01,\n",
       "        -8.2092e-02,  3.4982e-01,  3.1448e-01,  4.6931e-02,  1.1765e-02,\n",
       "         1.6186e-01, -1.1561e-01,  4.7531e-02,  1.9296e-01,  9.9233e-02,\n",
       "         1.9548e-01,  1.7313e-01, -1.2777e-01, -2.8600e-02,  1.8285e-01,\n",
       "         3.4062e-02,  7.6070e-02, -2.8702e-01, -3.5323e-01,  3.8177e-02,\n",
       "         2.0824e-02, -6.8675e-02,  1.7090e-01, -3.0528e-02, -2.1232e-01,\n",
       "         1.4025e-01, -9.5854e-03,  1.0847e-01, -3.8213e-02, -1.1505e-01,\n",
       "        -2.5042e-01, -9.2616e-02, -9.9369e-02,  1.1787e-01, -3.5744e-01,\n",
       "         1.0939e-01, -6.8394e-02,  1.3549e-01, -7.3680e-02,  1.2380e-01,\n",
       "         2.2062e-01, -2.1690e-02, -3.8435e-02, -3.8089e-02, -5.3090e-02,\n",
       "         1.3307e-03, -1.1050e-01,  6.6734e-02,  3.4869e-02,  1.7362e-03,\n",
       "        -1.9837e-01, -6.6624e-02, -7.3809e-02, -6.8402e-03,  1.2305e-02,\n",
       "        -9.1941e-02, -3.4523e-01, -9.9771e-03,  9.5069e-02,  5.6681e-02,\n",
       "         1.6095e-01,  2.8910e-01, -5.7421e-02,  5.7890e-02, -1.9915e-01,\n",
       "         1.0059e-01,  1.7647e-02,  1.2297e-01, -1.2903e-01, -1.0209e-02,\n",
       "         2.2391e-01, -1.0690e-01,  8.4082e-02, -7.3145e-02,  3.6055e-02,\n",
       "         6.7417e-02,  1.9091e-01, -4.2553e-02, -2.0994e-01, -3.3854e-01,\n",
       "         1.9368e-01, -4.1967e-02,  1.2046e-03,  2.5710e-02,  1.6626e-01,\n",
       "        -9.3536e-02, -1.1401e-01, -6.5159e-02, -1.1364e-01,  1.1365e-01,\n",
       "         2.1404e-01,  1.2148e-01,  1.9704e-01,  1.1093e-02, -1.2459e-01,\n",
       "        -6.2049e-02,  8.3267e-02,  2.0524e-01, -2.3668e-01,  9.5162e-02,\n",
       "         1.3932e-01,  2.1022e-01, -2.4211e-01, -1.3885e-01,  1.0167e-01,\n",
       "         8.0389e-02,  6.6886e-02,  2.0966e-01,  3.0433e-02, -5.6871e-02,\n",
       "        -9.8485e-02,  8.1147e-02, -3.6136e-01, -8.9268e-02, -7.5514e-02,\n",
       "         7.4251e-02,  1.8535e-01, -4.8341e-02,  1.0311e-01,  1.6448e-02,\n",
       "        -9.9500e-02,  3.4811e-02, -3.6967e-03,  5.0428e-02,  3.7154e-02,\n",
       "        -8.3682e-02,  1.1195e-01, -1.8609e-01, -4.1268e-02,  4.2509e-02,\n",
       "         7.7675e-02, -2.0716e-01, -7.0059e-03,  8.5423e-02, -7.2517e-02,\n",
       "        -3.0655e-02,  4.4898e-02,  5.6275e-03, -4.3525e-03,  2.5646e-01,\n",
       "         1.8793e-02,  2.2449e-01, -1.2147e-01, -1.9079e-01,  1.3104e-01,\n",
       "         4.8518e-02,  5.4714e-03,  1.8927e-01,  8.7192e-02, -1.5185e-01,\n",
       "        -1.5147e-02, -1.9886e-01,  2.4377e-01, -1.4577e-01, -4.3536e-02,\n",
       "        -7.1703e-02, -2.5168e-03, -4.6212e-02], device='cuda:0')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_result_dict[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creating_pairs(\n",
    "    sample_list: list[AnalisysSampleImage],\n",
    "    sample_dict: dict,\n",
    "    original_list: list[CropBbox],\n",
    "    original_dict: dict,\n",
    ") -> list[SampleImageConnection]:\n",
    "    list_for_colorization = []\n",
    "\n",
    "    def compare_tensors(tensor1, tensor2):\n",
    "        return torch.allclose(tensor1, tensor2)\n",
    "\n",
    "    for key, tensor_value in sample_dict.items():\n",
    "        matching_object: AnalisysSampleImage | None = next(\n",
    "            (obj for obj in sample_list if compare_tensors(obj.embeddings_for_batch, tensor_value)),\n",
    "            None,\n",
    "        )\n",
    "        if matching_object:\n",
    "            picture_sample = matching_object.sample_image\n",
    "            picture_name = matching_object.full_file_name\n",
    "\n",
    "        for j in range(original_dict[key].shape[0]):\n",
    "            matching_original: CropBbox | None = next(\n",
    "                (\n",
    "                    obj\n",
    "                    for obj in original_list\n",
    "                    if compare_tensors(obj.embeddings_for_batch, original_dict[key][j])\n",
    "                ),\n",
    "                None,\n",
    "            )\n",
    "            if matching_original:\n",
    "                list_for_colorization.append(\n",
    "                    SampleImageConnection(\n",
    "                        crop_image_bbox=matching_original.image_bbox,\n",
    "                        crop_bboxes_coordinates=matching_original.crop_bboxes_for,\n",
    "                        file_page_name=matching_original.file_name,\n",
    "                        sample_image=picture_sample,\n",
    "                        full_sample_file_name=picture_name,\n",
    "                    )\n",
    "                )\n",
    "    return list_for_colorization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_character_list = creating_pairs(\n",
    "    my_images_color_for_analysis, my_sample_dict, list_of_bboxes, my_result_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Путь к файлу, куда вы хотите сохранить пустой JSON файл/ Надо чепм-то заменить\n",
    "file_path_json = \"data/json_files/Bleach/Vol74_Ch685.json\"\n",
    "# Путь к папке с файлами/Path to the folder with files\n",
    "directory_path = \"data/masi_mangas/Oshi no Ko/[Ai's fanclub] Vol. 7 Ch. 65\"\n",
    "\n",
    "manga_images = ubpload_pages(directory_path)\n",
    "images_classes, list_of_all_embedddings = get_embeddings(manga_images)\n",
    "prep_embeddings = prepreparing_embeddings(list_of_all_embedddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Uploading images\n",
    "# Первая версия\n",
    "# Путь к файлу, куда вы хотите сохранить пустой JSON файл/ Надо чепм-то заменить\n",
    "file_path_json = \"data/json_files/Bleach/Vol74_Ch685.json\"\n",
    "# Путь к папке с файлами/Path to the folder with files\n",
    "directory_path = \"data/masi_mangas/Oshi no Ko/[Ai's fanclub] Vol. 7 Ch. 65\"\n",
    "images_bw = []\n",
    "images_color = []\n",
    "\n",
    "# Заглушка для чтания файлов из папки с целой главой\n",
    "for filename in os.listdir(directory_path):\n",
    "    if filename.endswith(\"_bw.png\"):\n",
    "        full_path = os.path.join(directory_path, filename)\n",
    "        try:\n",
    "            img = np.asarray(Image.open(full_path).convert(\"RGB\"))\n",
    "            images_bw.append(ImageInfo(image=img, full_file_name=full_path))\n",
    "        except Exception as e:\n",
    "            print(f\"Error when opening {full_path}: {e}\")\n",
    "    elif filename.endswith(\"_color.png\"):\n",
    "        full_path = os.path.join(directory_path, filename)\n",
    "        try:\n",
    "            img = np.asarray(Image.open(full_path).convert(\"RGB\"))\n",
    "            images_color.append(ImageInfo(image=img, full_file_name=full_path))\n",
    "        except Exception as e:\n",
    "            print(f\"Error when opening {full_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**image_bboxes** - list со вложенными lists, вложенный список - одна страница, где каждый элемент - np.array\n",
    "**character_scores** - list с тензорами, тензор - одна страница, каждый элемент - для каждого bbox score\n",
    "**crop_embeddings_for_batch** - list с тензорами, где каждый тензор - страница, а строка - для каждого bbox\n",
    "**crop_bboxes** - list c тензорами, где каждый тензор - страница, а строка - для каждого bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get embeddings\n",
    "# Первая версия\n",
    "crop_bboxes_bw = []\n",
    "crop_embeddings_for_batch_bw = []\n",
    "image_bboxes_bw = []\n",
    "character_scores_bw = []\n",
    "\n",
    "\n",
    "images_bw_for_everything_765 = []\n",
    "\n",
    "\n",
    "list_of_embedddings = []\n",
    "\n",
    "id_number = 0\n",
    "\n",
    "for batch in images_bw:\n",
    "    with torch.no_grad():\n",
    "        page_image = [batch.image]\n",
    "        page_name = batch.full_file_name\n",
    "\n",
    "        (\n",
    "            batch_crop_bboxes,\n",
    "            batch_crop_embeddings_for_batch,\n",
    "            batch_image_bboxes,\n",
    "            batch_character_scores,\n",
    "        ) = model.get_crops_and_embeddings(page_image)\n",
    "\n",
    "    num_rows = len(batch_crop_embeddings_for_batch[0])\n",
    "\n",
    "    for i in range(num_rows):\n",
    "        images_bw_for_everything_765.append(\n",
    "            CropBbox(\n",
    "                id_crop_bbox=i+id_number,\n",
    "                image_bbox=batch_image_bboxes[0][i],\n",
    "                character_score=batch_character_scores[0][i],\n",
    "                embeddings_for_batch=batch_crop_embeddings_for_batch[0][i],\n",
    "                crop_bboxes_for=batch_crop_bboxes[0][i],\n",
    "                file_name=page_name,\n",
    "            )\n",
    "        )\n",
    "        list_of_embedddings.append(batch_crop_embeddings_for_batch[0][i])\n",
    "    \n",
    "    id_number = id_number + num_rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get embeddings\n",
    "\n",
    "\n",
    "def get_embeddings(images_pages): #-> list[CropBbox], list[np.ndarray]:\n",
    "    images_for_everything = []\n",
    "    list_of_embedddings = []\n",
    "    id_number = 0\n",
    "    for batch in images_pages:\n",
    "        with torch.no_grad():\n",
    "            page_image = [batch.image]\n",
    "            page_name = batch.full_file_name\n",
    "\n",
    "            (\n",
    "                batch_crop_bboxes,\n",
    "                batch_crop_embeddings_for_batch,\n",
    "                batch_image_bboxes,\n",
    "                batch_character_scores,\n",
    "            ) = model.get_crops_and_embeddings(page_image)\n",
    "\n",
    "        num_rows = len(batch_crop_embeddings_for_batch[0])\n",
    "\n",
    "        for i in range(num_rows):\n",
    "            images_for_everything.append(\n",
    "                CropBbox(\n",
    "                    id_crop_bbox=i + id_number,\n",
    "                    image_bbox=batch_image_bboxes[0][i],\n",
    "                    character_score=batch_character_scores[0][i],\n",
    "                    embeddings_for_batch=batch_crop_embeddings_for_batch[0][i],\n",
    "                    crop_bboxes_for=batch_crop_bboxes[0][i],\n",
    "                    file_name=page_name,\n",
    "                )\n",
    "            )\n",
    "            list_of_embedddings.append(batch_crop_embeddings_for_batch[0][i])\n",
    "\n",
    "        id_number = id_number + num_rows\n",
    "    return images_for_everything, list_of_embedddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Разделим по 100 bboxes\n",
    "# Первая версия\n",
    "# Тут подсписки длиной в 100 экземпляров BBox\n",
    "# sublists_for_clasterization = []\n",
    "# Тут подсписки длиной в 100 тензоров каждый - тензоры для cosine_simularity\n",
    "sublists_for_embedddings = []\n",
    "for i in range(0, len(list_of_embedddings), 100):\n",
    "    #sublist_for_bbox = images_bw_for_everything[i:i+100]\n",
    "    sublist_for_embedddings = list_of_embedddings[i:i+100]\n",
    "    #sublists_for_clasterization.append(sublist_for_bbox)\n",
    "    sublists_for_embedddings.append(sublist_for_embedddings)\n",
    "# Тут просто в тензоры объединим\n",
    "list_for_analysis = []\n",
    "for one_list in sublists_for_embedddings:\n",
    "    crop_embedddings_bw = None\n",
    "    \n",
    "    # Проходим по каждому элементу в подмножестве\n",
    "    for i in range(len(one_list)):\n",
    "        # Извлекаем эмбеддинги из текущего элемента\n",
    "        current_embeddings = one_list[i].unsqueeze(dim=0)\n",
    "        \n",
    "        # Если crop_embeds_bw еще не инициализирована, инициализируем ее текущими эмбеддингами\n",
    "        if crop_embedddings_bw is None:\n",
    "            crop_embedddings_bw = current_embeddings\n",
    "        else:\n",
    "            # Иначе объединяем текущие эмбеддинги с предыдущими\n",
    "            crop_embedddings_bw = torch.cat((crop_embedddings_bw, current_embeddings), dim=0)\n",
    "    \n",
    "    list_for_analysis.append(crop_embedddings_bw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Разделим по 100 bboxes\n",
    "# Тут подсписки длиной в 100 экземпляров BBox\n",
    "# sublists_for_clasterization = []\n",
    "# Тут подсписки длиной в 100 тензоров каждый - тензоры для cosine_simularity\n",
    "def prepreparing_embeddings(list_of_embedddings) -> list[torch.Tensor]:\n",
    "    sublists_for_embedddings = []\n",
    "    for i in range(0, len(list_of_embedddings), 100):\n",
    "        #sublist_for_bbox = images_bw_for_everything[i:i+100]\n",
    "        sublist_for_embedddings = list_of_embedddings[i:i+100]\n",
    "        #sublists_for_clasterization.append(sublist_for_bbox)\n",
    "        sublists_for_embedddings.append(sublist_for_embedddings)\n",
    "\n",
    "    # Тут просто в тензоры объединим\n",
    "    list_for_analysis = []\n",
    "    for one_list in sublists_for_embedddings:\n",
    "        crop_embedddings = None\n",
    "        \n",
    "        # Проходим по каждому элементу в подмножестве\n",
    "        for i in range(len(one_list)):\n",
    "            # Извлекаем эмбеддинги из текущего элемента\n",
    "            current_embeddings = one_list[i].unsqueeze(dim=0)\n",
    "            \n",
    "            # Если crop_embeds еще не инициализирована, инициализируем ее текущими эмбеддингами\n",
    "            if crop_embedddings is None:\n",
    "                crop_embedddings = current_embeddings\n",
    "            else:\n",
    "                # Иначе объединяем текущие эмбеддинги с предыдущими\n",
    "                crop_embedddings = torch.cat((crop_embedddings, current_embeddings), dim=0)\n",
    "        \n",
    "        list_for_analysis.append(crop_embedddings)\n",
    "    return list_for_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Проба с максимумом по главе\n",
    "# Список для поиска совпадений с раскрашенными персами\n",
    "compare_list = []\n",
    "# Перебираем все подмножества в sublists_for_embedddings\n",
    "for one_pack_for_analysis in list_for_analysis:\n",
    "\n",
    "    # Матрица с косинусными совпадениями все-на-все\n",
    "    pcs = pairwise_cosine_similarity(one_pack_for_analysis, one_pack_for_analysis)\n",
    "    # Меняем единицы в главной диагонали на нули\n",
    "    pcs = pcs.fill_diagonal_(0.0)\n",
    "    # Получаем индексы всех максимумов - лучшие совпадения\n",
    "    new_var = torch.argmax(pcs, dim=1)\n",
    "    # Объединяем индексы лучших совпадений друг с другом попарно\n",
    "    char_to = torch.cat(\n",
    "        (new_var.unsqueeze(1), torch.arange(len(new_var)).cuda().unsqueeze(1)), dim=1\n",
    "    )\n",
    "    # Делаем граф из совпадающих вершин\n",
    "    graphs_chapter_one_max = nx.Graph(char_to.tolist())\n",
    "    # Объединяем все совпавшие вершины друг с другом  \n",
    "    indixes_per_chapter = [\n",
    "        list(c_) for c_ in nx.connected_components(graphs_chapter_one_max)\n",
    "    ]\n",
    "    # Создаём список compare_list и добавляем внутри него тензоры\n",
    "    for c_k in indixes_per_chapter:\n",
    "        for character_index in range(len(c_k)):\n",
    "            num = int(c_k[character_index])\n",
    "            if character_index == 0:\n",
    "                first_compare_batch = one_pack_for_analysis[num].unsqueeze(dim=0)\n",
    "            else:\n",
    "                first_compare_batch = torch.cat(\n",
    "                    (first_compare_batch, one_pack_for_analysis[num].unsqueeze(dim=0)), dim=0\n",
    "                )\n",
    "        compare_list.append(first_compare_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Проба с максимумом по главе\n",
    "# Список для поиска совпадений с раскрашенными персами\n",
    "def sample_compare(list_for_analysis):\n",
    "    compare_list = []\n",
    "    # Перебираем все подмножества в sublists_for_embedddings\n",
    "    for one_pack_for_analysis in list_for_analysis:\n",
    "\n",
    "        # Матрица с косинусными совпадениями все-на-все\n",
    "        pcs = pairwise_cosine_similarity(one_pack_for_analysis, one_pack_for_analysis)\n",
    "        # Меняем единицы в главной диагонали на нули\n",
    "        pcs = pcs.fill_diagonal_(0.0)\n",
    "        # Получаем индексы всех максимумов - лучшие совпадения\n",
    "        new_var = torch.argmax(pcs, dim=1)\n",
    "        # Объединяем индексы лучших совпадений друг с другом попарно\n",
    "        char_to = torch.cat(\n",
    "            (new_var.unsqueeze(1), torch.arange(len(new_var)).cuda().unsqueeze(1)), dim=1\n",
    "        )\n",
    "        # Делаем граф из совпадающих вершин\n",
    "        graphs_chapter_one_max = nx.Graph(char_to.tolist())\n",
    "        # Объединяем все совпавшие вершины друг с другом  \n",
    "        indixes_per_chapter = [\n",
    "            list(c_) for c_ in nx.connected_components(graphs_chapter_one_max)\n",
    "        ]\n",
    "        # Создаём список compare_list и добавляем внутри него тензоры\n",
    "        for c_k in indixes_per_chapter:\n",
    "            for character_index in range(len(c_k)):\n",
    "                num = int(c_k[character_index])\n",
    "                if character_index == 0:\n",
    "                    first_compare_batch = one_pack_for_analysis[num].unsqueeze(dim=0)\n",
    "                else:\n",
    "                    first_compare_batch = torch.cat(\n",
    "                        (first_compare_batch, one_pack_for_analysis[num].unsqueeze(dim=0)), dim=0\n",
    "                    )\n",
    "            compare_list.append(first_compare_batch)\n",
    "    return compare_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Поиск samples\n",
    "directory_path_samples = \"data/samples/Bleach\"\n",
    "\n",
    "\n",
    "# images_samples - список, содержащий в себе примеры для окраски в виде numpy arrays\n",
    "images_samples = []\n",
    "# Заглушка для чтения файлов из папки с примерами раскраски\n",
    "for filename in os.listdir(directory_path_samples):\n",
    "    full_path = os.path.join(directory_path_samples, filename)\n",
    "    try:\n",
    "        img = np.asarray(Image.open(full_path).convert(\"RGB\"))\n",
    "        images_samples.append(SampleImage(sample_image=img, full_file_name=full_path))\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при открытии {full_path}: {e}\")\n",
    "\n",
    "type(images_samples)\n",
    "images_color_for_analysis = []\n",
    "list_of_compare_embeddings = []\n",
    "\n",
    "# записываем все картинки для раскраски в такую же структуру, что и bboxes\n",
    "for batch in images_samples:\n",
    "    with torch.no_grad():\n",
    "        page_image = [batch.sample_image]\n",
    "        page_name = batch.full_file_name\n",
    "\n",
    "        (\n",
    "            batch_crop_bboxes,\n",
    "            batch_crop_embeddings_for_batch,\n",
    "            batch_image_bboxes,\n",
    "            batch_character_scores,\n",
    "        ) = model.get_crops_and_embeddings(page_image)\n",
    "\n",
    "    num_rows = len(batch_crop_embeddings_for_batch[0])\n",
    "\n",
    "    for i in range(num_rows):\n",
    "        images_color_for_analysis.append(\n",
    "            AnalisysSampleImage(\n",
    "                #id_sample_image=i+id_number,\n",
    "                sample_image=batch_image_bboxes[0][i],\n",
    "                embeddings_for_batch=batch_crop_embeddings_for_batch[0][i],\n",
    "                full_file_name=page_name,\n",
    "            )\n",
    "        )\n",
    "        list_of_compare_embeddings.append(batch_crop_embeddings_for_batch[0][i])\n",
    "    \n",
    "    id_number = id_number + num_rows\n",
    "\n",
    "# Обрабатываем список с эмбеддингами - делаем из них единый тензор\n",
    "crop_embedddings_sample = None\n",
    "# Проходим по каждому элементу в подмножестве\n",
    "for i in range(len(list_of_compare_embeddings)):\n",
    "    # Извлекаем эмбеддинги из текущего элемента\n",
    "    current_embeddings = list_of_compare_embeddings[i].unsqueeze(dim=0)\n",
    "    \n",
    "    # Если crop_embeds_bw еще не инициализирована, инициализируем ее текущими эмбеддингами\n",
    "    if crop_embedddings_sample is None:\n",
    "        crop_embedddings_sample = current_embeddings\n",
    "    else:\n",
    "        # Иначе объединяем текущие эмбеддинги с предыдущими\n",
    "        crop_embedddings_sample = torch.cat((crop_embedddings_sample, current_embeddings), dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Поиск samples\n",
    "directory_path_samples = \"data/samples/Bleach\"\n",
    "\n",
    "def sample_img(directory_path_samples):\n",
    "# images_samples - список, содержащий в себе примеры для окраски в виде numpy arrays\n",
    "    images_samples = []\n",
    "    # Заглушка для чтения файлов из папки с примерами раскраски\n",
    "    for filename in os.listdir(directory_path_samples):\n",
    "        full_path = os.path.join(directory_path_samples, filename)\n",
    "        try:\n",
    "            img = np.asarray(Image.open(full_path).convert(\"RGB\"))\n",
    "            images_samples.append(SampleImage(sample_image=img, full_file_name=full_path))\n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка при открытии {full_path}: {e}\")\n",
    "\n",
    "    images_color_for_analysis = []\n",
    "    list_of_compare_embeddings = []\n",
    "\n",
    "    # записываем все картинки для раскраски в такую же структуру, что и bboxes\n",
    "    for batch in images_samples:\n",
    "        with torch.no_grad():\n",
    "            page_image = [batch.sample_image]\n",
    "            page_name = batch.full_file_name\n",
    "\n",
    "            (\n",
    "                batch_crop_bboxes,\n",
    "                batch_crop_embeddings_for_batch,\n",
    "                batch_image_bboxes,\n",
    "                batch_character_scores,\n",
    "            ) = model.get_crops_and_embeddings(page_image)\n",
    "\n",
    "        num_rows = len(batch_crop_embeddings_for_batch[0])\n",
    "\n",
    "        for i in range(num_rows):\n",
    "            images_color_for_analysis.append(\n",
    "                AnalisysSampleImage(\n",
    "                    sample_image=batch_image_bboxes[0][i],\n",
    "                    embeddings_for_batch=batch_crop_embeddings_for_batch[0][i],\n",
    "                    full_file_name=page_name,\n",
    "                )\n",
    "            )\n",
    "            list_of_compare_embeddings.append(batch_crop_embeddings_for_batch[0][i])\n",
    "        \n",
    "\n",
    "    # Обрабатываем список с эмбеддингами - делаем из них единый тензор\n",
    "    crop_embedddings_sample = None\n",
    "    # Проходим по каждому элементу в подмножестве\n",
    "    for i in range(len(list_of_compare_embeddings)):\n",
    "        # Извлекаем эмбеддинги из текущего элемента\n",
    "        current_embeddings = list_of_compare_embeddings[i].unsqueeze(dim=0)\n",
    "        \n",
    "        # Если crop_embeds_bw еще не инициализирована, инициализируем ее текущими эмбеддингами\n",
    "        if crop_embedddings_sample is None:\n",
    "            crop_embedddings_sample = current_embeddings\n",
    "        else:\n",
    "            # Иначе объединяем текущие эмбеддинги с предыдущими\n",
    "            crop_embedddings_sample = torch.cat((crop_embedddings_sample, current_embeddings), dim=0)\n",
    "    return crop_embedddings_sample, images_color_for_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Сопоставляем эмбеддинги цветных примеров и эмбеддинги bboxes\n",
    "# result_dict - тут будет цифра, которая привязана только к sample_dict и подходящие эмбеддинги среди нераскрашенных кропов\n",
    "result_dict = {}\n",
    "# sample_dict - тут будет цифра, которая привязана связана с result_dict и эмбеддинг раскраски\n",
    "sample_dict = {}\n",
    "\n",
    "for one_tensor in compare_list:\n",
    "    # Составляем матрицу и сравниваем с самплами\n",
    "    pcs_samples = pairwise_cosine_similarity(one_tensor, crop_embedddings_sample)\n",
    "    # Ищем сумму по всем значениям\n",
    "    comp = torch.sum(pcs_samples, dim=0)\n",
    "    # Ищем индекс максимального совпадения\n",
    "    max_coincidence = int(torch.argmax(comp))\n",
    "    if result_dict.get(max_coincidence) is not None:\n",
    "        inter_res = torch.cat(\n",
    "                (result_dict[max_coincidence], one_tensor), dim=0)\n",
    "        result_dict[max_coincidence] = inter_res\n",
    "    else:\n",
    "        result_dict[max_coincidence] = one_tensor\n",
    "        # Запись под аналогичным номером эмбеддинга с примером раскраски\n",
    "        sample_dict[max_coincidence] = crop_embedddings_sample[max_coincidence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Сопоставляем эмбеддинги цветных примеров и эмбеддинги bboxes\n",
    "def finding_samples(crop_embedddings_sample, compare_list):\n",
    "    # result_dict - тут будет цифра, которая привязана только к sample_dict и подходящие эмбеддинги среди нераскрашенных кропов\n",
    "    result_dict = {}\n",
    "    # sample_dict - тут будет цифра, которая привязана связана с result_dict и эмбеддинг раскраски\n",
    "    sample_dict = {}\n",
    "\n",
    "    for one_tensor in compare_list:\n",
    "        # Составляем матрицу и сравниваем с самплами\n",
    "        pcs_samples = pairwise_cosine_similarity(one_tensor, crop_embedddings_sample)\n",
    "        # Ищем сумму по всем значениям\n",
    "        comp = torch.sum(pcs_samples, dim=0)\n",
    "        # Ищем индекс максимального совпадения\n",
    "        max_coincidence = int(torch.argmax(comp))\n",
    "        if result_dict.get(max_coincidence) is not None:\n",
    "            inter_res = torch.cat(\n",
    "                    (result_dict[max_coincidence], one_tensor), dim=0)\n",
    "            result_dict[max_coincidence] = inter_res\n",
    "        else:\n",
    "            result_dict[max_coincidence] = one_tensor\n",
    "            # Запись под аналогичным номером эмбеддинга с примером раскраски\n",
    "            sample_dict[max_coincidence] = crop_embedddings_sample[max_coincidence]\n",
    "    return result_dict, sample_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creating_pairs(sample_list, sample_dict, original_list, original_dict):\n",
    "    list_for_colorization = []\n",
    "    for key, tensor_value in sample_dict.items():\n",
    "        matching_object: AnalisysSampleImage | None = next(\n",
    "            (obj for obj in sample_list if obj.embeddings_for_batch == tensor_value), None\n",
    "        )\n",
    "        if matching_object:\n",
    "            picture_sample = matching_object.sample_image\n",
    "            picture_name = matching_object.full_file_name\n",
    "\n",
    "        for j in range(original_dict[key].size()):\n",
    "            matching_original: CropBbox | None = next(\n",
    "                (obj for obj in original_list if obj.embeddings_for_batch == original_dict[key][j]),\n",
    "                None,\n",
    "            )\n",
    "            if matching_original:\n",
    "                list_for_colorization.append(\n",
    "                    SampleImageConnection(\n",
    "                        crop_image_bbox=matching_original.image_bbox,\n",
    "                        crop_bboxes_coordinates=matching_original.crop_bboxes_for,\n",
    "                        file_page_name=matching_original.file_name,\n",
    "                        sample_image=picture_sample,\n",
    "                        full_sample_file_name=picture_name,\n",
    "                    )\n",
    "                )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
