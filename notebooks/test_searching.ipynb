{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from libs.lizi.my_magi import MyMagiModel\n",
    "from libs.lizi.my_magi.config import MagiConfig\n",
    "from libs.lizi.my_magi.utils import UnionFind\n",
    "from libs.lizi.my_magi.utils import read_image_as_np_array as read_image\n",
    "from numpy.typing import NDArray\n",
    "from PIL import Image\n",
    "from rich.pretty import pprint as pp\n",
    "from torchmetrics.functional.pairwise import pairwise_cosine_similarity\n",
    "from transformers.modeling_utils import load_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = \"data/masi_mangas/Oshi no Ko/[Ai's fanclub] Vol. 3 Ch. 28\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_bw = []\n",
    "images_color = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ImageInfo:\n",
    "    image: np.ndarray\n",
    "    full_file_name: str\n",
    "    \n",
    "    def get_image_array(self):\n",
    "        return self.image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Заглушка для чтания файлов из папки с целой главой\n",
    "for filename in os.listdir(directory_path):\n",
    "    if filename.endswith(\"_bw.png\"):\n",
    "        full_path = os.path.join(directory_path, filename)\n",
    "        try:\n",
    "            img = np.asarray(Image.open(full_path).convert(\"RGB\"))\n",
    "            images_bw.append(ImageInfo(image=img, full_file_name=full_path))\n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка при открытии {full_path}: {e}\")\n",
    "    elif filename.endswith(\"_color.png\"):\n",
    "        full_path = os.path.join(directory_path, filename)\n",
    "        try:\n",
    "            img = np.asarray(Image.open(full_path).convert(\"RGB\"))\n",
    "            images_color.append(ImageInfo(image=img, full_file_name=full_path))\n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка при открытии {full_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp(images_bw[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пока не трогай ячейку\n",
    "group_size = 10\n",
    "images_bw_grouped = [images_bw[i : i + group_size] for i in range(0, len(images_bw), group_size)]\n",
    "images_color_grouped = [\n",
    "    images_color[i : i + group_size] for i in range(0, len(images_color), group_size)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(images_bw_grouped[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_bw_grouped[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружаем pre-train веса из файла в dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ocr_model.encoder.embeddings.cls_token', 'ocr_model.encoder.embeddings.position_embeddings', 'ocr_model.encoder.embeddings.patch_embeddings.projection.weight', 'ocr_model.encoder.embeddings.patch_embeddings.projection.bias', 'ocr_model.encoder.encoder.layer.0.attention.attention.query.weight', 'ocr_model.encoder.encoder.layer.0.attention.attention.key.weight', 'ocr_model.encoder.encoder.layer.0.attention.attention.value.weight', 'ocr_model.encoder.encoder.layer.0.attention.output.dense.weight', 'ocr_model.encoder.encoder.layer.0.attention.output.dense.bias', 'ocr_model.encoder.encoder.layer.0.intermediate.dense.weight', 'ocr_model.encoder.encoder.layer.0.intermediate.dense.bias', 'ocr_model.encoder.encoder.layer.0.output.dense.weight', 'ocr_model.encoder.encoder.layer.0.output.dense.bias', 'ocr_model.encoder.encoder.layer.0.layernorm_before.weight', 'ocr_model.encoder.encoder.layer.0.layernorm_before.bias', 'ocr_model.encoder.encoder.layer.0.layernorm_after.weight', 'ocr_model.encoder.encoder.layer.0.layernorm_after.bias', 'ocr_model.encoder.encoder.layer.1.attention.attention.query.weight', 'ocr_model.encoder.encoder.layer.1.attention.attention.key.weight', 'ocr_model.encoder.encoder.layer.1.attention.attention.value.weight', 'ocr_model.encoder.encoder.layer.1.attention.output.dense.weight', 'ocr_model.encoder.encoder.layer.1.attention.output.dense.bias', 'ocr_model.encoder.encoder.layer.1.intermediate.dense.weight', 'ocr_model.encoder.encoder.layer.1.intermediate.dense.bias', 'ocr_model.encoder.encoder.layer.1.output.dense.weight', 'ocr_model.encoder.encoder.layer.1.output.dense.bias', 'ocr_model.encoder.encoder.layer.1.layernorm_before.weight', 'ocr_model.encoder.encoder.layer.1.layernorm_before.bias', 'ocr_model.encoder.encoder.layer.1.layernorm_after.weight', 'ocr_model.encoder.encoder.layer.1.layernorm_after.bias', 'ocr_model.encoder.encoder.layer.2.attention.attention.query.weight', 'ocr_model.encoder.encoder.layer.2.attention.attention.key.weight', 'ocr_model.encoder.encoder.layer.2.attention.attention.value.weight', 'ocr_model.encoder.encoder.layer.2.attention.output.dense.weight', 'ocr_model.encoder.encoder.layer.2.attention.output.dense.bias', 'ocr_model.encoder.encoder.layer.2.intermediate.dense.weight', 'ocr_model.encoder.encoder.layer.2.intermediate.dense.bias', 'ocr_model.encoder.encoder.layer.2.output.dense.weight', 'ocr_model.encoder.encoder.layer.2.output.dense.bias', 'ocr_model.encoder.encoder.layer.2.layernorm_before.weight', 'ocr_model.encoder.encoder.layer.2.layernorm_before.bias', 'ocr_model.encoder.encoder.layer.2.layernorm_after.weight', 'ocr_model.encoder.encoder.layer.2.layernorm_after.bias', 'ocr_model.encoder.encoder.layer.3.attention.attention.query.weight', 'ocr_model.encoder.encoder.layer.3.attention.attention.key.weight', 'ocr_model.encoder.encoder.layer.3.attention.attention.value.weight', 'ocr_model.encoder.encoder.layer.3.attention.output.dense.weight', 'ocr_model.encoder.encoder.layer.3.attention.output.dense.bias', 'ocr_model.encoder.encoder.layer.3.intermediate.dense.weight', 'ocr_model.encoder.encoder.layer.3.intermediate.dense.bias', 'ocr_model.encoder.encoder.layer.3.output.dense.weight', 'ocr_model.encoder.encoder.layer.3.output.dense.bias', 'ocr_model.encoder.encoder.layer.3.layernorm_before.weight', 'ocr_model.encoder.encoder.layer.3.layernorm_before.bias', 'ocr_model.encoder.encoder.layer.3.layernorm_after.weight', 'ocr_model.encoder.encoder.layer.3.layernorm_after.bias', 'ocr_model.encoder.encoder.layer.4.attention.attention.query.weight', 'ocr_model.encoder.encoder.layer.4.attention.attention.key.weight', 'ocr_model.encoder.encoder.layer.4.attention.attention.value.weight', 'ocr_model.encoder.encoder.layer.4.attention.output.dense.weight', 'ocr_model.encoder.encoder.layer.4.attention.output.dense.bias', 'ocr_model.encoder.encoder.layer.4.intermediate.dense.weight', 'ocr_model.encoder.encoder.layer.4.intermediate.dense.bias', 'ocr_model.encoder.encoder.layer.4.output.dense.weight', 'ocr_model.encoder.encoder.layer.4.output.dense.bias', 'ocr_model.encoder.encoder.layer.4.layernorm_before.weight', 'ocr_model.encoder.encoder.layer.4.layernorm_before.bias', 'ocr_model.encoder.encoder.layer.4.layernorm_after.weight', 'ocr_model.encoder.encoder.layer.4.layernorm_after.bias', 'ocr_model.encoder.encoder.layer.5.attention.attention.query.weight', 'ocr_model.encoder.encoder.layer.5.attention.attention.key.weight', 'ocr_model.encoder.encoder.layer.5.attention.attention.value.weight', 'ocr_model.encoder.encoder.layer.5.attention.output.dense.weight', 'ocr_model.encoder.encoder.layer.5.attention.output.dense.bias', 'ocr_model.encoder.encoder.layer.5.intermediate.dense.weight', 'ocr_model.encoder.encoder.layer.5.intermediate.dense.bias', 'ocr_model.encoder.encoder.layer.5.output.dense.weight', 'ocr_model.encoder.encoder.layer.5.output.dense.bias', 'ocr_model.encoder.encoder.layer.5.layernorm_before.weight', 'ocr_model.encoder.encoder.layer.5.layernorm_before.bias', 'ocr_model.encoder.encoder.layer.5.layernorm_after.weight', 'ocr_model.encoder.encoder.layer.5.layernorm_after.bias', 'ocr_model.encoder.encoder.layer.6.attention.attention.query.weight', 'ocr_model.encoder.encoder.layer.6.attention.attention.key.weight', 'ocr_model.encoder.encoder.layer.6.attention.attention.value.weight', 'ocr_model.encoder.encoder.layer.6.attention.output.dense.weight', 'ocr_model.encoder.encoder.layer.6.attention.output.dense.bias', 'ocr_model.encoder.encoder.layer.6.intermediate.dense.weight', 'ocr_model.encoder.encoder.layer.6.intermediate.dense.bias', 'ocr_model.encoder.encoder.layer.6.output.dense.weight', 'ocr_model.encoder.encoder.layer.6.output.dense.bias', 'ocr_model.encoder.encoder.layer.6.layernorm_before.weight', 'ocr_model.encoder.encoder.layer.6.layernorm_before.bias', 'ocr_model.encoder.encoder.layer.6.layernorm_after.weight', 'ocr_model.encoder.encoder.layer.6.layernorm_after.bias', 'ocr_model.encoder.encoder.layer.7.attention.attention.query.weight', 'ocr_model.encoder.encoder.layer.7.attention.attention.key.weight', 'ocr_model.encoder.encoder.layer.7.attention.attention.value.weight', 'ocr_model.encoder.encoder.layer.7.attention.output.dense.weight', 'ocr_model.encoder.encoder.layer.7.attention.output.dense.bias', 'ocr_model.encoder.encoder.layer.7.intermediate.dense.weight', 'ocr_model.encoder.encoder.layer.7.intermediate.dense.bias', 'ocr_model.encoder.encoder.layer.7.output.dense.weight', 'ocr_model.encoder.encoder.layer.7.output.dense.bias', 'ocr_model.encoder.encoder.layer.7.layernorm_before.weight', 'ocr_model.encoder.encoder.layer.7.layernorm_before.bias', 'ocr_model.encoder.encoder.layer.7.layernorm_after.weight', 'ocr_model.encoder.encoder.layer.7.layernorm_after.bias', 'ocr_model.encoder.encoder.layer.8.attention.attention.query.weight', 'ocr_model.encoder.encoder.layer.8.attention.attention.key.weight', 'ocr_model.encoder.encoder.layer.8.attention.attention.value.weight', 'ocr_model.encoder.encoder.layer.8.attention.output.dense.weight', 'ocr_model.encoder.encoder.layer.8.attention.output.dense.bias', 'ocr_model.encoder.encoder.layer.8.intermediate.dense.weight', 'ocr_model.encoder.encoder.layer.8.intermediate.dense.bias', 'ocr_model.encoder.encoder.layer.8.output.dense.weight', 'ocr_model.encoder.encoder.layer.8.output.dense.bias', 'ocr_model.encoder.encoder.layer.8.layernorm_before.weight', 'ocr_model.encoder.encoder.layer.8.layernorm_before.bias', 'ocr_model.encoder.encoder.layer.8.layernorm_after.weight', 'ocr_model.encoder.encoder.layer.8.layernorm_after.bias', 'ocr_model.encoder.encoder.layer.9.attention.attention.query.weight', 'ocr_model.encoder.encoder.layer.9.attention.attention.key.weight', 'ocr_model.encoder.encoder.layer.9.attention.attention.value.weight', 'ocr_model.encoder.encoder.layer.9.attention.output.dense.weight', 'ocr_model.encoder.encoder.layer.9.attention.output.dense.bias', 'ocr_model.encoder.encoder.layer.9.intermediate.dense.weight', 'ocr_model.encoder.encoder.layer.9.intermediate.dense.bias', 'ocr_model.encoder.encoder.layer.9.output.dense.weight', 'ocr_model.encoder.encoder.layer.9.output.dense.bias', 'ocr_model.encoder.encoder.layer.9.layernorm_before.weight', 'ocr_model.encoder.encoder.layer.9.layernorm_before.bias', 'ocr_model.encoder.encoder.layer.9.layernorm_after.weight', 'ocr_model.encoder.encoder.layer.9.layernorm_after.bias', 'ocr_model.encoder.encoder.layer.10.attention.attention.query.weight', 'ocr_model.encoder.encoder.layer.10.attention.attention.key.weight', 'ocr_model.encoder.encoder.layer.10.attention.attention.value.weight', 'ocr_model.encoder.encoder.layer.10.attention.output.dense.weight', 'ocr_model.encoder.encoder.layer.10.attention.output.dense.bias', 'ocr_model.encoder.encoder.layer.10.intermediate.dense.weight', 'ocr_model.encoder.encoder.layer.10.intermediate.dense.bias', 'ocr_model.encoder.encoder.layer.10.output.dense.weight', 'ocr_model.encoder.encoder.layer.10.output.dense.bias', 'ocr_model.encoder.encoder.layer.10.layernorm_before.weight', 'ocr_model.encoder.encoder.layer.10.layernorm_before.bias', 'ocr_model.encoder.encoder.layer.10.layernorm_after.weight', 'ocr_model.encoder.encoder.layer.10.layernorm_after.bias', 'ocr_model.encoder.encoder.layer.11.attention.attention.query.weight', 'ocr_model.encoder.encoder.layer.11.attention.attention.key.weight', 'ocr_model.encoder.encoder.layer.11.attention.attention.value.weight', 'ocr_model.encoder.encoder.layer.11.attention.output.dense.weight', 'ocr_model.encoder.encoder.layer.11.attention.output.dense.bias', 'ocr_model.encoder.encoder.layer.11.intermediate.dense.weight', 'ocr_model.encoder.encoder.layer.11.intermediate.dense.bias', 'ocr_model.encoder.encoder.layer.11.output.dense.weight', 'ocr_model.encoder.encoder.layer.11.output.dense.bias', 'ocr_model.encoder.encoder.layer.11.layernorm_before.weight', 'ocr_model.encoder.encoder.layer.11.layernorm_before.bias', 'ocr_model.encoder.encoder.layer.11.layernorm_after.weight', 'ocr_model.encoder.encoder.layer.11.layernorm_after.bias', 'ocr_model.encoder.layernorm.weight', 'ocr_model.encoder.layernorm.bias', 'ocr_model.encoder.pooler.dense.weight', 'ocr_model.encoder.pooler.dense.bias', 'ocr_model.decoder.model.decoder.embed_tokens.weight', 'ocr_model.decoder.model.decoder.embed_positions.weight', 'ocr_model.decoder.model.decoder.layernorm_embedding.weight', 'ocr_model.decoder.model.decoder.layernorm_embedding.bias', 'ocr_model.decoder.model.decoder.layers.0.self_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.0.self_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.0.self_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.0.self_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.0.self_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.0.self_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.0.self_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.0.self_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.0.self_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.0.self_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.0.encoder_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.0.encoder_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.0.encoder_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.0.encoder_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.0.encoder_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.0.encoder_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.0.encoder_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.0.encoder_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.0.encoder_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.0.encoder_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.0.fc1.weight', 'ocr_model.decoder.model.decoder.layers.0.fc1.bias', 'ocr_model.decoder.model.decoder.layers.0.fc2.weight', 'ocr_model.decoder.model.decoder.layers.0.fc2.bias', 'ocr_model.decoder.model.decoder.layers.0.final_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.0.final_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.1.self_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.1.self_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.1.self_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.1.self_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.1.self_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.1.self_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.1.self_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.1.self_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.1.self_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.1.self_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.1.encoder_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.1.encoder_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.1.encoder_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.1.encoder_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.1.encoder_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.1.encoder_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.1.encoder_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.1.encoder_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.1.encoder_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.1.encoder_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.1.fc1.weight', 'ocr_model.decoder.model.decoder.layers.1.fc1.bias', 'ocr_model.decoder.model.decoder.layers.1.fc2.weight', 'ocr_model.decoder.model.decoder.layers.1.fc2.bias', 'ocr_model.decoder.model.decoder.layers.1.final_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.1.final_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.2.self_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.2.self_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.2.self_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.2.self_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.2.self_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.2.self_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.2.self_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.2.self_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.2.self_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.2.self_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.2.encoder_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.2.encoder_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.2.encoder_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.2.encoder_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.2.encoder_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.2.encoder_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.2.encoder_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.2.encoder_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.2.encoder_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.2.encoder_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.2.fc1.weight', 'ocr_model.decoder.model.decoder.layers.2.fc1.bias', 'ocr_model.decoder.model.decoder.layers.2.fc2.weight', 'ocr_model.decoder.model.decoder.layers.2.fc2.bias', 'ocr_model.decoder.model.decoder.layers.2.final_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.2.final_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.3.self_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.3.self_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.3.self_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.3.self_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.3.self_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.3.self_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.3.self_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.3.self_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.3.self_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.3.self_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.3.encoder_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.3.encoder_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.3.encoder_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.3.encoder_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.3.encoder_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.3.encoder_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.3.encoder_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.3.encoder_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.3.encoder_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.3.encoder_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.3.fc1.weight', 'ocr_model.decoder.model.decoder.layers.3.fc1.bias', 'ocr_model.decoder.model.decoder.layers.3.fc2.weight', 'ocr_model.decoder.model.decoder.layers.3.fc2.bias', 'ocr_model.decoder.model.decoder.layers.3.final_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.3.final_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.4.self_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.4.self_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.4.self_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.4.self_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.4.self_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.4.self_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.4.self_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.4.self_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.4.self_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.4.self_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.4.encoder_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.4.encoder_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.4.encoder_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.4.encoder_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.4.encoder_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.4.encoder_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.4.encoder_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.4.encoder_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.4.encoder_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.4.encoder_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.4.fc1.weight', 'ocr_model.decoder.model.decoder.layers.4.fc1.bias', 'ocr_model.decoder.model.decoder.layers.4.fc2.weight', 'ocr_model.decoder.model.decoder.layers.4.fc2.bias', 'ocr_model.decoder.model.decoder.layers.4.final_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.4.final_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.5.self_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.5.self_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.5.self_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.5.self_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.5.self_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.5.self_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.5.self_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.5.self_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.5.self_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.5.self_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.5.encoder_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.5.encoder_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.5.encoder_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.5.encoder_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.5.encoder_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.5.encoder_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.5.encoder_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.5.encoder_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.5.encoder_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.5.encoder_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.5.fc1.weight', 'ocr_model.decoder.model.decoder.layers.5.fc1.bias', 'ocr_model.decoder.model.decoder.layers.5.fc2.weight', 'ocr_model.decoder.model.decoder.layers.5.fc2.bias', 'ocr_model.decoder.model.decoder.layers.5.final_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.5.final_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.6.self_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.6.self_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.6.self_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.6.self_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.6.self_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.6.self_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.6.self_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.6.self_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.6.self_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.6.self_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.6.encoder_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.6.encoder_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.6.encoder_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.6.encoder_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.6.encoder_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.6.encoder_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.6.encoder_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.6.encoder_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.6.encoder_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.6.encoder_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.6.fc1.weight', 'ocr_model.decoder.model.decoder.layers.6.fc1.bias', 'ocr_model.decoder.model.decoder.layers.6.fc2.weight', 'ocr_model.decoder.model.decoder.layers.6.fc2.bias', 'ocr_model.decoder.model.decoder.layers.6.final_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.6.final_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.7.self_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.7.self_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.7.self_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.7.self_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.7.self_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.7.self_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.7.self_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.7.self_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.7.self_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.7.self_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.7.encoder_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.7.encoder_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.7.encoder_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.7.encoder_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.7.encoder_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.7.encoder_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.7.encoder_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.7.encoder_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.7.encoder_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.7.encoder_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.7.fc1.weight', 'ocr_model.decoder.model.decoder.layers.7.fc1.bias', 'ocr_model.decoder.model.decoder.layers.7.fc2.weight', 'ocr_model.decoder.model.decoder.layers.7.fc2.bias', 'ocr_model.decoder.model.decoder.layers.7.final_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.7.final_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.8.self_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.8.self_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.8.self_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.8.self_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.8.self_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.8.self_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.8.self_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.8.self_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.8.self_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.8.self_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.8.encoder_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.8.encoder_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.8.encoder_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.8.encoder_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.8.encoder_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.8.encoder_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.8.encoder_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.8.encoder_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.8.encoder_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.8.encoder_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.8.fc1.weight', 'ocr_model.decoder.model.decoder.layers.8.fc1.bias', 'ocr_model.decoder.model.decoder.layers.8.fc2.weight', 'ocr_model.decoder.model.decoder.layers.8.fc2.bias', 'ocr_model.decoder.model.decoder.layers.8.final_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.8.final_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.9.self_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.9.self_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.9.self_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.9.self_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.9.self_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.9.self_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.9.self_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.9.self_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.9.self_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.9.self_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.9.encoder_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.9.encoder_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.9.encoder_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.9.encoder_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.9.encoder_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.9.encoder_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.9.encoder_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.9.encoder_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.9.encoder_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.9.encoder_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.9.fc1.weight', 'ocr_model.decoder.model.decoder.layers.9.fc1.bias', 'ocr_model.decoder.model.decoder.layers.9.fc2.weight', 'ocr_model.decoder.model.decoder.layers.9.fc2.bias', 'ocr_model.decoder.model.decoder.layers.9.final_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.9.final_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.10.self_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.10.self_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.10.self_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.10.self_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.10.self_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.10.self_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.10.self_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.10.self_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.10.self_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.10.self_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.10.encoder_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.10.encoder_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.10.encoder_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.10.encoder_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.10.encoder_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.10.encoder_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.10.encoder_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.10.encoder_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.10.encoder_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.10.encoder_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.10.fc1.weight', 'ocr_model.decoder.model.decoder.layers.10.fc1.bias', 'ocr_model.decoder.model.decoder.layers.10.fc2.weight', 'ocr_model.decoder.model.decoder.layers.10.fc2.bias', 'ocr_model.decoder.model.decoder.layers.10.final_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.10.final_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.11.self_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.11.self_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.11.self_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.11.self_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.11.self_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.11.self_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.11.self_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.11.self_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.11.self_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.11.self_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.11.encoder_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.11.encoder_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.11.encoder_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.11.encoder_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.11.encoder_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.11.encoder_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.11.encoder_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.11.encoder_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.11.encoder_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.11.encoder_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.11.fc1.weight', 'ocr_model.decoder.model.decoder.layers.11.fc1.bias', 'ocr_model.decoder.model.decoder.layers.11.fc2.weight', 'ocr_model.decoder.model.decoder.layers.11.fc2.bias', 'ocr_model.decoder.model.decoder.layers.11.final_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.11.final_layer_norm.bias', 'ocr_model.decoder.output_projection.weight', 'crop_embedding_model.embeddings.cls_token', 'crop_embedding_model.embeddings.position_embeddings', 'crop_embedding_model.embeddings.patch_embeddings.projection.weight', 'crop_embedding_model.embeddings.patch_embeddings.projection.bias', 'crop_embedding_model.encoder.layer.0.attention.attention.query.weight', 'crop_embedding_model.encoder.layer.0.attention.attention.query.bias', 'crop_embedding_model.encoder.layer.0.attention.attention.key.weight', 'crop_embedding_model.encoder.layer.0.attention.attention.key.bias', 'crop_embedding_model.encoder.layer.0.attention.attention.value.weight', 'crop_embedding_model.encoder.layer.0.attention.attention.value.bias', 'crop_embedding_model.encoder.layer.0.attention.output.dense.weight', 'crop_embedding_model.encoder.layer.0.attention.output.dense.bias', 'crop_embedding_model.encoder.layer.0.intermediate.dense.weight', 'crop_embedding_model.encoder.layer.0.intermediate.dense.bias', 'crop_embedding_model.encoder.layer.0.output.dense.weight', 'crop_embedding_model.encoder.layer.0.output.dense.bias', 'crop_embedding_model.encoder.layer.0.layernorm_before.weight', 'crop_embedding_model.encoder.layer.0.layernorm_before.bias', 'crop_embedding_model.encoder.layer.0.layernorm_after.weight', 'crop_embedding_model.encoder.layer.0.layernorm_after.bias', 'crop_embedding_model.encoder.layer.1.attention.attention.query.weight', 'crop_embedding_model.encoder.layer.1.attention.attention.query.bias', 'crop_embedding_model.encoder.layer.1.attention.attention.key.weight', 'crop_embedding_model.encoder.layer.1.attention.attention.key.bias', 'crop_embedding_model.encoder.layer.1.attention.attention.value.weight', 'crop_embedding_model.encoder.layer.1.attention.attention.value.bias', 'crop_embedding_model.encoder.layer.1.attention.output.dense.weight', 'crop_embedding_model.encoder.layer.1.attention.output.dense.bias', 'crop_embedding_model.encoder.layer.1.intermediate.dense.weight', 'crop_embedding_model.encoder.layer.1.intermediate.dense.bias', 'crop_embedding_model.encoder.layer.1.output.dense.weight', 'crop_embedding_model.encoder.layer.1.output.dense.bias', 'crop_embedding_model.encoder.layer.1.layernorm_before.weight', 'crop_embedding_model.encoder.layer.1.layernorm_before.bias', 'crop_embedding_model.encoder.layer.1.layernorm_after.weight', 'crop_embedding_model.encoder.layer.1.layernorm_after.bias', 'crop_embedding_model.encoder.layer.2.attention.attention.query.weight', 'crop_embedding_model.encoder.layer.2.attention.attention.query.bias', 'crop_embedding_model.encoder.layer.2.attention.attention.key.weight', 'crop_embedding_model.encoder.layer.2.attention.attention.key.bias', 'crop_embedding_model.encoder.layer.2.attention.attention.value.weight', 'crop_embedding_model.encoder.layer.2.attention.attention.value.bias', 'crop_embedding_model.encoder.layer.2.attention.output.dense.weight', 'crop_embedding_model.encoder.layer.2.attention.output.dense.bias', 'crop_embedding_model.encoder.layer.2.intermediate.dense.weight', 'crop_embedding_model.encoder.layer.2.intermediate.dense.bias', 'crop_embedding_model.encoder.layer.2.output.dense.weight', 'crop_embedding_model.encoder.layer.2.output.dense.bias', 'crop_embedding_model.encoder.layer.2.layernorm_before.weight', 'crop_embedding_model.encoder.layer.2.layernorm_before.bias', 'crop_embedding_model.encoder.layer.2.layernorm_after.weight', 'crop_embedding_model.encoder.layer.2.layernorm_after.bias', 'crop_embedding_model.encoder.layer.3.attention.attention.query.weight', 'crop_embedding_model.encoder.layer.3.attention.attention.query.bias', 'crop_embedding_model.encoder.layer.3.attention.attention.key.weight', 'crop_embedding_model.encoder.layer.3.attention.attention.key.bias', 'crop_embedding_model.encoder.layer.3.attention.attention.value.weight', 'crop_embedding_model.encoder.layer.3.attention.attention.value.bias', 'crop_embedding_model.encoder.layer.3.attention.output.dense.weight', 'crop_embedding_model.encoder.layer.3.attention.output.dense.bias', 'crop_embedding_model.encoder.layer.3.intermediate.dense.weight', 'crop_embedding_model.encoder.layer.3.intermediate.dense.bias', 'crop_embedding_model.encoder.layer.3.output.dense.weight', 'crop_embedding_model.encoder.layer.3.output.dense.bias', 'crop_embedding_model.encoder.layer.3.layernorm_before.weight', 'crop_embedding_model.encoder.layer.3.layernorm_before.bias', 'crop_embedding_model.encoder.layer.3.layernorm_after.weight', 'crop_embedding_model.encoder.layer.3.layernorm_after.bias', 'crop_embedding_model.encoder.layer.4.attention.attention.query.weight', 'crop_embedding_model.encoder.layer.4.attention.attention.query.bias', 'crop_embedding_model.encoder.layer.4.attention.attention.key.weight', 'crop_embedding_model.encoder.layer.4.attention.attention.key.bias', 'crop_embedding_model.encoder.layer.4.attention.attention.value.weight', 'crop_embedding_model.encoder.layer.4.attention.attention.value.bias', 'crop_embedding_model.encoder.layer.4.attention.output.dense.weight', 'crop_embedding_model.encoder.layer.4.attention.output.dense.bias', 'crop_embedding_model.encoder.layer.4.intermediate.dense.weight', 'crop_embedding_model.encoder.layer.4.intermediate.dense.bias', 'crop_embedding_model.encoder.layer.4.output.dense.weight', 'crop_embedding_model.encoder.layer.4.output.dense.bias', 'crop_embedding_model.encoder.layer.4.layernorm_before.weight', 'crop_embedding_model.encoder.layer.4.layernorm_before.bias', 'crop_embedding_model.encoder.layer.4.layernorm_after.weight', 'crop_embedding_model.encoder.layer.4.layernorm_after.bias', 'crop_embedding_model.encoder.layer.5.attention.attention.query.weight', 'crop_embedding_model.encoder.layer.5.attention.attention.query.bias', 'crop_embedding_model.encoder.layer.5.attention.attention.key.weight', 'crop_embedding_model.encoder.layer.5.attention.attention.key.bias', 'crop_embedding_model.encoder.layer.5.attention.attention.value.weight', 'crop_embedding_model.encoder.layer.5.attention.attention.value.bias', 'crop_embedding_model.encoder.layer.5.attention.output.dense.weight', 'crop_embedding_model.encoder.layer.5.attention.output.dense.bias', 'crop_embedding_model.encoder.layer.5.intermediate.dense.weight', 'crop_embedding_model.encoder.layer.5.intermediate.dense.bias', 'crop_embedding_model.encoder.layer.5.output.dense.weight', 'crop_embedding_model.encoder.layer.5.output.dense.bias', 'crop_embedding_model.encoder.layer.5.layernorm_before.weight', 'crop_embedding_model.encoder.layer.5.layernorm_before.bias', 'crop_embedding_model.encoder.layer.5.layernorm_after.weight', 'crop_embedding_model.encoder.layer.5.layernorm_after.bias', 'crop_embedding_model.encoder.layer.6.attention.attention.query.weight', 'crop_embedding_model.encoder.layer.6.attention.attention.query.bias', 'crop_embedding_model.encoder.layer.6.attention.attention.key.weight', 'crop_embedding_model.encoder.layer.6.attention.attention.key.bias', 'crop_embedding_model.encoder.layer.6.attention.attention.value.weight', 'crop_embedding_model.encoder.layer.6.attention.attention.value.bias', 'crop_embedding_model.encoder.layer.6.attention.output.dense.weight', 'crop_embedding_model.encoder.layer.6.attention.output.dense.bias', 'crop_embedding_model.encoder.layer.6.intermediate.dense.weight', 'crop_embedding_model.encoder.layer.6.intermediate.dense.bias', 'crop_embedding_model.encoder.layer.6.output.dense.weight', 'crop_embedding_model.encoder.layer.6.output.dense.bias', 'crop_embedding_model.encoder.layer.6.layernorm_before.weight', 'crop_embedding_model.encoder.layer.6.layernorm_before.bias', 'crop_embedding_model.encoder.layer.6.layernorm_after.weight', 'crop_embedding_model.encoder.layer.6.layernorm_after.bias', 'crop_embedding_model.encoder.layer.7.attention.attention.query.weight', 'crop_embedding_model.encoder.layer.7.attention.attention.query.bias', 'crop_embedding_model.encoder.layer.7.attention.attention.key.weight', 'crop_embedding_model.encoder.layer.7.attention.attention.key.bias', 'crop_embedding_model.encoder.layer.7.attention.attention.value.weight', 'crop_embedding_model.encoder.layer.7.attention.attention.value.bias', 'crop_embedding_model.encoder.layer.7.attention.output.dense.weight', 'crop_embedding_model.encoder.layer.7.attention.output.dense.bias', 'crop_embedding_model.encoder.layer.7.intermediate.dense.weight', 'crop_embedding_model.encoder.layer.7.intermediate.dense.bias', 'crop_embedding_model.encoder.layer.7.output.dense.weight', 'crop_embedding_model.encoder.layer.7.output.dense.bias', 'crop_embedding_model.encoder.layer.7.layernorm_before.weight', 'crop_embedding_model.encoder.layer.7.layernorm_before.bias', 'crop_embedding_model.encoder.layer.7.layernorm_after.weight', 'crop_embedding_model.encoder.layer.7.layernorm_after.bias', 'crop_embedding_model.encoder.layer.8.attention.attention.query.weight', 'crop_embedding_model.encoder.layer.8.attention.attention.query.bias', 'crop_embedding_model.encoder.layer.8.attention.attention.key.weight', 'crop_embedding_model.encoder.layer.8.attention.attention.key.bias', 'crop_embedding_model.encoder.layer.8.attention.attention.value.weight', 'crop_embedding_model.encoder.layer.8.attention.attention.value.bias', 'crop_embedding_model.encoder.layer.8.attention.output.dense.weight', 'crop_embedding_model.encoder.layer.8.attention.output.dense.bias', 'crop_embedding_model.encoder.layer.8.intermediate.dense.weight', 'crop_embedding_model.encoder.layer.8.intermediate.dense.bias', 'crop_embedding_model.encoder.layer.8.output.dense.weight', 'crop_embedding_model.encoder.layer.8.output.dense.bias', 'crop_embedding_model.encoder.layer.8.layernorm_before.weight', 'crop_embedding_model.encoder.layer.8.layernorm_before.bias', 'crop_embedding_model.encoder.layer.8.layernorm_after.weight', 'crop_embedding_model.encoder.layer.8.layernorm_after.bias', 'crop_embedding_model.encoder.layer.9.attention.attention.query.weight', 'crop_embedding_model.encoder.layer.9.attention.attention.query.bias', 'crop_embedding_model.encoder.layer.9.attention.attention.key.weight', 'crop_embedding_model.encoder.layer.9.attention.attention.key.bias', 'crop_embedding_model.encoder.layer.9.attention.attention.value.weight', 'crop_embedding_model.encoder.layer.9.attention.attention.value.bias', 'crop_embedding_model.encoder.layer.9.attention.output.dense.weight', 'crop_embedding_model.encoder.layer.9.attention.output.dense.bias', 'crop_embedding_model.encoder.layer.9.intermediate.dense.weight', 'crop_embedding_model.encoder.layer.9.intermediate.dense.bias', 'crop_embedding_model.encoder.layer.9.output.dense.weight', 'crop_embedding_model.encoder.layer.9.output.dense.bias', 'crop_embedding_model.encoder.layer.9.layernorm_before.weight', 'crop_embedding_model.encoder.layer.9.layernorm_before.bias', 'crop_embedding_model.encoder.layer.9.layernorm_after.weight', 'crop_embedding_model.encoder.layer.9.layernorm_after.bias', 'crop_embedding_model.encoder.layer.10.attention.attention.query.weight', 'crop_embedding_model.encoder.layer.10.attention.attention.query.bias', 'crop_embedding_model.encoder.layer.10.attention.attention.key.weight', 'crop_embedding_model.encoder.layer.10.attention.attention.key.bias', 'crop_embedding_model.encoder.layer.10.attention.attention.value.weight', 'crop_embedding_model.encoder.layer.10.attention.attention.value.bias', 'crop_embedding_model.encoder.layer.10.attention.output.dense.weight', 'crop_embedding_model.encoder.layer.10.attention.output.dense.bias', 'crop_embedding_model.encoder.layer.10.intermediate.dense.weight', 'crop_embedding_model.encoder.layer.10.intermediate.dense.bias', 'crop_embedding_model.encoder.layer.10.output.dense.weight', 'crop_embedding_model.encoder.layer.10.output.dense.bias', 'crop_embedding_model.encoder.layer.10.layernorm_before.weight', 'crop_embedding_model.encoder.layer.10.layernorm_before.bias', 'crop_embedding_model.encoder.layer.10.layernorm_after.weight', 'crop_embedding_model.encoder.layer.10.layernorm_after.bias', 'crop_embedding_model.encoder.layer.11.attention.attention.query.weight', 'crop_embedding_model.encoder.layer.11.attention.attention.query.bias', 'crop_embedding_model.encoder.layer.11.attention.attention.key.weight', 'crop_embedding_model.encoder.layer.11.attention.attention.key.bias', 'crop_embedding_model.encoder.layer.11.attention.attention.value.weight', 'crop_embedding_model.encoder.layer.11.attention.attention.value.bias', 'crop_embedding_model.encoder.layer.11.attention.output.dense.weight', 'crop_embedding_model.encoder.layer.11.attention.output.dense.bias', 'crop_embedding_model.encoder.layer.11.intermediate.dense.weight', 'crop_embedding_model.encoder.layer.11.intermediate.dense.bias', 'crop_embedding_model.encoder.layer.11.output.dense.weight', 'crop_embedding_model.encoder.layer.11.output.dense.bias', 'crop_embedding_model.encoder.layer.11.layernorm_before.weight', 'crop_embedding_model.encoder.layer.11.layernorm_before.bias', 'crop_embedding_model.encoder.layer.11.layernorm_after.weight', 'crop_embedding_model.encoder.layer.11.layernorm_after.bias', 'crop_embedding_model.layernorm.weight', 'crop_embedding_model.layernorm.bias', 'detection_transformer.backbone.conv_encoder.model.conv1.weight', 'detection_transformer.backbone.conv_encoder.model.bn1.weight', 'detection_transformer.backbone.conv_encoder.model.bn1.bias', 'detection_transformer.backbone.conv_encoder.model.bn1.running_mean', 'detection_transformer.backbone.conv_encoder.model.bn1.running_var', 'detection_transformer.backbone.conv_encoder.model.layer1.0.conv1.weight', 'detection_transformer.backbone.conv_encoder.model.layer1.0.bn1.weight', 'detection_transformer.backbone.conv_encoder.model.layer1.0.bn1.bias', 'detection_transformer.backbone.conv_encoder.model.layer1.0.bn1.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer1.0.bn1.running_var', 'detection_transformer.backbone.conv_encoder.model.layer1.0.conv2.weight', 'detection_transformer.backbone.conv_encoder.model.layer1.0.bn2.weight', 'detection_transformer.backbone.conv_encoder.model.layer1.0.bn2.bias', 'detection_transformer.backbone.conv_encoder.model.layer1.0.bn2.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer1.0.bn2.running_var', 'detection_transformer.backbone.conv_encoder.model.layer1.0.conv3.weight', 'detection_transformer.backbone.conv_encoder.model.layer1.0.bn3.weight', 'detection_transformer.backbone.conv_encoder.model.layer1.0.bn3.bias', 'detection_transformer.backbone.conv_encoder.model.layer1.0.bn3.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer1.0.bn3.running_var', 'detection_transformer.backbone.conv_encoder.model.layer1.0.downsample.0.weight', 'detection_transformer.backbone.conv_encoder.model.layer1.0.downsample.1.weight', 'detection_transformer.backbone.conv_encoder.model.layer1.0.downsample.1.bias', 'detection_transformer.backbone.conv_encoder.model.layer1.0.downsample.1.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer1.0.downsample.1.running_var', 'detection_transformer.backbone.conv_encoder.model.layer1.1.conv1.weight', 'detection_transformer.backbone.conv_encoder.model.layer1.1.bn1.weight', 'detection_transformer.backbone.conv_encoder.model.layer1.1.bn1.bias', 'detection_transformer.backbone.conv_encoder.model.layer1.1.bn1.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer1.1.bn1.running_var', 'detection_transformer.backbone.conv_encoder.model.layer1.1.conv2.weight', 'detection_transformer.backbone.conv_encoder.model.layer1.1.bn2.weight', 'detection_transformer.backbone.conv_encoder.model.layer1.1.bn2.bias', 'detection_transformer.backbone.conv_encoder.model.layer1.1.bn2.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer1.1.bn2.running_var', 'detection_transformer.backbone.conv_encoder.model.layer1.1.conv3.weight', 'detection_transformer.backbone.conv_encoder.model.layer1.1.bn3.weight', 'detection_transformer.backbone.conv_encoder.model.layer1.1.bn3.bias', 'detection_transformer.backbone.conv_encoder.model.layer1.1.bn3.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer1.1.bn3.running_var', 'detection_transformer.backbone.conv_encoder.model.layer1.2.conv1.weight', 'detection_transformer.backbone.conv_encoder.model.layer1.2.bn1.weight', 'detection_transformer.backbone.conv_encoder.model.layer1.2.bn1.bias', 'detection_transformer.backbone.conv_encoder.model.layer1.2.bn1.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer1.2.bn1.running_var', 'detection_transformer.backbone.conv_encoder.model.layer1.2.conv2.weight', 'detection_transformer.backbone.conv_encoder.model.layer1.2.bn2.weight', 'detection_transformer.backbone.conv_encoder.model.layer1.2.bn2.bias', 'detection_transformer.backbone.conv_encoder.model.layer1.2.bn2.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer1.2.bn2.running_var', 'detection_transformer.backbone.conv_encoder.model.layer1.2.conv3.weight', 'detection_transformer.backbone.conv_encoder.model.layer1.2.bn3.weight', 'detection_transformer.backbone.conv_encoder.model.layer1.2.bn3.bias', 'detection_transformer.backbone.conv_encoder.model.layer1.2.bn3.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer1.2.bn3.running_var', 'detection_transformer.backbone.conv_encoder.model.layer2.0.conv1.weight', 'detection_transformer.backbone.conv_encoder.model.layer2.0.bn1.weight', 'detection_transformer.backbone.conv_encoder.model.layer2.0.bn1.bias', 'detection_transformer.backbone.conv_encoder.model.layer2.0.bn1.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer2.0.bn1.running_var', 'detection_transformer.backbone.conv_encoder.model.layer2.0.conv2.weight', 'detection_transformer.backbone.conv_encoder.model.layer2.0.bn2.weight', 'detection_transformer.backbone.conv_encoder.model.layer2.0.bn2.bias', 'detection_transformer.backbone.conv_encoder.model.layer2.0.bn2.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer2.0.bn2.running_var', 'detection_transformer.backbone.conv_encoder.model.layer2.0.conv3.weight', 'detection_transformer.backbone.conv_encoder.model.layer2.0.bn3.weight', 'detection_transformer.backbone.conv_encoder.model.layer2.0.bn3.bias', 'detection_transformer.backbone.conv_encoder.model.layer2.0.bn3.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer2.0.bn3.running_var', 'detection_transformer.backbone.conv_encoder.model.layer2.0.downsample.0.weight', 'detection_transformer.backbone.conv_encoder.model.layer2.0.downsample.1.weight', 'detection_transformer.backbone.conv_encoder.model.layer2.0.downsample.1.bias', 'detection_transformer.backbone.conv_encoder.model.layer2.0.downsample.1.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer2.0.downsample.1.running_var', 'detection_transformer.backbone.conv_encoder.model.layer2.1.conv1.weight', 'detection_transformer.backbone.conv_encoder.model.layer2.1.bn1.weight', 'detection_transformer.backbone.conv_encoder.model.layer2.1.bn1.bias', 'detection_transformer.backbone.conv_encoder.model.layer2.1.bn1.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer2.1.bn1.running_var', 'detection_transformer.backbone.conv_encoder.model.layer2.1.conv2.weight', 'detection_transformer.backbone.conv_encoder.model.layer2.1.bn2.weight', 'detection_transformer.backbone.conv_encoder.model.layer2.1.bn2.bias', 'detection_transformer.backbone.conv_encoder.model.layer2.1.bn2.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer2.1.bn2.running_var', 'detection_transformer.backbone.conv_encoder.model.layer2.1.conv3.weight', 'detection_transformer.backbone.conv_encoder.model.layer2.1.bn3.weight', 'detection_transformer.backbone.conv_encoder.model.layer2.1.bn3.bias', 'detection_transformer.backbone.conv_encoder.model.layer2.1.bn3.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer2.1.bn3.running_var', 'detection_transformer.backbone.conv_encoder.model.layer2.2.conv1.weight', 'detection_transformer.backbone.conv_encoder.model.layer2.2.bn1.weight', 'detection_transformer.backbone.conv_encoder.model.layer2.2.bn1.bias', 'detection_transformer.backbone.conv_encoder.model.layer2.2.bn1.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer2.2.bn1.running_var', 'detection_transformer.backbone.conv_encoder.model.layer2.2.conv2.weight', 'detection_transformer.backbone.conv_encoder.model.layer2.2.bn2.weight', 'detection_transformer.backbone.conv_encoder.model.layer2.2.bn2.bias', 'detection_transformer.backbone.conv_encoder.model.layer2.2.bn2.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer2.2.bn2.running_var', 'detection_transformer.backbone.conv_encoder.model.layer2.2.conv3.weight', 'detection_transformer.backbone.conv_encoder.model.layer2.2.bn3.weight', 'detection_transformer.backbone.conv_encoder.model.layer2.2.bn3.bias', 'detection_transformer.backbone.conv_encoder.model.layer2.2.bn3.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer2.2.bn3.running_var', 'detection_transformer.backbone.conv_encoder.model.layer2.3.conv1.weight', 'detection_transformer.backbone.conv_encoder.model.layer2.3.bn1.weight', 'detection_transformer.backbone.conv_encoder.model.layer2.3.bn1.bias', 'detection_transformer.backbone.conv_encoder.model.layer2.3.bn1.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer2.3.bn1.running_var', 'detection_transformer.backbone.conv_encoder.model.layer2.3.conv2.weight', 'detection_transformer.backbone.conv_encoder.model.layer2.3.bn2.weight', 'detection_transformer.backbone.conv_encoder.model.layer2.3.bn2.bias', 'detection_transformer.backbone.conv_encoder.model.layer2.3.bn2.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer2.3.bn2.running_var', 'detection_transformer.backbone.conv_encoder.model.layer2.3.conv3.weight', 'detection_transformer.backbone.conv_encoder.model.layer2.3.bn3.weight', 'detection_transformer.backbone.conv_encoder.model.layer2.3.bn3.bias', 'detection_transformer.backbone.conv_encoder.model.layer2.3.bn3.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer2.3.bn3.running_var', 'detection_transformer.backbone.conv_encoder.model.layer3.0.conv1.weight', 'detection_transformer.backbone.conv_encoder.model.layer3.0.bn1.weight', 'detection_transformer.backbone.conv_encoder.model.layer3.0.bn1.bias', 'detection_transformer.backbone.conv_encoder.model.layer3.0.bn1.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer3.0.bn1.running_var', 'detection_transformer.backbone.conv_encoder.model.layer3.0.conv2.weight', 'detection_transformer.backbone.conv_encoder.model.layer3.0.bn2.weight', 'detection_transformer.backbone.conv_encoder.model.layer3.0.bn2.bias', 'detection_transformer.backbone.conv_encoder.model.layer3.0.bn2.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer3.0.bn2.running_var', 'detection_transformer.backbone.conv_encoder.model.layer3.0.conv3.weight', 'detection_transformer.backbone.conv_encoder.model.layer3.0.bn3.weight', 'detection_transformer.backbone.conv_encoder.model.layer3.0.bn3.bias', 'detection_transformer.backbone.conv_encoder.model.layer3.0.bn3.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer3.0.bn3.running_var', 'detection_transformer.backbone.conv_encoder.model.layer3.0.downsample.0.weight', 'detection_transformer.backbone.conv_encoder.model.layer3.0.downsample.1.weight', 'detection_transformer.backbone.conv_encoder.model.layer3.0.downsample.1.bias', 'detection_transformer.backbone.conv_encoder.model.layer3.0.downsample.1.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer3.0.downsample.1.running_var', 'detection_transformer.backbone.conv_encoder.model.layer3.1.conv1.weight', 'detection_transformer.backbone.conv_encoder.model.layer3.1.bn1.weight', 'detection_transformer.backbone.conv_encoder.model.layer3.1.bn1.bias', 'detection_transformer.backbone.conv_encoder.model.layer3.1.bn1.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer3.1.bn1.running_var', 'detection_transformer.backbone.conv_encoder.model.layer3.1.conv2.weight', 'detection_transformer.backbone.conv_encoder.model.layer3.1.bn2.weight', 'detection_transformer.backbone.conv_encoder.model.layer3.1.bn2.bias', 'detection_transformer.backbone.conv_encoder.model.layer3.1.bn2.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer3.1.bn2.running_var', 'detection_transformer.backbone.conv_encoder.model.layer3.1.conv3.weight', 'detection_transformer.backbone.conv_encoder.model.layer3.1.bn3.weight', 'detection_transformer.backbone.conv_encoder.model.layer3.1.bn3.bias', 'detection_transformer.backbone.conv_encoder.model.layer3.1.bn3.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer3.1.bn3.running_var', 'detection_transformer.backbone.conv_encoder.model.layer3.2.conv1.weight', 'detection_transformer.backbone.conv_encoder.model.layer3.2.bn1.weight', 'detection_transformer.backbone.conv_encoder.model.layer3.2.bn1.bias', 'detection_transformer.backbone.conv_encoder.model.layer3.2.bn1.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer3.2.bn1.running_var', 'detection_transformer.backbone.conv_encoder.model.layer3.2.conv2.weight', 'detection_transformer.backbone.conv_encoder.model.layer3.2.bn2.weight', 'detection_transformer.backbone.conv_encoder.model.layer3.2.bn2.bias', 'detection_transformer.backbone.conv_encoder.model.layer3.2.bn2.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer3.2.bn2.running_var', 'detection_transformer.backbone.conv_encoder.model.layer3.2.conv3.weight', 'detection_transformer.backbone.conv_encoder.model.layer3.2.bn3.weight', 'detection_transformer.backbone.conv_encoder.model.layer3.2.bn3.bias', 'detection_transformer.backbone.conv_encoder.model.layer3.2.bn3.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer3.2.bn3.running_var', 'detection_transformer.backbone.conv_encoder.model.layer3.3.conv1.weight', 'detection_transformer.backbone.conv_encoder.model.layer3.3.bn1.weight', 'detection_transformer.backbone.conv_encoder.model.layer3.3.bn1.bias', 'detection_transformer.backbone.conv_encoder.model.layer3.3.bn1.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer3.3.bn1.running_var', 'detection_transformer.backbone.conv_encoder.model.layer3.3.conv2.weight', 'detection_transformer.backbone.conv_encoder.model.layer3.3.bn2.weight', 'detection_transformer.backbone.conv_encoder.model.layer3.3.bn2.bias', 'detection_transformer.backbone.conv_encoder.model.layer3.3.bn2.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer3.3.bn2.running_var', 'detection_transformer.backbone.conv_encoder.model.layer3.3.conv3.weight', 'detection_transformer.backbone.conv_encoder.model.layer3.3.bn3.weight', 'detection_transformer.backbone.conv_encoder.model.layer3.3.bn3.bias', 'detection_transformer.backbone.conv_encoder.model.layer3.3.bn3.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer3.3.bn3.running_var', 'detection_transformer.backbone.conv_encoder.model.layer3.4.conv1.weight', 'detection_transformer.backbone.conv_encoder.model.layer3.4.bn1.weight', 'detection_transformer.backbone.conv_encoder.model.layer3.4.bn1.bias', 'detection_transformer.backbone.conv_encoder.model.layer3.4.bn1.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer3.4.bn1.running_var', 'detection_transformer.backbone.conv_encoder.model.layer3.4.conv2.weight', 'detection_transformer.backbone.conv_encoder.model.layer3.4.bn2.weight', 'detection_transformer.backbone.conv_encoder.model.layer3.4.bn2.bias', 'detection_transformer.backbone.conv_encoder.model.layer3.4.bn2.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer3.4.bn2.running_var', 'detection_transformer.backbone.conv_encoder.model.layer3.4.conv3.weight', 'detection_transformer.backbone.conv_encoder.model.layer3.4.bn3.weight', 'detection_transformer.backbone.conv_encoder.model.layer3.4.bn3.bias', 'detection_transformer.backbone.conv_encoder.model.layer3.4.bn3.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer3.4.bn3.running_var', 'detection_transformer.backbone.conv_encoder.model.layer3.5.conv1.weight', 'detection_transformer.backbone.conv_encoder.model.layer3.5.bn1.weight', 'detection_transformer.backbone.conv_encoder.model.layer3.5.bn1.bias', 'detection_transformer.backbone.conv_encoder.model.layer3.5.bn1.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer3.5.bn1.running_var', 'detection_transformer.backbone.conv_encoder.model.layer3.5.conv2.weight', 'detection_transformer.backbone.conv_encoder.model.layer3.5.bn2.weight', 'detection_transformer.backbone.conv_encoder.model.layer3.5.bn2.bias', 'detection_transformer.backbone.conv_encoder.model.layer3.5.bn2.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer3.5.bn2.running_var', 'detection_transformer.backbone.conv_encoder.model.layer3.5.conv3.weight', 'detection_transformer.backbone.conv_encoder.model.layer3.5.bn3.weight', 'detection_transformer.backbone.conv_encoder.model.layer3.5.bn3.bias', 'detection_transformer.backbone.conv_encoder.model.layer3.5.bn3.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer3.5.bn3.running_var', 'detection_transformer.backbone.conv_encoder.model.layer4.0.conv1.weight', 'detection_transformer.backbone.conv_encoder.model.layer4.0.bn1.weight', 'detection_transformer.backbone.conv_encoder.model.layer4.0.bn1.bias', 'detection_transformer.backbone.conv_encoder.model.layer4.0.bn1.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer4.0.bn1.running_var', 'detection_transformer.backbone.conv_encoder.model.layer4.0.conv2.weight', 'detection_transformer.backbone.conv_encoder.model.layer4.0.bn2.weight', 'detection_transformer.backbone.conv_encoder.model.layer4.0.bn2.bias', 'detection_transformer.backbone.conv_encoder.model.layer4.0.bn2.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer4.0.bn2.running_var', 'detection_transformer.backbone.conv_encoder.model.layer4.0.conv3.weight', 'detection_transformer.backbone.conv_encoder.model.layer4.0.bn3.weight', 'detection_transformer.backbone.conv_encoder.model.layer4.0.bn3.bias', 'detection_transformer.backbone.conv_encoder.model.layer4.0.bn3.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer4.0.bn3.running_var', 'detection_transformer.backbone.conv_encoder.model.layer4.0.downsample.0.weight', 'detection_transformer.backbone.conv_encoder.model.layer4.0.downsample.1.weight', 'detection_transformer.backbone.conv_encoder.model.layer4.0.downsample.1.bias', 'detection_transformer.backbone.conv_encoder.model.layer4.0.downsample.1.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer4.0.downsample.1.running_var', 'detection_transformer.backbone.conv_encoder.model.layer4.1.conv1.weight', 'detection_transformer.backbone.conv_encoder.model.layer4.1.bn1.weight', 'detection_transformer.backbone.conv_encoder.model.layer4.1.bn1.bias', 'detection_transformer.backbone.conv_encoder.model.layer4.1.bn1.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer4.1.bn1.running_var', 'detection_transformer.backbone.conv_encoder.model.layer4.1.conv2.weight', 'detection_transformer.backbone.conv_encoder.model.layer4.1.bn2.weight', 'detection_transformer.backbone.conv_encoder.model.layer4.1.bn2.bias', 'detection_transformer.backbone.conv_encoder.model.layer4.1.bn2.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer4.1.bn2.running_var', 'detection_transformer.backbone.conv_encoder.model.layer4.1.conv3.weight', 'detection_transformer.backbone.conv_encoder.model.layer4.1.bn3.weight', 'detection_transformer.backbone.conv_encoder.model.layer4.1.bn3.bias', 'detection_transformer.backbone.conv_encoder.model.layer4.1.bn3.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer4.1.bn3.running_var', 'detection_transformer.backbone.conv_encoder.model.layer4.2.conv1.weight', 'detection_transformer.backbone.conv_encoder.model.layer4.2.bn1.weight', 'detection_transformer.backbone.conv_encoder.model.layer4.2.bn1.bias', 'detection_transformer.backbone.conv_encoder.model.layer4.2.bn1.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer4.2.bn1.running_var', 'detection_transformer.backbone.conv_encoder.model.layer4.2.conv2.weight', 'detection_transformer.backbone.conv_encoder.model.layer4.2.bn2.weight', 'detection_transformer.backbone.conv_encoder.model.layer4.2.bn2.bias', 'detection_transformer.backbone.conv_encoder.model.layer4.2.bn2.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer4.2.bn2.running_var', 'detection_transformer.backbone.conv_encoder.model.layer4.2.conv3.weight', 'detection_transformer.backbone.conv_encoder.model.layer4.2.bn3.weight', 'detection_transformer.backbone.conv_encoder.model.layer4.2.bn3.bias', 'detection_transformer.backbone.conv_encoder.model.layer4.2.bn3.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer4.2.bn3.running_var', 'detection_transformer.input_projection.weight', 'detection_transformer.input_projection.bias', 'detection_transformer.query_position_embeddings.weight', 'detection_transformer.encoder.layers.0.self_attn.k_proj.weight', 'detection_transformer.encoder.layers.0.self_attn.k_proj.bias', 'detection_transformer.encoder.layers.0.self_attn.v_proj.weight', 'detection_transformer.encoder.layers.0.self_attn.v_proj.bias', 'detection_transformer.encoder.layers.0.self_attn.q_proj.weight', 'detection_transformer.encoder.layers.0.self_attn.q_proj.bias', 'detection_transformer.encoder.layers.0.self_attn.out_proj.weight', 'detection_transformer.encoder.layers.0.self_attn.out_proj.bias', 'detection_transformer.encoder.layers.0.self_attn_layer_norm.weight', 'detection_transformer.encoder.layers.0.self_attn_layer_norm.bias', 'detection_transformer.encoder.layers.0.fc1.weight', 'detection_transformer.encoder.layers.0.fc1.bias', 'detection_transformer.encoder.layers.0.fc2.weight', 'detection_transformer.encoder.layers.0.fc2.bias', 'detection_transformer.encoder.layers.0.final_layer_norm.weight', 'detection_transformer.encoder.layers.0.final_layer_norm.bias', 'detection_transformer.encoder.layers.1.self_attn.k_proj.weight', 'detection_transformer.encoder.layers.1.self_attn.k_proj.bias', 'detection_transformer.encoder.layers.1.self_attn.v_proj.weight', 'detection_transformer.encoder.layers.1.self_attn.v_proj.bias', 'detection_transformer.encoder.layers.1.self_attn.q_proj.weight', 'detection_transformer.encoder.layers.1.self_attn.q_proj.bias', 'detection_transformer.encoder.layers.1.self_attn.out_proj.weight', 'detection_transformer.encoder.layers.1.self_attn.out_proj.bias', 'detection_transformer.encoder.layers.1.self_attn_layer_norm.weight', 'detection_transformer.encoder.layers.1.self_attn_layer_norm.bias', 'detection_transformer.encoder.layers.1.fc1.weight', 'detection_transformer.encoder.layers.1.fc1.bias', 'detection_transformer.encoder.layers.1.fc2.weight', 'detection_transformer.encoder.layers.1.fc2.bias', 'detection_transformer.encoder.layers.1.final_layer_norm.weight', 'detection_transformer.encoder.layers.1.final_layer_norm.bias', 'detection_transformer.encoder.layers.2.self_attn.k_proj.weight', 'detection_transformer.encoder.layers.2.self_attn.k_proj.bias', 'detection_transformer.encoder.layers.2.self_attn.v_proj.weight', 'detection_transformer.encoder.layers.2.self_attn.v_proj.bias', 'detection_transformer.encoder.layers.2.self_attn.q_proj.weight', 'detection_transformer.encoder.layers.2.self_attn.q_proj.bias', 'detection_transformer.encoder.layers.2.self_attn.out_proj.weight', 'detection_transformer.encoder.layers.2.self_attn.out_proj.bias', 'detection_transformer.encoder.layers.2.self_attn_layer_norm.weight', 'detection_transformer.encoder.layers.2.self_attn_layer_norm.bias', 'detection_transformer.encoder.layers.2.fc1.weight', 'detection_transformer.encoder.layers.2.fc1.bias', 'detection_transformer.encoder.layers.2.fc2.weight', 'detection_transformer.encoder.layers.2.fc2.bias', 'detection_transformer.encoder.layers.2.final_layer_norm.weight', 'detection_transformer.encoder.layers.2.final_layer_norm.bias', 'detection_transformer.encoder.layers.3.self_attn.k_proj.weight', 'detection_transformer.encoder.layers.3.self_attn.k_proj.bias', 'detection_transformer.encoder.layers.3.self_attn.v_proj.weight', 'detection_transformer.encoder.layers.3.self_attn.v_proj.bias', 'detection_transformer.encoder.layers.3.self_attn.q_proj.weight', 'detection_transformer.encoder.layers.3.self_attn.q_proj.bias', 'detection_transformer.encoder.layers.3.self_attn.out_proj.weight', 'detection_transformer.encoder.layers.3.self_attn.out_proj.bias', 'detection_transformer.encoder.layers.3.self_attn_layer_norm.weight', 'detection_transformer.encoder.layers.3.self_attn_layer_norm.bias', 'detection_transformer.encoder.layers.3.fc1.weight', 'detection_transformer.encoder.layers.3.fc1.bias', 'detection_transformer.encoder.layers.3.fc2.weight', 'detection_transformer.encoder.layers.3.fc2.bias', 'detection_transformer.encoder.layers.3.final_layer_norm.weight', 'detection_transformer.encoder.layers.3.final_layer_norm.bias', 'detection_transformer.encoder.layers.4.self_attn.k_proj.weight', 'detection_transformer.encoder.layers.4.self_attn.k_proj.bias', 'detection_transformer.encoder.layers.4.self_attn.v_proj.weight', 'detection_transformer.encoder.layers.4.self_attn.v_proj.bias', 'detection_transformer.encoder.layers.4.self_attn.q_proj.weight', 'detection_transformer.encoder.layers.4.self_attn.q_proj.bias', 'detection_transformer.encoder.layers.4.self_attn.out_proj.weight', 'detection_transformer.encoder.layers.4.self_attn.out_proj.bias', 'detection_transformer.encoder.layers.4.self_attn_layer_norm.weight', 'detection_transformer.encoder.layers.4.self_attn_layer_norm.bias', 'detection_transformer.encoder.layers.4.fc1.weight', 'detection_transformer.encoder.layers.4.fc1.bias', 'detection_transformer.encoder.layers.4.fc2.weight', 'detection_transformer.encoder.layers.4.fc2.bias', 'detection_transformer.encoder.layers.4.final_layer_norm.weight', 'detection_transformer.encoder.layers.4.final_layer_norm.bias', 'detection_transformer.encoder.layers.5.self_attn.k_proj.weight', 'detection_transformer.encoder.layers.5.self_attn.k_proj.bias', 'detection_transformer.encoder.layers.5.self_attn.v_proj.weight', 'detection_transformer.encoder.layers.5.self_attn.v_proj.bias', 'detection_transformer.encoder.layers.5.self_attn.q_proj.weight', 'detection_transformer.encoder.layers.5.self_attn.q_proj.bias', 'detection_transformer.encoder.layers.5.self_attn.out_proj.weight', 'detection_transformer.encoder.layers.5.self_attn.out_proj.bias', 'detection_transformer.encoder.layers.5.self_attn_layer_norm.weight', 'detection_transformer.encoder.layers.5.self_attn_layer_norm.bias', 'detection_transformer.encoder.layers.5.fc1.weight', 'detection_transformer.encoder.layers.5.fc1.bias', 'detection_transformer.encoder.layers.5.fc2.weight', 'detection_transformer.encoder.layers.5.fc2.bias', 'detection_transformer.encoder.layers.5.final_layer_norm.weight', 'detection_transformer.encoder.layers.5.final_layer_norm.bias', 'detection_transformer.decoder.layers.0.sa_qcontent_proj.weight', 'detection_transformer.decoder.layers.0.sa_qcontent_proj.bias', 'detection_transformer.decoder.layers.0.sa_qpos_proj.weight', 'detection_transformer.decoder.layers.0.sa_qpos_proj.bias', 'detection_transformer.decoder.layers.0.sa_kcontent_proj.weight', 'detection_transformer.decoder.layers.0.sa_kcontent_proj.bias', 'detection_transformer.decoder.layers.0.sa_kpos_proj.weight', 'detection_transformer.decoder.layers.0.sa_kpos_proj.bias', 'detection_transformer.decoder.layers.0.sa_v_proj.weight', 'detection_transformer.decoder.layers.0.sa_v_proj.bias', 'detection_transformer.decoder.layers.0.self_attn.out_proj.weight', 'detection_transformer.decoder.layers.0.self_attn.out_proj.bias', 'detection_transformer.decoder.layers.0.self_attn_layer_norm.weight', 'detection_transformer.decoder.layers.0.self_attn_layer_norm.bias', 'detection_transformer.decoder.layers.0.ca_qcontent_proj.weight', 'detection_transformer.decoder.layers.0.ca_qcontent_proj.bias', 'detection_transformer.decoder.layers.0.ca_qpos_proj.weight', 'detection_transformer.decoder.layers.0.ca_qpos_proj.bias', 'detection_transformer.decoder.layers.0.ca_kcontent_proj.weight', 'detection_transformer.decoder.layers.0.ca_kcontent_proj.bias', 'detection_transformer.decoder.layers.0.ca_kpos_proj.weight', 'detection_transformer.decoder.layers.0.ca_kpos_proj.bias', 'detection_transformer.decoder.layers.0.ca_v_proj.weight', 'detection_transformer.decoder.layers.0.ca_v_proj.bias', 'detection_transformer.decoder.layers.0.ca_qpos_sine_proj.weight', 'detection_transformer.decoder.layers.0.ca_qpos_sine_proj.bias', 'detection_transformer.decoder.layers.0.encoder_attn.out_proj.weight', 'detection_transformer.decoder.layers.0.encoder_attn.out_proj.bias', 'detection_transformer.decoder.layers.0.encoder_attn_layer_norm.weight', 'detection_transformer.decoder.layers.0.encoder_attn_layer_norm.bias', 'detection_transformer.decoder.layers.0.fc1.weight', 'detection_transformer.decoder.layers.0.fc1.bias', 'detection_transformer.decoder.layers.0.fc2.weight', 'detection_transformer.decoder.layers.0.fc2.bias', 'detection_transformer.decoder.layers.0.final_layer_norm.weight', 'detection_transformer.decoder.layers.0.final_layer_norm.bias', 'detection_transformer.decoder.layers.1.sa_qcontent_proj.weight', 'detection_transformer.decoder.layers.1.sa_qcontent_proj.bias', 'detection_transformer.decoder.layers.1.sa_qpos_proj.weight', 'detection_transformer.decoder.layers.1.sa_qpos_proj.bias', 'detection_transformer.decoder.layers.1.sa_kcontent_proj.weight', 'detection_transformer.decoder.layers.1.sa_kcontent_proj.bias', 'detection_transformer.decoder.layers.1.sa_kpos_proj.weight', 'detection_transformer.decoder.layers.1.sa_kpos_proj.bias', 'detection_transformer.decoder.layers.1.sa_v_proj.weight', 'detection_transformer.decoder.layers.1.sa_v_proj.bias', 'detection_transformer.decoder.layers.1.self_attn.out_proj.weight', 'detection_transformer.decoder.layers.1.self_attn.out_proj.bias', 'detection_transformer.decoder.layers.1.self_attn_layer_norm.weight', 'detection_transformer.decoder.layers.1.self_attn_layer_norm.bias', 'detection_transformer.decoder.layers.1.ca_qcontent_proj.weight', 'detection_transformer.decoder.layers.1.ca_qcontent_proj.bias', 'detection_transformer.decoder.layers.1.ca_kcontent_proj.weight', 'detection_transformer.decoder.layers.1.ca_kcontent_proj.bias', 'detection_transformer.decoder.layers.1.ca_kpos_proj.weight', 'detection_transformer.decoder.layers.1.ca_kpos_proj.bias', 'detection_transformer.decoder.layers.1.ca_v_proj.weight', 'detection_transformer.decoder.layers.1.ca_v_proj.bias', 'detection_transformer.decoder.layers.1.ca_qpos_sine_proj.weight', 'detection_transformer.decoder.layers.1.ca_qpos_sine_proj.bias', 'detection_transformer.decoder.layers.1.encoder_attn.out_proj.weight', 'detection_transformer.decoder.layers.1.encoder_attn.out_proj.bias', 'detection_transformer.decoder.layers.1.encoder_attn_layer_norm.weight', 'detection_transformer.decoder.layers.1.encoder_attn_layer_norm.bias', 'detection_transformer.decoder.layers.1.fc1.weight', 'detection_transformer.decoder.layers.1.fc1.bias', 'detection_transformer.decoder.layers.1.fc2.weight', 'detection_transformer.decoder.layers.1.fc2.bias', 'detection_transformer.decoder.layers.1.final_layer_norm.weight', 'detection_transformer.decoder.layers.1.final_layer_norm.bias', 'detection_transformer.decoder.layers.2.sa_qcontent_proj.weight', 'detection_transformer.decoder.layers.2.sa_qcontent_proj.bias', 'detection_transformer.decoder.layers.2.sa_qpos_proj.weight', 'detection_transformer.decoder.layers.2.sa_qpos_proj.bias', 'detection_transformer.decoder.layers.2.sa_kcontent_proj.weight', 'detection_transformer.decoder.layers.2.sa_kcontent_proj.bias', 'detection_transformer.decoder.layers.2.sa_kpos_proj.weight', 'detection_transformer.decoder.layers.2.sa_kpos_proj.bias', 'detection_transformer.decoder.layers.2.sa_v_proj.weight', 'detection_transformer.decoder.layers.2.sa_v_proj.bias', 'detection_transformer.decoder.layers.2.self_attn.out_proj.weight', 'detection_transformer.decoder.layers.2.self_attn.out_proj.bias', 'detection_transformer.decoder.layers.2.self_attn_layer_norm.weight', 'detection_transformer.decoder.layers.2.self_attn_layer_norm.bias', 'detection_transformer.decoder.layers.2.ca_qcontent_proj.weight', 'detection_transformer.decoder.layers.2.ca_qcontent_proj.bias', 'detection_transformer.decoder.layers.2.ca_kcontent_proj.weight', 'detection_transformer.decoder.layers.2.ca_kcontent_proj.bias', 'detection_transformer.decoder.layers.2.ca_kpos_proj.weight', 'detection_transformer.decoder.layers.2.ca_kpos_proj.bias', 'detection_transformer.decoder.layers.2.ca_v_proj.weight', 'detection_transformer.decoder.layers.2.ca_v_proj.bias', 'detection_transformer.decoder.layers.2.ca_qpos_sine_proj.weight', 'detection_transformer.decoder.layers.2.ca_qpos_sine_proj.bias', 'detection_transformer.decoder.layers.2.encoder_attn.out_proj.weight', 'detection_transformer.decoder.layers.2.encoder_attn.out_proj.bias', 'detection_transformer.decoder.layers.2.encoder_attn_layer_norm.weight', 'detection_transformer.decoder.layers.2.encoder_attn_layer_norm.bias', 'detection_transformer.decoder.layers.2.fc1.weight', 'detection_transformer.decoder.layers.2.fc1.bias', 'detection_transformer.decoder.layers.2.fc2.weight', 'detection_transformer.decoder.layers.2.fc2.bias', 'detection_transformer.decoder.layers.2.final_layer_norm.weight', 'detection_transformer.decoder.layers.2.final_layer_norm.bias', 'detection_transformer.decoder.layers.3.sa_qcontent_proj.weight', 'detection_transformer.decoder.layers.3.sa_qcontent_proj.bias', 'detection_transformer.decoder.layers.3.sa_qpos_proj.weight', 'detection_transformer.decoder.layers.3.sa_qpos_proj.bias', 'detection_transformer.decoder.layers.3.sa_kcontent_proj.weight', 'detection_transformer.decoder.layers.3.sa_kcontent_proj.bias', 'detection_transformer.decoder.layers.3.sa_kpos_proj.weight', 'detection_transformer.decoder.layers.3.sa_kpos_proj.bias', 'detection_transformer.decoder.layers.3.sa_v_proj.weight', 'detection_transformer.decoder.layers.3.sa_v_proj.bias', 'detection_transformer.decoder.layers.3.self_attn.out_proj.weight', 'detection_transformer.decoder.layers.3.self_attn.out_proj.bias', 'detection_transformer.decoder.layers.3.self_attn_layer_norm.weight', 'detection_transformer.decoder.layers.3.self_attn_layer_norm.bias', 'detection_transformer.decoder.layers.3.ca_qcontent_proj.weight', 'detection_transformer.decoder.layers.3.ca_qcontent_proj.bias', 'detection_transformer.decoder.layers.3.ca_kcontent_proj.weight', 'detection_transformer.decoder.layers.3.ca_kcontent_proj.bias', 'detection_transformer.decoder.layers.3.ca_kpos_proj.weight', 'detection_transformer.decoder.layers.3.ca_kpos_proj.bias', 'detection_transformer.decoder.layers.3.ca_v_proj.weight', 'detection_transformer.decoder.layers.3.ca_v_proj.bias', 'detection_transformer.decoder.layers.3.ca_qpos_sine_proj.weight', 'detection_transformer.decoder.layers.3.ca_qpos_sine_proj.bias', 'detection_transformer.decoder.layers.3.encoder_attn.out_proj.weight', 'detection_transformer.decoder.layers.3.encoder_attn.out_proj.bias', 'detection_transformer.decoder.layers.3.encoder_attn_layer_norm.weight', 'detection_transformer.decoder.layers.3.encoder_attn_layer_norm.bias', 'detection_transformer.decoder.layers.3.fc1.weight', 'detection_transformer.decoder.layers.3.fc1.bias', 'detection_transformer.decoder.layers.3.fc2.weight', 'detection_transformer.decoder.layers.3.fc2.bias', 'detection_transformer.decoder.layers.3.final_layer_norm.weight', 'detection_transformer.decoder.layers.3.final_layer_norm.bias', 'detection_transformer.decoder.layers.4.sa_qcontent_proj.weight', 'detection_transformer.decoder.layers.4.sa_qcontent_proj.bias', 'detection_transformer.decoder.layers.4.sa_qpos_proj.weight', 'detection_transformer.decoder.layers.4.sa_qpos_proj.bias', 'detection_transformer.decoder.layers.4.sa_kcontent_proj.weight', 'detection_transformer.decoder.layers.4.sa_kcontent_proj.bias', 'detection_transformer.decoder.layers.4.sa_kpos_proj.weight', 'detection_transformer.decoder.layers.4.sa_kpos_proj.bias', 'detection_transformer.decoder.layers.4.sa_v_proj.weight', 'detection_transformer.decoder.layers.4.sa_v_proj.bias', 'detection_transformer.decoder.layers.4.self_attn.out_proj.weight', 'detection_transformer.decoder.layers.4.self_attn.out_proj.bias', 'detection_transformer.decoder.layers.4.self_attn_layer_norm.weight', 'detection_transformer.decoder.layers.4.self_attn_layer_norm.bias', 'detection_transformer.decoder.layers.4.ca_qcontent_proj.weight', 'detection_transformer.decoder.layers.4.ca_qcontent_proj.bias', 'detection_transformer.decoder.layers.4.ca_kcontent_proj.weight', 'detection_transformer.decoder.layers.4.ca_kcontent_proj.bias', 'detection_transformer.decoder.layers.4.ca_kpos_proj.weight', 'detection_transformer.decoder.layers.4.ca_kpos_proj.bias', 'detection_transformer.decoder.layers.4.ca_v_proj.weight', 'detection_transformer.decoder.layers.4.ca_v_proj.bias', 'detection_transformer.decoder.layers.4.ca_qpos_sine_proj.weight', 'detection_transformer.decoder.layers.4.ca_qpos_sine_proj.bias', 'detection_transformer.decoder.layers.4.encoder_attn.out_proj.weight', 'detection_transformer.decoder.layers.4.encoder_attn.out_proj.bias', 'detection_transformer.decoder.layers.4.encoder_attn_layer_norm.weight', 'detection_transformer.decoder.layers.4.encoder_attn_layer_norm.bias', 'detection_transformer.decoder.layers.4.fc1.weight', 'detection_transformer.decoder.layers.4.fc1.bias', 'detection_transformer.decoder.layers.4.fc2.weight', 'detection_transformer.decoder.layers.4.fc2.bias', 'detection_transformer.decoder.layers.4.final_layer_norm.weight', 'detection_transformer.decoder.layers.4.final_layer_norm.bias', 'detection_transformer.decoder.layers.5.sa_qcontent_proj.weight', 'detection_transformer.decoder.layers.5.sa_qcontent_proj.bias', 'detection_transformer.decoder.layers.5.sa_qpos_proj.weight', 'detection_transformer.decoder.layers.5.sa_qpos_proj.bias', 'detection_transformer.decoder.layers.5.sa_kcontent_proj.weight', 'detection_transformer.decoder.layers.5.sa_kcontent_proj.bias', 'detection_transformer.decoder.layers.5.sa_kpos_proj.weight', 'detection_transformer.decoder.layers.5.sa_kpos_proj.bias', 'detection_transformer.decoder.layers.5.sa_v_proj.weight', 'detection_transformer.decoder.layers.5.sa_v_proj.bias', 'detection_transformer.decoder.layers.5.self_attn.out_proj.weight', 'detection_transformer.decoder.layers.5.self_attn.out_proj.bias', 'detection_transformer.decoder.layers.5.self_attn_layer_norm.weight', 'detection_transformer.decoder.layers.5.self_attn_layer_norm.bias', 'detection_transformer.decoder.layers.5.ca_qcontent_proj.weight', 'detection_transformer.decoder.layers.5.ca_qcontent_proj.bias', 'detection_transformer.decoder.layers.5.ca_kcontent_proj.weight', 'detection_transformer.decoder.layers.5.ca_kcontent_proj.bias', 'detection_transformer.decoder.layers.5.ca_kpos_proj.weight', 'detection_transformer.decoder.layers.5.ca_kpos_proj.bias', 'detection_transformer.decoder.layers.5.ca_v_proj.weight', 'detection_transformer.decoder.layers.5.ca_v_proj.bias', 'detection_transformer.decoder.layers.5.ca_qpos_sine_proj.weight', 'detection_transformer.decoder.layers.5.ca_qpos_sine_proj.bias', 'detection_transformer.decoder.layers.5.encoder_attn.out_proj.weight', 'detection_transformer.decoder.layers.5.encoder_attn.out_proj.bias', 'detection_transformer.decoder.layers.5.encoder_attn_layer_norm.weight', 'detection_transformer.decoder.layers.5.encoder_attn_layer_norm.bias', 'detection_transformer.decoder.layers.5.fc1.weight', 'detection_transformer.decoder.layers.5.fc1.bias', 'detection_transformer.decoder.layers.5.fc2.weight', 'detection_transformer.decoder.layers.5.fc2.bias', 'detection_transformer.decoder.layers.5.final_layer_norm.weight', 'detection_transformer.decoder.layers.5.final_layer_norm.bias', 'detection_transformer.decoder.layernorm.weight', 'detection_transformer.decoder.layernorm.bias', 'detection_transformer.decoder.query_scale.layers.0.weight', 'detection_transformer.decoder.query_scale.layers.0.bias', 'detection_transformer.decoder.query_scale.layers.1.weight', 'detection_transformer.decoder.query_scale.layers.1.bias', 'detection_transformer.decoder.ref_point_head.layers.0.weight', 'detection_transformer.decoder.ref_point_head.layers.0.bias', 'detection_transformer.decoder.ref_point_head.layers.1.weight', 'detection_transformer.decoder.ref_point_head.layers.1.bias', 'bbox_predictor.layers.0.weight', 'bbox_predictor.layers.0.bias', 'bbox_predictor.layers.1.weight', 'bbox_predictor.layers.1.bias', 'bbox_predictor.layers.2.weight', 'bbox_predictor.layers.2.bias', 'is_this_text_a_dialogue.layers.0.weight', 'is_this_text_a_dialogue.layers.0.bias', 'is_this_text_a_dialogue.layers.1.weight', 'is_this_text_a_dialogue.layers.1.bias', 'is_this_text_a_dialogue.layers.2.weight', 'is_this_text_a_dialogue.layers.2.bias', 'character_character_matching_head.layers.0.weight', 'character_character_matching_head.layers.0.bias', 'character_character_matching_head.layers.1.weight', 'character_character_matching_head.layers.1.bias', 'character_character_matching_head.layers.2.weight', 'character_character_matching_head.layers.2.bias', 'text_character_matching_head.layers.0.weight', 'text_character_matching_head.layers.0.bias', 'text_character_matching_head.layers.1.weight', 'text_character_matching_head.layers.1.bias', 'text_character_matching_head.layers.2.weight', 'text_character_matching_head.layers.2.bias', 'class_labels_classifier.weight', 'class_labels_classifier.bias'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = load_state_dict(str(Path(\"models/magi/pytorch_model.bin\").resolve()))\n",
    "state_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Считываем конфиг из локальной директории и инициализируем нашу модель с ним"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "config: MagiConfig = MagiConfig.from_json_file(Path(\"libs/lizi/my_magi/config.json\").resolve())  # type: ignore\n",
    "model = MyMagiModel(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружаем pre-train веса в модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=[], unexpected_keys=['ocr_model.encoder.embeddings.cls_token', 'ocr_model.encoder.embeddings.position_embeddings', 'ocr_model.encoder.embeddings.patch_embeddings.projection.weight', 'ocr_model.encoder.embeddings.patch_embeddings.projection.bias', 'ocr_model.encoder.encoder.layer.0.attention.attention.query.weight', 'ocr_model.encoder.encoder.layer.0.attention.attention.key.weight', 'ocr_model.encoder.encoder.layer.0.attention.attention.value.weight', 'ocr_model.encoder.encoder.layer.0.attention.output.dense.weight', 'ocr_model.encoder.encoder.layer.0.attention.output.dense.bias', 'ocr_model.encoder.encoder.layer.0.intermediate.dense.weight', 'ocr_model.encoder.encoder.layer.0.intermediate.dense.bias', 'ocr_model.encoder.encoder.layer.0.output.dense.weight', 'ocr_model.encoder.encoder.layer.0.output.dense.bias', 'ocr_model.encoder.encoder.layer.0.layernorm_before.weight', 'ocr_model.encoder.encoder.layer.0.layernorm_before.bias', 'ocr_model.encoder.encoder.layer.0.layernorm_after.weight', 'ocr_model.encoder.encoder.layer.0.layernorm_after.bias', 'ocr_model.encoder.encoder.layer.1.attention.attention.query.weight', 'ocr_model.encoder.encoder.layer.1.attention.attention.key.weight', 'ocr_model.encoder.encoder.layer.1.attention.attention.value.weight', 'ocr_model.encoder.encoder.layer.1.attention.output.dense.weight', 'ocr_model.encoder.encoder.layer.1.attention.output.dense.bias', 'ocr_model.encoder.encoder.layer.1.intermediate.dense.weight', 'ocr_model.encoder.encoder.layer.1.intermediate.dense.bias', 'ocr_model.encoder.encoder.layer.1.output.dense.weight', 'ocr_model.encoder.encoder.layer.1.output.dense.bias', 'ocr_model.encoder.encoder.layer.1.layernorm_before.weight', 'ocr_model.encoder.encoder.layer.1.layernorm_before.bias', 'ocr_model.encoder.encoder.layer.1.layernorm_after.weight', 'ocr_model.encoder.encoder.layer.1.layernorm_after.bias', 'ocr_model.encoder.encoder.layer.2.attention.attention.query.weight', 'ocr_model.encoder.encoder.layer.2.attention.attention.key.weight', 'ocr_model.encoder.encoder.layer.2.attention.attention.value.weight', 'ocr_model.encoder.encoder.layer.2.attention.output.dense.weight', 'ocr_model.encoder.encoder.layer.2.attention.output.dense.bias', 'ocr_model.encoder.encoder.layer.2.intermediate.dense.weight', 'ocr_model.encoder.encoder.layer.2.intermediate.dense.bias', 'ocr_model.encoder.encoder.layer.2.output.dense.weight', 'ocr_model.encoder.encoder.layer.2.output.dense.bias', 'ocr_model.encoder.encoder.layer.2.layernorm_before.weight', 'ocr_model.encoder.encoder.layer.2.layernorm_before.bias', 'ocr_model.encoder.encoder.layer.2.layernorm_after.weight', 'ocr_model.encoder.encoder.layer.2.layernorm_after.bias', 'ocr_model.encoder.encoder.layer.3.attention.attention.query.weight', 'ocr_model.encoder.encoder.layer.3.attention.attention.key.weight', 'ocr_model.encoder.encoder.layer.3.attention.attention.value.weight', 'ocr_model.encoder.encoder.layer.3.attention.output.dense.weight', 'ocr_model.encoder.encoder.layer.3.attention.output.dense.bias', 'ocr_model.encoder.encoder.layer.3.intermediate.dense.weight', 'ocr_model.encoder.encoder.layer.3.intermediate.dense.bias', 'ocr_model.encoder.encoder.layer.3.output.dense.weight', 'ocr_model.encoder.encoder.layer.3.output.dense.bias', 'ocr_model.encoder.encoder.layer.3.layernorm_before.weight', 'ocr_model.encoder.encoder.layer.3.layernorm_before.bias', 'ocr_model.encoder.encoder.layer.3.layernorm_after.weight', 'ocr_model.encoder.encoder.layer.3.layernorm_after.bias', 'ocr_model.encoder.encoder.layer.4.attention.attention.query.weight', 'ocr_model.encoder.encoder.layer.4.attention.attention.key.weight', 'ocr_model.encoder.encoder.layer.4.attention.attention.value.weight', 'ocr_model.encoder.encoder.layer.4.attention.output.dense.weight', 'ocr_model.encoder.encoder.layer.4.attention.output.dense.bias', 'ocr_model.encoder.encoder.layer.4.intermediate.dense.weight', 'ocr_model.encoder.encoder.layer.4.intermediate.dense.bias', 'ocr_model.encoder.encoder.layer.4.output.dense.weight', 'ocr_model.encoder.encoder.layer.4.output.dense.bias', 'ocr_model.encoder.encoder.layer.4.layernorm_before.weight', 'ocr_model.encoder.encoder.layer.4.layernorm_before.bias', 'ocr_model.encoder.encoder.layer.4.layernorm_after.weight', 'ocr_model.encoder.encoder.layer.4.layernorm_after.bias', 'ocr_model.encoder.encoder.layer.5.attention.attention.query.weight', 'ocr_model.encoder.encoder.layer.5.attention.attention.key.weight', 'ocr_model.encoder.encoder.layer.5.attention.attention.value.weight', 'ocr_model.encoder.encoder.layer.5.attention.output.dense.weight', 'ocr_model.encoder.encoder.layer.5.attention.output.dense.bias', 'ocr_model.encoder.encoder.layer.5.intermediate.dense.weight', 'ocr_model.encoder.encoder.layer.5.intermediate.dense.bias', 'ocr_model.encoder.encoder.layer.5.output.dense.weight', 'ocr_model.encoder.encoder.layer.5.output.dense.bias', 'ocr_model.encoder.encoder.layer.5.layernorm_before.weight', 'ocr_model.encoder.encoder.layer.5.layernorm_before.bias', 'ocr_model.encoder.encoder.layer.5.layernorm_after.weight', 'ocr_model.encoder.encoder.layer.5.layernorm_after.bias', 'ocr_model.encoder.encoder.layer.6.attention.attention.query.weight', 'ocr_model.encoder.encoder.layer.6.attention.attention.key.weight', 'ocr_model.encoder.encoder.layer.6.attention.attention.value.weight', 'ocr_model.encoder.encoder.layer.6.attention.output.dense.weight', 'ocr_model.encoder.encoder.layer.6.attention.output.dense.bias', 'ocr_model.encoder.encoder.layer.6.intermediate.dense.weight', 'ocr_model.encoder.encoder.layer.6.intermediate.dense.bias', 'ocr_model.encoder.encoder.layer.6.output.dense.weight', 'ocr_model.encoder.encoder.layer.6.output.dense.bias', 'ocr_model.encoder.encoder.layer.6.layernorm_before.weight', 'ocr_model.encoder.encoder.layer.6.layernorm_before.bias', 'ocr_model.encoder.encoder.layer.6.layernorm_after.weight', 'ocr_model.encoder.encoder.layer.6.layernorm_after.bias', 'ocr_model.encoder.encoder.layer.7.attention.attention.query.weight', 'ocr_model.encoder.encoder.layer.7.attention.attention.key.weight', 'ocr_model.encoder.encoder.layer.7.attention.attention.value.weight', 'ocr_model.encoder.encoder.layer.7.attention.output.dense.weight', 'ocr_model.encoder.encoder.layer.7.attention.output.dense.bias', 'ocr_model.encoder.encoder.layer.7.intermediate.dense.weight', 'ocr_model.encoder.encoder.layer.7.intermediate.dense.bias', 'ocr_model.encoder.encoder.layer.7.output.dense.weight', 'ocr_model.encoder.encoder.layer.7.output.dense.bias', 'ocr_model.encoder.encoder.layer.7.layernorm_before.weight', 'ocr_model.encoder.encoder.layer.7.layernorm_before.bias', 'ocr_model.encoder.encoder.layer.7.layernorm_after.weight', 'ocr_model.encoder.encoder.layer.7.layernorm_after.bias', 'ocr_model.encoder.encoder.layer.8.attention.attention.query.weight', 'ocr_model.encoder.encoder.layer.8.attention.attention.key.weight', 'ocr_model.encoder.encoder.layer.8.attention.attention.value.weight', 'ocr_model.encoder.encoder.layer.8.attention.output.dense.weight', 'ocr_model.encoder.encoder.layer.8.attention.output.dense.bias', 'ocr_model.encoder.encoder.layer.8.intermediate.dense.weight', 'ocr_model.encoder.encoder.layer.8.intermediate.dense.bias', 'ocr_model.encoder.encoder.layer.8.output.dense.weight', 'ocr_model.encoder.encoder.layer.8.output.dense.bias', 'ocr_model.encoder.encoder.layer.8.layernorm_before.weight', 'ocr_model.encoder.encoder.layer.8.layernorm_before.bias', 'ocr_model.encoder.encoder.layer.8.layernorm_after.weight', 'ocr_model.encoder.encoder.layer.8.layernorm_after.bias', 'ocr_model.encoder.encoder.layer.9.attention.attention.query.weight', 'ocr_model.encoder.encoder.layer.9.attention.attention.key.weight', 'ocr_model.encoder.encoder.layer.9.attention.attention.value.weight', 'ocr_model.encoder.encoder.layer.9.attention.output.dense.weight', 'ocr_model.encoder.encoder.layer.9.attention.output.dense.bias', 'ocr_model.encoder.encoder.layer.9.intermediate.dense.weight', 'ocr_model.encoder.encoder.layer.9.intermediate.dense.bias', 'ocr_model.encoder.encoder.layer.9.output.dense.weight', 'ocr_model.encoder.encoder.layer.9.output.dense.bias', 'ocr_model.encoder.encoder.layer.9.layernorm_before.weight', 'ocr_model.encoder.encoder.layer.9.layernorm_before.bias', 'ocr_model.encoder.encoder.layer.9.layernorm_after.weight', 'ocr_model.encoder.encoder.layer.9.layernorm_after.bias', 'ocr_model.encoder.encoder.layer.10.attention.attention.query.weight', 'ocr_model.encoder.encoder.layer.10.attention.attention.key.weight', 'ocr_model.encoder.encoder.layer.10.attention.attention.value.weight', 'ocr_model.encoder.encoder.layer.10.attention.output.dense.weight', 'ocr_model.encoder.encoder.layer.10.attention.output.dense.bias', 'ocr_model.encoder.encoder.layer.10.intermediate.dense.weight', 'ocr_model.encoder.encoder.layer.10.intermediate.dense.bias', 'ocr_model.encoder.encoder.layer.10.output.dense.weight', 'ocr_model.encoder.encoder.layer.10.output.dense.bias', 'ocr_model.encoder.encoder.layer.10.layernorm_before.weight', 'ocr_model.encoder.encoder.layer.10.layernorm_before.bias', 'ocr_model.encoder.encoder.layer.10.layernorm_after.weight', 'ocr_model.encoder.encoder.layer.10.layernorm_after.bias', 'ocr_model.encoder.encoder.layer.11.attention.attention.query.weight', 'ocr_model.encoder.encoder.layer.11.attention.attention.key.weight', 'ocr_model.encoder.encoder.layer.11.attention.attention.value.weight', 'ocr_model.encoder.encoder.layer.11.attention.output.dense.weight', 'ocr_model.encoder.encoder.layer.11.attention.output.dense.bias', 'ocr_model.encoder.encoder.layer.11.intermediate.dense.weight', 'ocr_model.encoder.encoder.layer.11.intermediate.dense.bias', 'ocr_model.encoder.encoder.layer.11.output.dense.weight', 'ocr_model.encoder.encoder.layer.11.output.dense.bias', 'ocr_model.encoder.encoder.layer.11.layernorm_before.weight', 'ocr_model.encoder.encoder.layer.11.layernorm_before.bias', 'ocr_model.encoder.encoder.layer.11.layernorm_after.weight', 'ocr_model.encoder.encoder.layer.11.layernorm_after.bias', 'ocr_model.encoder.layernorm.weight', 'ocr_model.encoder.layernorm.bias', 'ocr_model.encoder.pooler.dense.weight', 'ocr_model.encoder.pooler.dense.bias', 'ocr_model.decoder.model.decoder.embed_tokens.weight', 'ocr_model.decoder.model.decoder.embed_positions.weight', 'ocr_model.decoder.model.decoder.layernorm_embedding.weight', 'ocr_model.decoder.model.decoder.layernorm_embedding.bias', 'ocr_model.decoder.model.decoder.layers.0.self_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.0.self_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.0.self_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.0.self_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.0.self_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.0.self_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.0.self_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.0.self_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.0.self_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.0.self_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.0.encoder_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.0.encoder_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.0.encoder_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.0.encoder_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.0.encoder_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.0.encoder_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.0.encoder_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.0.encoder_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.0.encoder_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.0.encoder_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.0.fc1.weight', 'ocr_model.decoder.model.decoder.layers.0.fc1.bias', 'ocr_model.decoder.model.decoder.layers.0.fc2.weight', 'ocr_model.decoder.model.decoder.layers.0.fc2.bias', 'ocr_model.decoder.model.decoder.layers.0.final_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.0.final_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.1.self_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.1.self_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.1.self_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.1.self_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.1.self_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.1.self_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.1.self_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.1.self_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.1.self_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.1.self_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.1.encoder_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.1.encoder_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.1.encoder_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.1.encoder_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.1.encoder_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.1.encoder_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.1.encoder_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.1.encoder_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.1.encoder_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.1.encoder_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.1.fc1.weight', 'ocr_model.decoder.model.decoder.layers.1.fc1.bias', 'ocr_model.decoder.model.decoder.layers.1.fc2.weight', 'ocr_model.decoder.model.decoder.layers.1.fc2.bias', 'ocr_model.decoder.model.decoder.layers.1.final_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.1.final_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.2.self_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.2.self_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.2.self_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.2.self_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.2.self_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.2.self_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.2.self_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.2.self_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.2.self_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.2.self_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.2.encoder_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.2.encoder_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.2.encoder_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.2.encoder_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.2.encoder_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.2.encoder_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.2.encoder_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.2.encoder_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.2.encoder_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.2.encoder_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.2.fc1.weight', 'ocr_model.decoder.model.decoder.layers.2.fc1.bias', 'ocr_model.decoder.model.decoder.layers.2.fc2.weight', 'ocr_model.decoder.model.decoder.layers.2.fc2.bias', 'ocr_model.decoder.model.decoder.layers.2.final_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.2.final_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.3.self_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.3.self_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.3.self_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.3.self_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.3.self_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.3.self_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.3.self_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.3.self_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.3.self_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.3.self_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.3.encoder_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.3.encoder_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.3.encoder_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.3.encoder_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.3.encoder_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.3.encoder_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.3.encoder_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.3.encoder_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.3.encoder_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.3.encoder_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.3.fc1.weight', 'ocr_model.decoder.model.decoder.layers.3.fc1.bias', 'ocr_model.decoder.model.decoder.layers.3.fc2.weight', 'ocr_model.decoder.model.decoder.layers.3.fc2.bias', 'ocr_model.decoder.model.decoder.layers.3.final_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.3.final_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.4.self_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.4.self_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.4.self_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.4.self_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.4.self_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.4.self_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.4.self_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.4.self_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.4.self_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.4.self_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.4.encoder_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.4.encoder_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.4.encoder_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.4.encoder_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.4.encoder_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.4.encoder_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.4.encoder_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.4.encoder_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.4.encoder_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.4.encoder_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.4.fc1.weight', 'ocr_model.decoder.model.decoder.layers.4.fc1.bias', 'ocr_model.decoder.model.decoder.layers.4.fc2.weight', 'ocr_model.decoder.model.decoder.layers.4.fc2.bias', 'ocr_model.decoder.model.decoder.layers.4.final_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.4.final_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.5.self_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.5.self_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.5.self_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.5.self_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.5.self_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.5.self_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.5.self_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.5.self_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.5.self_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.5.self_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.5.encoder_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.5.encoder_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.5.encoder_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.5.encoder_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.5.encoder_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.5.encoder_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.5.encoder_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.5.encoder_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.5.encoder_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.5.encoder_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.5.fc1.weight', 'ocr_model.decoder.model.decoder.layers.5.fc1.bias', 'ocr_model.decoder.model.decoder.layers.5.fc2.weight', 'ocr_model.decoder.model.decoder.layers.5.fc2.bias', 'ocr_model.decoder.model.decoder.layers.5.final_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.5.final_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.6.self_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.6.self_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.6.self_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.6.self_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.6.self_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.6.self_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.6.self_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.6.self_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.6.self_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.6.self_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.6.encoder_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.6.encoder_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.6.encoder_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.6.encoder_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.6.encoder_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.6.encoder_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.6.encoder_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.6.encoder_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.6.encoder_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.6.encoder_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.6.fc1.weight', 'ocr_model.decoder.model.decoder.layers.6.fc1.bias', 'ocr_model.decoder.model.decoder.layers.6.fc2.weight', 'ocr_model.decoder.model.decoder.layers.6.fc2.bias', 'ocr_model.decoder.model.decoder.layers.6.final_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.6.final_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.7.self_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.7.self_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.7.self_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.7.self_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.7.self_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.7.self_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.7.self_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.7.self_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.7.self_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.7.self_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.7.encoder_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.7.encoder_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.7.encoder_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.7.encoder_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.7.encoder_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.7.encoder_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.7.encoder_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.7.encoder_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.7.encoder_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.7.encoder_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.7.fc1.weight', 'ocr_model.decoder.model.decoder.layers.7.fc1.bias', 'ocr_model.decoder.model.decoder.layers.7.fc2.weight', 'ocr_model.decoder.model.decoder.layers.7.fc2.bias', 'ocr_model.decoder.model.decoder.layers.7.final_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.7.final_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.8.self_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.8.self_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.8.self_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.8.self_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.8.self_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.8.self_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.8.self_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.8.self_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.8.self_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.8.self_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.8.encoder_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.8.encoder_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.8.encoder_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.8.encoder_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.8.encoder_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.8.encoder_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.8.encoder_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.8.encoder_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.8.encoder_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.8.encoder_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.8.fc1.weight', 'ocr_model.decoder.model.decoder.layers.8.fc1.bias', 'ocr_model.decoder.model.decoder.layers.8.fc2.weight', 'ocr_model.decoder.model.decoder.layers.8.fc2.bias', 'ocr_model.decoder.model.decoder.layers.8.final_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.8.final_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.9.self_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.9.self_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.9.self_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.9.self_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.9.self_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.9.self_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.9.self_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.9.self_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.9.self_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.9.self_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.9.encoder_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.9.encoder_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.9.encoder_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.9.encoder_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.9.encoder_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.9.encoder_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.9.encoder_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.9.encoder_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.9.encoder_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.9.encoder_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.9.fc1.weight', 'ocr_model.decoder.model.decoder.layers.9.fc1.bias', 'ocr_model.decoder.model.decoder.layers.9.fc2.weight', 'ocr_model.decoder.model.decoder.layers.9.fc2.bias', 'ocr_model.decoder.model.decoder.layers.9.final_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.9.final_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.10.self_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.10.self_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.10.self_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.10.self_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.10.self_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.10.self_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.10.self_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.10.self_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.10.self_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.10.self_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.10.encoder_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.10.encoder_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.10.encoder_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.10.encoder_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.10.encoder_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.10.encoder_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.10.encoder_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.10.encoder_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.10.encoder_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.10.encoder_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.10.fc1.weight', 'ocr_model.decoder.model.decoder.layers.10.fc1.bias', 'ocr_model.decoder.model.decoder.layers.10.fc2.weight', 'ocr_model.decoder.model.decoder.layers.10.fc2.bias', 'ocr_model.decoder.model.decoder.layers.10.final_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.10.final_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.11.self_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.11.self_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.11.self_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.11.self_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.11.self_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.11.self_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.11.self_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.11.self_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.11.self_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.11.self_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.11.encoder_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.11.encoder_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.11.encoder_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.11.encoder_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.11.encoder_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.11.encoder_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.11.encoder_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.11.encoder_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.11.encoder_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.11.encoder_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.11.fc1.weight', 'ocr_model.decoder.model.decoder.layers.11.fc1.bias', 'ocr_model.decoder.model.decoder.layers.11.fc2.weight', 'ocr_model.decoder.model.decoder.layers.11.fc2.bias', 'ocr_model.decoder.model.decoder.layers.11.final_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.11.final_layer_norm.bias', 'ocr_model.decoder.output_projection.weight', 'is_this_text_a_dialogue.layers.0.weight', 'is_this_text_a_dialogue.layers.0.bias', 'is_this_text_a_dialogue.layers.1.weight', 'is_this_text_a_dialogue.layers.1.bias', 'is_this_text_a_dialogue.layers.2.weight', 'is_this_text_a_dialogue.layers.2.bias', 'text_character_matching_head.layers.0.weight', 'text_character_matching_head.layers.0.bias', 'text_character_matching_head.layers.1.weight', 'text_character_matching_head.layers.1.bias', 'text_character_matching_head.layers.2.weight', 'text_character_matching_head.layers.2.bias'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyMagiModel(\n",
       "  (crop_embedding_model): ViTMAEModel(\n",
       "    (embeddings): ViTMAEEmbeddings(\n",
       "      (patch_embeddings): ViTMAEPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "    )\n",
       "    (encoder): ViTMAEEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTMAELayer(\n",
       "          (attention): ViTMAEAttention(\n",
       "            (attention): ViTMAESelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTMAESelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTMAEIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTMAEOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (detection_transformer): ConditionalDetrModel(\n",
       "    (backbone): ConditionalDetrConvModel(\n",
       "      (conv_encoder): ConditionalDetrConvEncoder(\n",
       "        (model): FeatureListNet(\n",
       "          (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "          (bn1): ConditionalDetrFrozenBatchNorm2d()\n",
       "          (act1): ReLU(inplace=True)\n",
       "          (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "          (layer1): Sequential(\n",
       "            (0): Bottleneck(\n",
       "              (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "              (downsample): Sequential(\n",
       "                (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): ConditionalDetrFrozenBatchNorm2d()\n",
       "              )\n",
       "            )\n",
       "            (1): Bottleneck(\n",
       "              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): Bottleneck(\n",
       "              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (layer2): Sequential(\n",
       "            (0): Bottleneck(\n",
       "              (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "              (bn2): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "              (downsample): Sequential(\n",
       "                (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                (1): ConditionalDetrFrozenBatchNorm2d()\n",
       "              )\n",
       "            )\n",
       "            (1): Bottleneck(\n",
       "              (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): Bottleneck(\n",
       "              (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (3): Bottleneck(\n",
       "              (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (layer3): Sequential(\n",
       "            (0): Bottleneck(\n",
       "              (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "              (bn2): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "              (downsample): Sequential(\n",
       "                (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                (1): ConditionalDetrFrozenBatchNorm2d()\n",
       "              )\n",
       "            )\n",
       "            (1): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (3): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (4): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (5): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (layer4): Sequential(\n",
       "            (0): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "              (bn2): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "              (downsample): Sequential(\n",
       "                (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                (1): ConditionalDetrFrozenBatchNorm2d()\n",
       "              )\n",
       "            )\n",
       "            (1): Bottleneck(\n",
       "              (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): Bottleneck(\n",
       "              (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (position_embedding): ConditionalDetrSinePositionEmbedding()\n",
       "    )\n",
       "    (input_projection): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (query_position_embeddings): Embedding(305, 256)\n",
       "    (encoder): ConditionalDetrEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x ConditionalDetrEncoderLayer(\n",
       "          (self_attn): DetrAttention(\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): ReLU()\n",
       "          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): ConditionalDetrDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): ConditionalDetrDecoderLayer(\n",
       "          (sa_qcontent_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (sa_qpos_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (sa_kcontent_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (sa_kpos_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (sa_v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (self_attn): ConditionalDetrAttention(\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (ca_qcontent_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (ca_qpos_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (ca_kcontent_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (ca_kpos_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (ca_v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (ca_qpos_sine_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (encoder_attn): ConditionalDetrAttention(\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1-5): 5 x ConditionalDetrDecoderLayer(\n",
       "          (sa_qcontent_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (sa_qpos_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (sa_kcontent_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (sa_kpos_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (sa_v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (self_attn): ConditionalDetrAttention(\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (ca_qcontent_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (ca_qpos_proj): None\n",
       "          (ca_kcontent_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (ca_kpos_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (ca_v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (ca_qpos_sine_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (encoder_attn): ConditionalDetrAttention(\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (query_scale): MLP(\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ref_point_head): MLP(\n",
       "        (layers): ModuleList(\n",
       "          (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (1): Linear(in_features=256, out_features=2, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (bbox_predictor): ConditionalDetrMLPPredictionHead(\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "      (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (character_character_matching_head): ConditionalDetrMLPPredictionHead(\n",
       "    (layers): ModuleList(\n",
       "      (0): Linear(in_features=2304, out_features=256, bias=True)\n",
       "      (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (2): Linear(in_features=256, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (class_labels_classifier): Linear(in_features=256, out_features=3, bias=True)\n",
       "  (matcher): ConditionalDetrHungarianMatcher()\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cuda() # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**image_bboxes** - list со вложенными lists, вложенный список - одна страница, где каждый элемент - np.array\n",
    "**character_scores** - list с тензорами, тензор - одна страница, каждый элемент - для каждого bbox score\n",
    "**crop_embeddings_for_batch** - list с тензорами, где каждый тензор - страница, а строка - для каждого bbox\n",
    "**crop_bboxes** - list c тензорами, где каждый тензор - страница, а строка - для каждого bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сериализация тензора PyTorch в байты\n",
    "# Требует доработки\n",
    "pt_tensor_bytes = pt_tensor.numpy().tobytes()\n",
    "\n",
    "# Сериализация массива NumPy в байты\n",
    "np_array_bytes = np_array.tobytes()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание списка с сериализованными байтами\n",
    "# Требует доработки\n",
    "data_list = [\n",
    "    {\"type\": \"pytorch\", \"data\": pt_tensor_bytes},\n",
    "    {\"type\": \"numpy\", \"data\": np_array_bytes}\n",
    "]\n",
    "\n",
    "# Сериализация списка в JSON\n",
    "json_data = json.dumps(data_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_bboxes_bw = []\n",
    "crop_embeddings_for_batch_bw = []\n",
    "image_bboxes_bw = []\n",
    "character_scores_bw = []\n",
    "crop_bboxes_color = []\n",
    "crop_embeddings_for_batch_color = []\n",
    "image_bboxes_color = []\n",
    "character_scores_color = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Тензор с эмбеддингами примеров окраски\n",
    "samples_character = torch.stack([\n",
    "    crop_embeddings_for_batch_color[0][0][7],\n",
    "    crop_embeddings_for_batch_color[0][0][2],\n",
    "    crop_embeddings_for_batch_color[0][0][3],\n",
    "    crop_embeddings_for_batch_color[0][2][0],\n",
    "    crop_embeddings_for_batch_color[0][2][3],\n",
    "    crop_embeddings_for_batch_color[0][3][2],\n",
    "    crop_embeddings_for_batch_color[0][3][3],\n",
    "    crop_embeddings_for_batch_color[0][5][6],\n",
    "    crop_embeddings_for_batch_color[0][9][1],\n",
    "    crop_embeddings_for_batch_color[0][9][2],\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(image_bboxes_color[0][2][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(images_color_grouped[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CropBbox:\n",
    "    image_bbox: np.ndarray # картиночка самого bboxa\n",
    "    character_score: float # не нужен, но есть\n",
    "    embeddings_for_batch: torch.Tensor # эмбеддинг для сравнения\n",
    "    crop_bboxes_for: torch.Tensor # 4 координаты\n",
    "    file_name: str # имя страницы исходной, иначе хрен сравним с раскраской"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_bw_for_everything = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/userr/micromamba/envs/manga311/lib/python3.11/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at /opt/conda/conda-bld/pytorch_1712608853099/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    }
   ],
   "source": [
    "for batch in images_bw:\n",
    "    with torch.no_grad():\n",
    "        # Извлечение массива изображения и названия файла из каждого экземпляра ImageInfo в batch\n",
    "        page_image = [batch.image]\n",
    "        page_name = batch.full_file_name\n",
    "\n",
    "        # Предполагается, что model.get_crops_and_embeddings() может принимать список массивов NumPy\n",
    "        (\n",
    "            batch_crop_bboxes,\n",
    "            batch_crop_embeddings_for_batch,\n",
    "            batch_image_bboxes,\n",
    "            batch_character_scores,\n",
    "        ) = model.get_crops_and_embeddings(page_image)\n",
    "\n",
    "    num_rows = len(batch_crop_embeddings_for_batch[0])\n",
    "\n",
    "    for i in range(num_rows):\n",
    "        images_bw_for_everything.append(\n",
    "            CropBbox(\n",
    "                image_bbox=batch_image_bboxes[0][i],\n",
    "                character_score=batch_character_scores[0][i],\n",
    "                embeddings_for_batch=batch_crop_embeddings_for_batch[0][i],\n",
    "                crop_bboxes_for=batch_crop_bboxes[0][i],\n",
    "                file_name=page_name,\n",
    "            )\n",
    "        )\n",
    "    # crop_bboxes_bw.extend(batch_crop_bboxes)\n",
    "    # crop_embeddings_for_batch_bw.extend(batch_crop_embeddings_for_batch)\n",
    "    # image_bboxes_bw.extend(batch_image_bboxes)\n",
    "    # character_scores_bw.extend(batch_character_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp(file_names_bw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp(image_bboxes_bw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "image_bboxes - list со вложенными lists, вложенный список - одна страница, где каждый элемент - np.array\n",
    "character_scores - list с тензорами, тензор - одна страница, каждый элемент - для каждого bbox score\n",
    "crop_embeddings_for_batch - list с тензорами, где каждый тензор - страница, а строка - для каждого bbox\n",
    "crop_bboxes - list c тензорами, где каждый тензор - страница, а строка - для каждого bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in images_color_grouped:\n",
    "    with torch.no_grad():\n",
    "        (\n",
    "            batch_crop_bboxes,\n",
    "            batch_crop_embeddings_for_batch,\n",
    "            batch_image_bboxes,\n",
    "            batch_character_scores,\n",
    "        ) = model.get_crops_and_embeddings(batch)\n",
    "    crop_bboxes_color.append(batch_crop_bboxes)\n",
    "    crop_embeddings_for_batch_color.append(batch_crop_embeddings_for_batch)\n",
    "    image_bboxes_color.append(batch_image_bboxes)\n",
    "    character_scores_color.append(batch_character_scores)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in images_bw_grouped:\n",
    "    with torch.no_grad():\n",
    "        (\n",
    "            batch_crop_bboxes,\n",
    "            batch_crop_embeddings_for_batch,\n",
    "            batch_image_bboxes,\n",
    "            batch_character_scores,\n",
    "        ) = model.get_crops_and_embeddings(batch)\n",
    "    crop_bboxes_bw.append(batch_crop_bboxes)\n",
    "    crop_embeddings_for_batch_bw.append(batch_crop_embeddings_for_batch)\n",
    "    image_bboxes_bw.append(batch_image_bboxes)\n",
    "    character_scores_bw.append(batch_character_scores)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get matrix per page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проба с максимумом по главе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Список для поиска совпадений с раскрашенными персами\n",
    "compare_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Перебираем все эмбеддинги, которые упакованы по 10 страниц\n",
    "for one_pack in range(len(crop_embeddings_for_batch_bw)):\n",
    "    # Заходим в каждую страницу и объединяем все 10 страниц в батче в один тензор эмбеддингов\n",
    "    for i in range(len(crop_embeddings_for_batch_bw[one_pack])):\n",
    "        if i == 0:\n",
    "            crop_embeds_bw = crop_embeddings_for_batch_bw[one_pack][0]\n",
    "        else:\n",
    "            crop_embeds_bw = torch.cat(\n",
    "                (crop_embeds_bw, crop_embeddings_for_batch_bw[one_pack][i]), dim=0\n",
    "            )\n",
    "\n",
    "    # Матрица с косинусными совпадениями все-на-все\n",
    "    pcs = pairwise_cosine_similarity(crop_embeds_bw, crop_embeds_bw)\n",
    "    # Меняем единицы в главной диагонали на нули\n",
    "    pcs = pcs.fill_diagonal_(0.0)\n",
    "    # Получаем индексы всех максимумов - лучшие совпадения\n",
    "    new_var = torch.argmax(pcs, dim=1)\n",
    "    # Объединяем индексы лучших совпадений друг с другом попарно\n",
    "    char_to = torch.cat(\n",
    "        (new_var.unsqueeze(1), torch.arange(len(new_var)).cuda().unsqueeze(1)), dim=1\n",
    "    )\n",
    "    # Делаем граф из совпадающих вершин\n",
    "    graphs_chapter_one_max = nx.Graph(char_to.tolist())\n",
    "\n",
    "    # Объединяем все совпавшие вершины друг с другом  \n",
    "    indixes_per_chapter = [\n",
    "        list(c_) for c_ in nx.connected_components(graphs_chapter_one_max)\n",
    "    ]\n",
    "\n",
    "    # Создаём список compare_list и добавляем внутри него тензоры\n",
    "    for c_k in indixes_per_chapter:\n",
    "        for character_index in range(len(c_k)):\n",
    "            num = int(c_k[character_index])\n",
    "            if character_index == 0:\n",
    "                first_compare_batch = crop_embeds_bw[num].unsqueeze(dim=0)\n",
    "            else:\n",
    "                first_compare_batch = torch.cat(\n",
    "                (first_compare_batch, crop_embeds_bw[num].unsqueeze(dim=0)), dim=0\n",
    "            )\n",
    "        # Тензор с эмбендингами, внутри каждого тензора строки с эмбеддингами совпавших персонажей        \n",
    "        compare_list.append(first_compare_batch)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_list[0].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сравнение samples со списками"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict = {'key1': 'value1', 'key2': 'value2', 'key3': 'value3'}\n",
    "\n",
    "if my_dict.get('key1') is not None:\n",
    "    print(\"Key exists in the dictionary.\")\n",
    "else:\n",
    "    print(\"Key does not exist in the dictionary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for one_tensor in compare_list:\n",
    "    # Составляем матрицу и сравниваем с самплами\n",
    "    pcs_samples = pairwise_cosine_similarity(one_tensor, samples_character)\n",
    "    # Ищем сумму по всем значениям\n",
    "    comp = torch.sum(pcs_samples, dim=0)\n",
    "    # Ищем индекс максимального совпадения\n",
    "    max_coincidence = int(torch.argmax(comp))\n",
    "    if result_dict.get(max_coincidence) is not None:\n",
    "        inter_res = torch.cat(\n",
    "                (result_dict[max_coincidence], one_tensor), dim=0)\n",
    "        result_dict[max_coincidence] = inter_res\n",
    "    else:\n",
    "        result_dict[max_coincidence] = one_tensor\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp(result_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(image_bboxes_bw[1][1][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_inds = torch.argmax(dict_pcs[0], dim=1)\n",
    "char_to = torch.cat((first_inds.unsqueeze(1), torch.arange(len(first_inds)).cuda().unsqueeze(1)), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_pcs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pcs_without_max = pcs.clone().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first_inds = torch.max(new_pcs_without_, dim=1)\n",
    "first_inds = torch.argmax(new_pcs_without_max, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to = torch.cat((first_inds.unsqueeze(1), torch.arange(len(first_inds)).cuda().unsqueeze(1)), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp(char_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs_chapter_one_max = nx.Graph(char_to.tolist())\n",
    "indixes_per_chapter = [list(c_) for c_ in nx.connected_components(graphs_chapter_one_max)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw_spring(graphs_chapter_one_max, arrows=True, with_labels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проба с двумя максимумами по главе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(crop_embeddings_for_batch)):\n",
    "    if i == 0:\n",
    "        crop_embeds = crop_embeddings_for_batch[i]\n",
    "    else:\n",
    "        crop_embeds = torch.cat((crop_embeds, crop_embeddings_for_batch[i]), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_embeds.shape  # torch.Size([100, 768])\n",
    "pcs = pairwise_cosine_similarity(crop_embeds, crop_embeds)\n",
    "pcs.shape  # torch.Size([100, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pcs_without_ = pcs.clone().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pcs_without_.fill_diagonal_(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first_inds = torch.max(new_pcs_without_, dim=1)\n",
    "first_inds = torch.argmax(new_pcs_without_, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp(first_inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pcs_without_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pcs_without_[torch.arange(new_pcs_without_.shape[0]), first_inds] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pcs_without_[1][35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pcs_without_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pcs_without_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val, indi = torch.topk(new_pcs_without_, 2, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp(indi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp(torch.topk(new_pcs_without_, 2, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_inds = torch.argmax(new_pcs_without_, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp(second_inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_char = torch.stack((first_inds, second_inds), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_char.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_char_last = torch.cat((indi, torch.arange(len(indi)).cuda().unsqueeze(1)), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_char_last.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp(char_to_char_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(char_to_char_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.from_edgelist(char_to_char_last.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_tensor = char_to_char_last.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs_chapter = nx.Graph()\n",
    "\n",
    "# Добавляем вершины и ребра между ними\n",
    "for triplet in lst_tensor:\n",
    "    # Добавляем вершины в граф\n",
    "    for vertex in triplet:\n",
    "        graphs_chapter.add_node(vertex)\n",
    "    \n",
    "    # Добавляем ребра между всеми тремя вершинами\n",
    "    graphs_chapter.add_edge(triplet[0], triplet[1])\n",
    "    graphs_chapter.add_edge(triplet[1], triplet[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indixes_per_chapter = [list(c_) for c_ in nx.connected_components(graphs_chapter)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs_chapter = nx.Graph(char_to_char_last.tolist())\n",
    "indixes_per_chapter = [list(c_) for c_ in nx.connected_components(graphs_chapter)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_per_chapter = [0]*len(graphs_chapter.nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sublist in indixes_per_chapter:\n",
    "        for item in sublist:\n",
    "            class_per_chapter[item] = indixes_per_chapter.index(sublist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw_spring(graphs_chapter, arrows=True, with_labels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AgglomerativeClustering и HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering, HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_agg = AgglomerativeClustering(\n",
    "    n_clusters=None, metric=\"precomputed\", linkage=\"complete\", distance_threshold=0.9\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_agg.fit(pcs.cpu().numpy())\n",
    "len(set(clustering_agg.labels_)), clustering_agg.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = HDBSCAN(metric=\"precomputed\", min_cluster_size=7)\n",
    "clustering.fit(pcs.cpu().numpy())\n",
    "len(set(clustering.labels_)), clustering.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image_bboxes[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image_bboxes[2][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проба с трешхолдом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_character_matching_threshold = 0.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indixes = []\n",
    "in_pr = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for embedding_per_page in crop_embeddings_for_batch:\n",
    "    embedding_for_pairwise = embedding_per_page.cuda()\n",
    "    pcs =  pairwise_cosine_similarity(embedding_for_pairwise, embedding_for_pairwise)\n",
    "    char_i, char_j = torch.where(pcs > character_character_matching_threshold)\n",
    "    character_character_associations = torch.stack([char_i, char_j], dim=1)\n",
    "    graphs = nx.Graph(character_character_associations.tolist())\n",
    "    indixes_per_image = [list(c) for c in nx.connected_components(graphs)]\n",
    "    in_pr.append(indixes_per_image)\n",
    "    class_per_image = [0]*len(graphs.nodes)\n",
    "    for sublist in indixes_per_image:\n",
    "        for item in sublist:\n",
    "            class_per_image[item] = indixes_per_image.index(sublist)\n",
    "    indixes.append(class_per_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw_spring(graphs, arrows=True, with_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(graphs.nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image_bboxes[5][5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
