{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import huggingface_hub\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from anime_segmentation import character_segment as anime_character_segment\n",
    "from anime_segmentation import get_model as get_anime_segmentation_model\n",
    "from diffusers.schedulers import UniPCMultistepScheduler\n",
    "from libs.colouranga.from_magi_model import MyMagiModel\n",
    "from libs.colouranga.from_magi_model.config import MagiConfig\n",
    "from libs.colouranga.from_magi_model.utils import UnionFind\n",
    "from libs.colouranga.from_magi_model.utils import read_image_as_np_array as read_image\n",
    "from libs.colouranga.utils import (\n",
    "    AnalysisSampleImage,\n",
    "    CropBbox,\n",
    "    ImageInfo,\n",
    "    SampleImage,\n",
    "    SampleImageConnection,\n",
    "    creating_pairs,\n",
    "    finding_samples,\n",
    "    get_embeddings,\n",
    "    original_bboxes_compare,\n",
    "    prepreparing_embeddings,\n",
    "    sample_img,\n",
    "    upload_pages,\n",
    ")\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from numpy.typing import NDArray\n",
    "from PIL import Image\n",
    "from rich.pretty import pprint as pp\n",
    "from stable_diffusion_reference_only.pipelines.pipeline_stable_diffusion_reference_only import (\n",
    "    StableDiffusionReferenceOnlyPipeline,\n",
    ")\n",
    "from torchmetrics.functional.pairwise import pairwise_cosine_similarity\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "from transformers.modeling_utils import load_state_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "print(f\"Is CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/userr/MANGA/detection/.pixi/envs/default/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5be088ee9a354dd8ab3bc03ff8511154",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "automatic_coloring_pipeline = StableDiffusionReferenceOnlyPipeline.from_pretrained(\n",
    "    \"AisingioroHao0/stable-diffusion-reference-only-automatic-coloring-0.1.2\"\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "automatic_coloring_pipeline.scheduler = UniPCMultistepScheduler.from_config(\n",
    "    automatic_coloring_pipeline.scheduler.config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_model = get_anime_segmentation_model(\n",
    "    model_path=huggingface_hub.hf_hub_download(\"skytnt/anime-seg\", \"isnetis.ckpt\")\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyMagiModel(\n",
       "  (crop_embedding_model): ViTMAEModel(\n",
       "    (embeddings): ViTMAEEmbeddings(\n",
       "      (patch_embeddings): ViTMAEPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "    )\n",
       "    (encoder): ViTMAEEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTMAELayer(\n",
       "          (attention): ViTMAESdpaAttention(\n",
       "            (attention): ViTMAESdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTMAESelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTMAEIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTMAEOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (detection_transformer): ConditionalDetrModel(\n",
       "    (backbone): ConditionalDetrConvModel(\n",
       "      (conv_encoder): ConditionalDetrConvEncoder(\n",
       "        (model): FeatureListNet(\n",
       "          (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "          (bn1): ConditionalDetrFrozenBatchNorm2d()\n",
       "          (act1): ReLU(inplace=True)\n",
       "          (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "          (layer1): Sequential(\n",
       "            (0): Bottleneck(\n",
       "              (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "              (downsample): Sequential(\n",
       "                (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): ConditionalDetrFrozenBatchNorm2d()\n",
       "              )\n",
       "            )\n",
       "            (1): Bottleneck(\n",
       "              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): Bottleneck(\n",
       "              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (layer2): Sequential(\n",
       "            (0): Bottleneck(\n",
       "              (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "              (bn2): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "              (downsample): Sequential(\n",
       "                (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                (1): ConditionalDetrFrozenBatchNorm2d()\n",
       "              )\n",
       "            )\n",
       "            (1): Bottleneck(\n",
       "              (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): Bottleneck(\n",
       "              (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (3): Bottleneck(\n",
       "              (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (layer3): Sequential(\n",
       "            (0): Bottleneck(\n",
       "              (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "              (bn2): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "              (downsample): Sequential(\n",
       "                (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                (1): ConditionalDetrFrozenBatchNorm2d()\n",
       "              )\n",
       "            )\n",
       "            (1): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (3): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (4): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (5): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (layer4): Sequential(\n",
       "            (0): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "              (bn2): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "              (downsample): Sequential(\n",
       "                (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                (1): ConditionalDetrFrozenBatchNorm2d()\n",
       "              )\n",
       "            )\n",
       "            (1): Bottleneck(\n",
       "              (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): Bottleneck(\n",
       "              (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (position_embedding): ConditionalDetrSinePositionEmbedding()\n",
       "    )\n",
       "    (input_projection): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (query_position_embeddings): Embedding(305, 256)\n",
       "    (encoder): ConditionalDetrEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x ConditionalDetrEncoderLayer(\n",
       "          (self_attn): DetrAttention(\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): ReLU()\n",
       "          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): ConditionalDetrDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): ConditionalDetrDecoderLayer(\n",
       "          (sa_qcontent_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (sa_qpos_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (sa_kcontent_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (sa_kpos_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (sa_v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (self_attn): ConditionalDetrAttention(\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (ca_qcontent_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (ca_qpos_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (ca_kcontent_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (ca_kpos_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (ca_v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (ca_qpos_sine_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (encoder_attn): ConditionalDetrAttention(\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1-5): 5 x ConditionalDetrDecoderLayer(\n",
       "          (sa_qcontent_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (sa_qpos_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (sa_kcontent_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (sa_kpos_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (sa_v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (self_attn): ConditionalDetrAttention(\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (ca_qcontent_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (ca_qpos_proj): None\n",
       "          (ca_kcontent_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (ca_kpos_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (ca_v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (ca_qpos_sine_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (encoder_attn): ConditionalDetrAttention(\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (query_scale): MLP(\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ref_point_head): MLP(\n",
       "        (layers): ModuleList(\n",
       "          (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (1): Linear(in_features=256, out_features=2, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (bbox_predictor): ConditionalDetrMLPPredictionHead(\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "      (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (character_character_matching_head): ConditionalDetrMLPPredictionHead(\n",
       "    (layers): ModuleList(\n",
       "      (0): Linear(in_features=2304, out_features=256, bias=True)\n",
       "      (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (2): Linear(in_features=256, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (class_labels_classifier): Linear(in_features=256, out_features=3, bias=True)\n",
       "  (matcher): ConditionalDetrHungarianMatcher()\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загрузка модели/Model initialization\n",
    "state_dict = load_state_dict(str(Path(\"models/magi/pytorch_model.bin\").resolve()))\n",
    "state_dict.keys()\n",
    "config: MagiConfig = MagiConfig.from_json_file(Path(\"libs/lizi/my_magi/config.json\").resolve())  # type: ignore\n",
    "model = MyMagiModel(config)\n",
    "model.load_state_dict(state_dict, strict=False)\n",
    "model.cuda() # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataclasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing page image as a numpy array and its name\n",
    "@dataclass\n",
    "class ImageInfo:\n",
    "    image: np.ndarray\n",
    "    full_file_name: str\n",
    "\n",
    "    def get_image_array(self):\n",
    "        return self.image\n",
    "\n",
    "\n",
    "# for CropBbox\n",
    "@dataclass\n",
    "class CropBbox:\n",
    "    id_crop_bbox: int\n",
    "    image_bbox: np.ndarray  # image of the bbox itself\n",
    "    character_score: float\n",
    "    embeddings_for_batch: torch.Tensor  # embedding for comparison\n",
    "    crop_bboxes_for: torch.Tensor  # 4 coordinates\n",
    "    file_name: str  # name of the original page\n",
    "\n",
    "\n",
    "# example of the colored image and the name of the whole file with the image for coloring\n",
    "@dataclass\n",
    "class SampleImage:\n",
    "    sample_image: np.ndarray # image \n",
    "    full_file_name: str # name of the original page for later submission to coloring\n",
    "\n",
    "\n",
    "# already transformed image with embedding for comparison\n",
    "@dataclass\n",
    "class AnalisysSampleImage:\n",
    "    sample_image: np.ndarray # image \n",
    "    embeddings_for_batch: torch.Tensor # embedding for comparison\n",
    "    full_file_name: str # name of the original page for later submission to coloring\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SampleImageConnection:\n",
    "    crop_image_bbox: np.ndarray\n",
    "    crop_bboxes_coordinates: torch.Tensor # 4 coordinates\n",
    "    file_page_name: str # name of the original page, otherwise it won't match with coloring\n",
    "    \n",
    "    sample_image: np.ndarray # example of colored image\n",
    "    full_sample_file_name: str # name of the original page for later submission to coloring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uploading images\n",
    "\n",
    "def upload_pages(directory_path_uploading: str) -> list[ImageInfo]:\n",
    "    pages_images = []\n",
    "    for filename in os.listdir(directory_path_uploading):\n",
    "        if filename.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "            full_path = os.path.join(directory_path_uploading, filename)\n",
    "            try:\n",
    "                img = np.asarray(Image.open(full_path).convert(\"RGB\"))\n",
    "                pages_images.append(ImageInfo(image=img, full_file_name=full_path))\n",
    "            except Exception as e:\n",
    "                print(f\"Error when opening {full_path}: {e}\")\n",
    "        else:\n",
    "            print(f\"Incorrect file extension {directory_path_uploading}\")\n",
    "    return pages_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get embeddings\n",
    "def get_embeddings(images_pages: list[ImageInfo]) -> tuple[list[CropBbox], list[torch.Tensor]]:\n",
    "    images_for_everything = []\n",
    "    list_of_embeddings = []\n",
    "    id_number = 0\n",
    "    for batch in images_pages:\n",
    "        with torch.no_grad():\n",
    "            page_image = [batch.image]\n",
    "            page_name = batch.full_file_name\n",
    "\n",
    "            (\n",
    "                batch_crop_bboxes,\n",
    "                batch_crop_embeddings_for_batch,\n",
    "                batch_image_bboxes,\n",
    "                batch_character_scores,\n",
    "            ) = model.get_crops_and_embeddings(page_image)\n",
    "\n",
    "        num_rows = len(batch_crop_embeddings_for_batch[0])\n",
    "\n",
    "        for i in range(num_rows):\n",
    "            images_for_everything.append(\n",
    "                CropBbox(\n",
    "                    id_crop_bbox=i + id_number,\n",
    "                    image_bbox=batch_image_bboxes[0][i],\n",
    "                    character_score=batch_character_scores[0][i],\n",
    "                    embeddings_for_batch=batch_crop_embeddings_for_batch[0][i],\n",
    "                    crop_bboxes_for=batch_crop_bboxes[0][i],\n",
    "                    file_name=page_name,\n",
    "                )\n",
    "            )\n",
    "            list_of_embeddings.append(batch_crop_embeddings_for_batch[0][i])\n",
    "\n",
    "        id_number = id_number + num_rows\n",
    "    return images_for_everything, list_of_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing of embeddings\n",
    "def prepreparing_embeddings(list_of_embedddings: list[torch.Tensor]) -> list[torch.Tensor]:\n",
    "    sublists_for_embedddings = []\n",
    "    for i in range(0, len(list_of_embedddings), 100):\n",
    "        sublist_for_embedddings = list_of_embedddings[i : i + 100]\n",
    "        sublists_for_embedddings.append(sublist_for_embedddings)\n",
    "\n",
    "    # combine into tensors\n",
    "    list_for_analysis = []\n",
    "    for one_list in sublists_for_embedddings:\n",
    "        crop_embedddings = None\n",
    "\n",
    "        for i in range(len(one_list)):\n",
    "            current_embeddings = one_list[i].unsqueeze(dim=0)\n",
    "\n",
    "            if crop_embedddings is None:\n",
    "                crop_embedddings = current_embeddings\n",
    "            else:\n",
    "                crop_embedddings = torch.cat((crop_embedddings, current_embeddings), dim=0)\n",
    "\n",
    "        list_for_analysis.append(crop_embedddings)\n",
    "    return list_for_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding maximum simularity\n",
    "\n",
    "\n",
    "def original_bboxes_compare(list_for_analysis: list[torch.Tensor]) -> list[torch.Tensor]:\n",
    "    compare_list = []\n",
    "    for one_pack_for_analysis in list_for_analysis:\n",
    "        pcs = pairwise_cosine_similarity(one_pack_for_analysis, one_pack_for_analysis)\n",
    "        pcs = pcs.fill_diagonal_(0.0)\n",
    "        new_var = torch.argmax(pcs, dim=1)\n",
    "        char_to = torch.cat(\n",
    "            (new_var.unsqueeze(1), torch.arange(len(new_var)).cuda().unsqueeze(1)), dim=1\n",
    "        )\n",
    "        graphs_chapter_one_max = nx.Graph(char_to.tolist())\n",
    "        indixes_per_chapter = [list(c_) for c_ in nx.connected_components(graphs_chapter_one_max)]\n",
    "        for c_k in indixes_per_chapter:\n",
    "            for character_index in range(len(c_k)):\n",
    "                num = int(c_k[character_index])\n",
    "                if character_index == 0:\n",
    "                    first_compare_batch = one_pack_for_analysis[num].unsqueeze(dim=0)\n",
    "                else:\n",
    "                    first_compare_batch = torch.cat(\n",
    "                        (first_compare_batch, one_pack_for_analysis[num].unsqueeze(dim=0)), dim=0\n",
    "                    )\n",
    "            compare_list.append(first_compare_batch)\n",
    "    return compare_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparation for sample images\n",
    "def sample_img(\n",
    "    directory_path_samples: str,\n",
    ") -> tuple[torch.Tensor, list[AnalisysSampleImage]]:\n",
    "    images_samples = []\n",
    "    for filename in os.listdir(directory_path_samples):\n",
    "        full_path = os.path.join(directory_path_samples, filename)\n",
    "        try:\n",
    "            img = np.asarray(Image.open(full_path).convert(\"RGB\"))\n",
    "            images_samples.append(SampleImage(sample_image=img, full_file_name=full_path))\n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка при открытии {full_path}: {e}\")\n",
    "\n",
    "    images_color_for_analysis = []\n",
    "    list_of_compare_embeddings = []\n",
    "\n",
    "    for batch in images_samples:\n",
    "        with torch.no_grad():\n",
    "            page_image = [batch.sample_image]\n",
    "            page_name = batch.full_file_name\n",
    "\n",
    "            (\n",
    "                batch_crop_bboxes,\n",
    "                batch_crop_embeddings_for_batch,\n",
    "                batch_image_bboxes,\n",
    "                batch_character_scores,\n",
    "            ) = model.get_crops_and_embeddings(page_image)\n",
    "\n",
    "        num_rows = len(batch_crop_embeddings_for_batch[0])\n",
    "\n",
    "        for i in range(num_rows):\n",
    "            images_color_for_analysis.append(\n",
    "                AnalisysSampleImage(\n",
    "                    sample_image=batch_image_bboxes[0][i],\n",
    "                    embeddings_for_batch=batch_crop_embeddings_for_batch[0][i],\n",
    "                    full_file_name=page_name,\n",
    "                )\n",
    "            )\n",
    "            list_of_compare_embeddings.append(batch_crop_embeddings_for_batch[0][i])\n",
    "\n",
    "    crop_embedddings_sample = None\n",
    "\n",
    "    for i in range(len(list_of_compare_embeddings)):\n",
    "        current_embeddings = list_of_compare_embeddings[i].unsqueeze(dim=0)\n",
    "\n",
    "        if crop_embedddings_sample is None:\n",
    "            crop_embedddings_sample = current_embeddings\n",
    "        else:\n",
    "            crop_embedddings_sample = torch.cat(\n",
    "                (crop_embedddings_sample, current_embeddings), dim=0\n",
    "            )\n",
    "    return crop_embedddings_sample, images_color_for_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match color examples embeddings and bboxes embeddings\n",
    "def finding_samples(\n",
    "    crop_embedddings_sample: torch.Tensor, compare_list: list[torch.Tensor]\n",
    ") -> tuple[dict, dict]:\n",
    "    result_dict = {}\n",
    "    sample_dict = {}\n",
    "\n",
    "    for one_tensor in compare_list:\n",
    "        pcs_samples = pairwise_cosine_similarity(one_tensor, crop_embedddings_sample)\n",
    "        comp = torch.sum(pcs_samples, dim=0)\n",
    "        max_coincidence = int(torch.argmax(comp))\n",
    "        if result_dict.get(max_coincidence) is not None:\n",
    "            inter_res = torch.cat((result_dict[max_coincidence], one_tensor), dim=0)\n",
    "            result_dict[max_coincidence] = inter_res\n",
    "        else:\n",
    "            result_dict[max_coincidence] = one_tensor\n",
    "            sample_dict[max_coincidence] = crop_embedddings_sample[max_coincidence]\n",
    "    return result_dict, sample_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating pairs\n",
    "def creating_pairs(\n",
    "    sample_list: list[AnalisysSampleImage],\n",
    "    sample_dict: dict,\n",
    "    original_list: list[CropBbox],\n",
    "    original_dict: dict,\n",
    ") -> list[SampleImageConnection]:\n",
    "    list_for_colorization = []\n",
    "\n",
    "    def compare_tensors(tensor1, tensor2):\n",
    "        return torch.allclose(tensor1, tensor2)\n",
    "\n",
    "    for key, tensor_value in sample_dict.items():\n",
    "        matching_object: AnalisysSampleImage | None = next(\n",
    "            (obj for obj in sample_list if compare_tensors(obj.embeddings_for_batch, tensor_value)),\n",
    "            None,\n",
    "        )\n",
    "        if matching_object:\n",
    "            picture_sample = matching_object.sample_image\n",
    "            picture_name = matching_object.full_file_name\n",
    "\n",
    "        for j in range(original_dict[key].shape[0]):\n",
    "            matching_original: CropBbox | None = next(\n",
    "                (\n",
    "                    obj\n",
    "                    for obj in original_list\n",
    "                    if compare_tensors(obj.embeddings_for_batch, original_dict[key][j])\n",
    "                ),\n",
    "                None,\n",
    "            )\n",
    "            if matching_original:\n",
    "                list_for_colorization.append(\n",
    "                    SampleImageConnection(\n",
    "                        crop_image_bbox=matching_original.image_bbox,\n",
    "                        crop_bboxes_coordinates=matching_original.crop_bboxes_for,\n",
    "                        file_page_name=matching_original.file_name,\n",
    "                        sample_image=picture_sample,\n",
    "                        full_sample_file_name=picture_name,\n",
    "                    )\n",
    "                )\n",
    "    return list_for_colorization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = \"data/x_manga\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path_samples = \"data/ex_samples\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_pages = upload_pages(directory_path)\n",
    "list_of_bboxes, original_emb = get_embeddings(my_pages)\n",
    "list_original_embeddings = prepreparing_embeddings(original_emb)\n",
    "my_comp_list = original_bboxes_compare(list_original_embeddings)\n",
    "my_crop_embedddings_sample, my_images_color_for_analysis = sample_img(directory_path_samples)\n",
    "my_result_dict, my_sample_dict = finding_samples(my_crop_embedddings_sample, my_comp_list)\n",
    "final_character_list = creating_pairs(\n",
    "    my_images_color_for_analysis, my_sample_dict, list_of_bboxes, my_result_dict\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
