{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Пробы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "/home/userr/MANGA/detection/.pixi/envs/default/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so: undefined symbol: cudaLaunchKernelExC, version libcudart.so.11.0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NDArray\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n",
      "File \u001b[0;32m~/MANGA/detection/.pixi/envs/default/lib/python3.11/site-packages/torch/__init__.py:237\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m USE_GLOBAL_DEPS:\n\u001b[1;32m    236\u001b[0m         _load_global_deps()\n\u001b[0;32m--> 237\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;66;03m# Appease the type checker; ordinarily this binding is inserted by the\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;66;03m# torch._C module initialization code in C\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
      "\u001b[0;31mImportError\u001b[0m: /home/userr/MANGA/detection/.pixi/envs/default/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so: undefined symbol: cudaLaunchKernelExC, version libcudart.so.11.0"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from numpy.typing import NDArray\n",
    "from PIL import Image\n",
    "from rich.pretty import pprint as pp\n",
    "from transformers.modeling_utils import load_state_dict\n",
    "from torchmetrics.functional.pairwise import pairwise_cosine_similarity\n",
    "\n",
    "from libs.lizi.my_magi import MyMagiModel\n",
    "from libs.lizi.my_magi.config import MagiConfig\n",
    "from libs.lizi.my_magi.utils import read_image_as_np_array as read_image\n",
    "from libs.lizi.my_magi.utils import UnionFind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загружаем страницы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [read_image(str(image)) for image in Path(\"data/manga3/\").glob(\"*.jpg\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Инициализируем модель"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружаем pre-train веса из файла в dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ocr_model.encoder.embeddings.cls_token', 'ocr_model.encoder.embeddings.position_embeddings', 'ocr_model.encoder.embeddings.patch_embeddings.projection.weight', 'ocr_model.encoder.embeddings.patch_embeddings.projection.bias', 'ocr_model.encoder.encoder.layer.0.attention.attention.query.weight', 'ocr_model.encoder.encoder.layer.0.attention.attention.key.weight', 'ocr_model.encoder.encoder.layer.0.attention.attention.value.weight', 'ocr_model.encoder.encoder.layer.0.attention.output.dense.weight', 'ocr_model.encoder.encoder.layer.0.attention.output.dense.bias', 'ocr_model.encoder.encoder.layer.0.intermediate.dense.weight', 'ocr_model.encoder.encoder.layer.0.intermediate.dense.bias', 'ocr_model.encoder.encoder.layer.0.output.dense.weight', 'ocr_model.encoder.encoder.layer.0.output.dense.bias', 'ocr_model.encoder.encoder.layer.0.layernorm_before.weight', 'ocr_model.encoder.encoder.layer.0.layernorm_before.bias', 'ocr_model.encoder.encoder.layer.0.layernorm_after.weight', 'ocr_model.encoder.encoder.layer.0.layernorm_after.bias', 'ocr_model.encoder.encoder.layer.1.attention.attention.query.weight', 'ocr_model.encoder.encoder.layer.1.attention.attention.key.weight', 'ocr_model.encoder.encoder.layer.1.attention.attention.value.weight', 'ocr_model.encoder.encoder.layer.1.attention.output.dense.weight', 'ocr_model.encoder.encoder.layer.1.attention.output.dense.bias', 'ocr_model.encoder.encoder.layer.1.intermediate.dense.weight', 'ocr_model.encoder.encoder.layer.1.intermediate.dense.bias', 'ocr_model.encoder.encoder.layer.1.output.dense.weight', 'ocr_model.encoder.encoder.layer.1.output.dense.bias', 'ocr_model.encoder.encoder.layer.1.layernorm_before.weight', 'ocr_model.encoder.encoder.layer.1.layernorm_before.bias', 'ocr_model.encoder.encoder.layer.1.layernorm_after.weight', 'ocr_model.encoder.encoder.layer.1.layernorm_after.bias', 'ocr_model.encoder.encoder.layer.2.attention.attention.query.weight', 'ocr_model.encoder.encoder.layer.2.attention.attention.key.weight', 'ocr_model.encoder.encoder.layer.2.attention.attention.value.weight', 'ocr_model.encoder.encoder.layer.2.attention.output.dense.weight', 'ocr_model.encoder.encoder.layer.2.attention.output.dense.bias', 'ocr_model.encoder.encoder.layer.2.intermediate.dense.weight', 'ocr_model.encoder.encoder.layer.2.intermediate.dense.bias', 'ocr_model.encoder.encoder.layer.2.output.dense.weight', 'ocr_model.encoder.encoder.layer.2.output.dense.bias', 'ocr_model.encoder.encoder.layer.2.layernorm_before.weight', 'ocr_model.encoder.encoder.layer.2.layernorm_before.bias', 'ocr_model.encoder.encoder.layer.2.layernorm_after.weight', 'ocr_model.encoder.encoder.layer.2.layernorm_after.bias', 'ocr_model.encoder.encoder.layer.3.attention.attention.query.weight', 'ocr_model.encoder.encoder.layer.3.attention.attention.key.weight', 'ocr_model.encoder.encoder.layer.3.attention.attention.value.weight', 'ocr_model.encoder.encoder.layer.3.attention.output.dense.weight', 'ocr_model.encoder.encoder.layer.3.attention.output.dense.bias', 'ocr_model.encoder.encoder.layer.3.intermediate.dense.weight', 'ocr_model.encoder.encoder.layer.3.intermediate.dense.bias', 'ocr_model.encoder.encoder.layer.3.output.dense.weight', 'ocr_model.encoder.encoder.layer.3.output.dense.bias', 'ocr_model.encoder.encoder.layer.3.layernorm_before.weight', 'ocr_model.encoder.encoder.layer.3.layernorm_before.bias', 'ocr_model.encoder.encoder.layer.3.layernorm_after.weight', 'ocr_model.encoder.encoder.layer.3.layernorm_after.bias', 'ocr_model.encoder.encoder.layer.4.attention.attention.query.weight', 'ocr_model.encoder.encoder.layer.4.attention.attention.key.weight', 'ocr_model.encoder.encoder.layer.4.attention.attention.value.weight', 'ocr_model.encoder.encoder.layer.4.attention.output.dense.weight', 'ocr_model.encoder.encoder.layer.4.attention.output.dense.bias', 'ocr_model.encoder.encoder.layer.4.intermediate.dense.weight', 'ocr_model.encoder.encoder.layer.4.intermediate.dense.bias', 'ocr_model.encoder.encoder.layer.4.output.dense.weight', 'ocr_model.encoder.encoder.layer.4.output.dense.bias', 'ocr_model.encoder.encoder.layer.4.layernorm_before.weight', 'ocr_model.encoder.encoder.layer.4.layernorm_before.bias', 'ocr_model.encoder.encoder.layer.4.layernorm_after.weight', 'ocr_model.encoder.encoder.layer.4.layernorm_after.bias', 'ocr_model.encoder.encoder.layer.5.attention.attention.query.weight', 'ocr_model.encoder.encoder.layer.5.attention.attention.key.weight', 'ocr_model.encoder.encoder.layer.5.attention.attention.value.weight', 'ocr_model.encoder.encoder.layer.5.attention.output.dense.weight', 'ocr_model.encoder.encoder.layer.5.attention.output.dense.bias', 'ocr_model.encoder.encoder.layer.5.intermediate.dense.weight', 'ocr_model.encoder.encoder.layer.5.intermediate.dense.bias', 'ocr_model.encoder.encoder.layer.5.output.dense.weight', 'ocr_model.encoder.encoder.layer.5.output.dense.bias', 'ocr_model.encoder.encoder.layer.5.layernorm_before.weight', 'ocr_model.encoder.encoder.layer.5.layernorm_before.bias', 'ocr_model.encoder.encoder.layer.5.layernorm_after.weight', 'ocr_model.encoder.encoder.layer.5.layernorm_after.bias', 'ocr_model.encoder.encoder.layer.6.attention.attention.query.weight', 'ocr_model.encoder.encoder.layer.6.attention.attention.key.weight', 'ocr_model.encoder.encoder.layer.6.attention.attention.value.weight', 'ocr_model.encoder.encoder.layer.6.attention.output.dense.weight', 'ocr_model.encoder.encoder.layer.6.attention.output.dense.bias', 'ocr_model.encoder.encoder.layer.6.intermediate.dense.weight', 'ocr_model.encoder.encoder.layer.6.intermediate.dense.bias', 'ocr_model.encoder.encoder.layer.6.output.dense.weight', 'ocr_model.encoder.encoder.layer.6.output.dense.bias', 'ocr_model.encoder.encoder.layer.6.layernorm_before.weight', 'ocr_model.encoder.encoder.layer.6.layernorm_before.bias', 'ocr_model.encoder.encoder.layer.6.layernorm_after.weight', 'ocr_model.encoder.encoder.layer.6.layernorm_after.bias', 'ocr_model.encoder.encoder.layer.7.attention.attention.query.weight', 'ocr_model.encoder.encoder.layer.7.attention.attention.key.weight', 'ocr_model.encoder.encoder.layer.7.attention.attention.value.weight', 'ocr_model.encoder.encoder.layer.7.attention.output.dense.weight', 'ocr_model.encoder.encoder.layer.7.attention.output.dense.bias', 'ocr_model.encoder.encoder.layer.7.intermediate.dense.weight', 'ocr_model.encoder.encoder.layer.7.intermediate.dense.bias', 'ocr_model.encoder.encoder.layer.7.output.dense.weight', 'ocr_model.encoder.encoder.layer.7.output.dense.bias', 'ocr_model.encoder.encoder.layer.7.layernorm_before.weight', 'ocr_model.encoder.encoder.layer.7.layernorm_before.bias', 'ocr_model.encoder.encoder.layer.7.layernorm_after.weight', 'ocr_model.encoder.encoder.layer.7.layernorm_after.bias', 'ocr_model.encoder.encoder.layer.8.attention.attention.query.weight', 'ocr_model.encoder.encoder.layer.8.attention.attention.key.weight', 'ocr_model.encoder.encoder.layer.8.attention.attention.value.weight', 'ocr_model.encoder.encoder.layer.8.attention.output.dense.weight', 'ocr_model.encoder.encoder.layer.8.attention.output.dense.bias', 'ocr_model.encoder.encoder.layer.8.intermediate.dense.weight', 'ocr_model.encoder.encoder.layer.8.intermediate.dense.bias', 'ocr_model.encoder.encoder.layer.8.output.dense.weight', 'ocr_model.encoder.encoder.layer.8.output.dense.bias', 'ocr_model.encoder.encoder.layer.8.layernorm_before.weight', 'ocr_model.encoder.encoder.layer.8.layernorm_before.bias', 'ocr_model.encoder.encoder.layer.8.layernorm_after.weight', 'ocr_model.encoder.encoder.layer.8.layernorm_after.bias', 'ocr_model.encoder.encoder.layer.9.attention.attention.query.weight', 'ocr_model.encoder.encoder.layer.9.attention.attention.key.weight', 'ocr_model.encoder.encoder.layer.9.attention.attention.value.weight', 'ocr_model.encoder.encoder.layer.9.attention.output.dense.weight', 'ocr_model.encoder.encoder.layer.9.attention.output.dense.bias', 'ocr_model.encoder.encoder.layer.9.intermediate.dense.weight', 'ocr_model.encoder.encoder.layer.9.intermediate.dense.bias', 'ocr_model.encoder.encoder.layer.9.output.dense.weight', 'ocr_model.encoder.encoder.layer.9.output.dense.bias', 'ocr_model.encoder.encoder.layer.9.layernorm_before.weight', 'ocr_model.encoder.encoder.layer.9.layernorm_before.bias', 'ocr_model.encoder.encoder.layer.9.layernorm_after.weight', 'ocr_model.encoder.encoder.layer.9.layernorm_after.bias', 'ocr_model.encoder.encoder.layer.10.attention.attention.query.weight', 'ocr_model.encoder.encoder.layer.10.attention.attention.key.weight', 'ocr_model.encoder.encoder.layer.10.attention.attention.value.weight', 'ocr_model.encoder.encoder.layer.10.attention.output.dense.weight', 'ocr_model.encoder.encoder.layer.10.attention.output.dense.bias', 'ocr_model.encoder.encoder.layer.10.intermediate.dense.weight', 'ocr_model.encoder.encoder.layer.10.intermediate.dense.bias', 'ocr_model.encoder.encoder.layer.10.output.dense.weight', 'ocr_model.encoder.encoder.layer.10.output.dense.bias', 'ocr_model.encoder.encoder.layer.10.layernorm_before.weight', 'ocr_model.encoder.encoder.layer.10.layernorm_before.bias', 'ocr_model.encoder.encoder.layer.10.layernorm_after.weight', 'ocr_model.encoder.encoder.layer.10.layernorm_after.bias', 'ocr_model.encoder.encoder.layer.11.attention.attention.query.weight', 'ocr_model.encoder.encoder.layer.11.attention.attention.key.weight', 'ocr_model.encoder.encoder.layer.11.attention.attention.value.weight', 'ocr_model.encoder.encoder.layer.11.attention.output.dense.weight', 'ocr_model.encoder.encoder.layer.11.attention.output.dense.bias', 'ocr_model.encoder.encoder.layer.11.intermediate.dense.weight', 'ocr_model.encoder.encoder.layer.11.intermediate.dense.bias', 'ocr_model.encoder.encoder.layer.11.output.dense.weight', 'ocr_model.encoder.encoder.layer.11.output.dense.bias', 'ocr_model.encoder.encoder.layer.11.layernorm_before.weight', 'ocr_model.encoder.encoder.layer.11.layernorm_before.bias', 'ocr_model.encoder.encoder.layer.11.layernorm_after.weight', 'ocr_model.encoder.encoder.layer.11.layernorm_after.bias', 'ocr_model.encoder.layernorm.weight', 'ocr_model.encoder.layernorm.bias', 'ocr_model.encoder.pooler.dense.weight', 'ocr_model.encoder.pooler.dense.bias', 'ocr_model.decoder.model.decoder.embed_tokens.weight', 'ocr_model.decoder.model.decoder.embed_positions.weight', 'ocr_model.decoder.model.decoder.layernorm_embedding.weight', 'ocr_model.decoder.model.decoder.layernorm_embedding.bias', 'ocr_model.decoder.model.decoder.layers.0.self_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.0.self_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.0.self_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.0.self_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.0.self_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.0.self_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.0.self_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.0.self_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.0.self_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.0.self_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.0.encoder_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.0.encoder_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.0.encoder_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.0.encoder_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.0.encoder_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.0.encoder_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.0.encoder_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.0.encoder_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.0.encoder_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.0.encoder_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.0.fc1.weight', 'ocr_model.decoder.model.decoder.layers.0.fc1.bias', 'ocr_model.decoder.model.decoder.layers.0.fc2.weight', 'ocr_model.decoder.model.decoder.layers.0.fc2.bias', 'ocr_model.decoder.model.decoder.layers.0.final_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.0.final_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.1.self_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.1.self_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.1.self_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.1.self_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.1.self_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.1.self_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.1.self_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.1.self_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.1.self_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.1.self_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.1.encoder_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.1.encoder_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.1.encoder_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.1.encoder_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.1.encoder_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.1.encoder_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.1.encoder_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.1.encoder_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.1.encoder_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.1.encoder_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.1.fc1.weight', 'ocr_model.decoder.model.decoder.layers.1.fc1.bias', 'ocr_model.decoder.model.decoder.layers.1.fc2.weight', 'ocr_model.decoder.model.decoder.layers.1.fc2.bias', 'ocr_model.decoder.model.decoder.layers.1.final_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.1.final_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.2.self_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.2.self_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.2.self_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.2.self_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.2.self_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.2.self_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.2.self_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.2.self_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.2.self_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.2.self_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.2.encoder_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.2.encoder_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.2.encoder_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.2.encoder_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.2.encoder_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.2.encoder_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.2.encoder_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.2.encoder_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.2.encoder_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.2.encoder_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.2.fc1.weight', 'ocr_model.decoder.model.decoder.layers.2.fc1.bias', 'ocr_model.decoder.model.decoder.layers.2.fc2.weight', 'ocr_model.decoder.model.decoder.layers.2.fc2.bias', 'ocr_model.decoder.model.decoder.layers.2.final_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.2.final_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.3.self_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.3.self_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.3.self_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.3.self_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.3.self_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.3.self_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.3.self_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.3.self_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.3.self_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.3.self_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.3.encoder_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.3.encoder_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.3.encoder_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.3.encoder_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.3.encoder_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.3.encoder_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.3.encoder_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.3.encoder_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.3.encoder_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.3.encoder_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.3.fc1.weight', 'ocr_model.decoder.model.decoder.layers.3.fc1.bias', 'ocr_model.decoder.model.decoder.layers.3.fc2.weight', 'ocr_model.decoder.model.decoder.layers.3.fc2.bias', 'ocr_model.decoder.model.decoder.layers.3.final_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.3.final_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.4.self_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.4.self_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.4.self_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.4.self_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.4.self_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.4.self_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.4.self_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.4.self_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.4.self_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.4.self_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.4.encoder_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.4.encoder_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.4.encoder_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.4.encoder_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.4.encoder_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.4.encoder_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.4.encoder_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.4.encoder_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.4.encoder_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.4.encoder_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.4.fc1.weight', 'ocr_model.decoder.model.decoder.layers.4.fc1.bias', 'ocr_model.decoder.model.decoder.layers.4.fc2.weight', 'ocr_model.decoder.model.decoder.layers.4.fc2.bias', 'ocr_model.decoder.model.decoder.layers.4.final_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.4.final_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.5.self_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.5.self_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.5.self_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.5.self_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.5.self_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.5.self_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.5.self_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.5.self_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.5.self_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.5.self_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.5.encoder_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.5.encoder_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.5.encoder_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.5.encoder_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.5.encoder_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.5.encoder_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.5.encoder_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.5.encoder_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.5.encoder_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.5.encoder_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.5.fc1.weight', 'ocr_model.decoder.model.decoder.layers.5.fc1.bias', 'ocr_model.decoder.model.decoder.layers.5.fc2.weight', 'ocr_model.decoder.model.decoder.layers.5.fc2.bias', 'ocr_model.decoder.model.decoder.layers.5.final_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.5.final_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.6.self_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.6.self_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.6.self_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.6.self_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.6.self_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.6.self_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.6.self_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.6.self_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.6.self_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.6.self_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.6.encoder_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.6.encoder_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.6.encoder_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.6.encoder_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.6.encoder_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.6.encoder_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.6.encoder_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.6.encoder_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.6.encoder_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.6.encoder_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.6.fc1.weight', 'ocr_model.decoder.model.decoder.layers.6.fc1.bias', 'ocr_model.decoder.model.decoder.layers.6.fc2.weight', 'ocr_model.decoder.model.decoder.layers.6.fc2.bias', 'ocr_model.decoder.model.decoder.layers.6.final_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.6.final_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.7.self_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.7.self_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.7.self_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.7.self_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.7.self_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.7.self_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.7.self_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.7.self_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.7.self_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.7.self_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.7.encoder_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.7.encoder_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.7.encoder_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.7.encoder_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.7.encoder_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.7.encoder_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.7.encoder_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.7.encoder_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.7.encoder_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.7.encoder_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.7.fc1.weight', 'ocr_model.decoder.model.decoder.layers.7.fc1.bias', 'ocr_model.decoder.model.decoder.layers.7.fc2.weight', 'ocr_model.decoder.model.decoder.layers.7.fc2.bias', 'ocr_model.decoder.model.decoder.layers.7.final_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.7.final_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.8.self_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.8.self_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.8.self_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.8.self_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.8.self_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.8.self_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.8.self_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.8.self_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.8.self_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.8.self_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.8.encoder_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.8.encoder_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.8.encoder_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.8.encoder_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.8.encoder_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.8.encoder_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.8.encoder_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.8.encoder_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.8.encoder_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.8.encoder_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.8.fc1.weight', 'ocr_model.decoder.model.decoder.layers.8.fc1.bias', 'ocr_model.decoder.model.decoder.layers.8.fc2.weight', 'ocr_model.decoder.model.decoder.layers.8.fc2.bias', 'ocr_model.decoder.model.decoder.layers.8.final_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.8.final_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.9.self_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.9.self_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.9.self_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.9.self_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.9.self_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.9.self_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.9.self_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.9.self_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.9.self_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.9.self_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.9.encoder_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.9.encoder_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.9.encoder_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.9.encoder_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.9.encoder_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.9.encoder_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.9.encoder_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.9.encoder_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.9.encoder_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.9.encoder_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.9.fc1.weight', 'ocr_model.decoder.model.decoder.layers.9.fc1.bias', 'ocr_model.decoder.model.decoder.layers.9.fc2.weight', 'ocr_model.decoder.model.decoder.layers.9.fc2.bias', 'ocr_model.decoder.model.decoder.layers.9.final_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.9.final_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.10.self_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.10.self_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.10.self_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.10.self_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.10.self_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.10.self_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.10.self_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.10.self_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.10.self_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.10.self_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.10.encoder_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.10.encoder_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.10.encoder_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.10.encoder_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.10.encoder_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.10.encoder_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.10.encoder_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.10.encoder_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.10.encoder_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.10.encoder_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.10.fc1.weight', 'ocr_model.decoder.model.decoder.layers.10.fc1.bias', 'ocr_model.decoder.model.decoder.layers.10.fc2.weight', 'ocr_model.decoder.model.decoder.layers.10.fc2.bias', 'ocr_model.decoder.model.decoder.layers.10.final_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.10.final_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.11.self_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.11.self_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.11.self_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.11.self_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.11.self_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.11.self_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.11.self_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.11.self_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.11.self_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.11.self_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.11.encoder_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.11.encoder_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.11.encoder_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.11.encoder_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.11.encoder_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.11.encoder_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.11.encoder_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.11.encoder_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.11.encoder_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.11.encoder_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.11.fc1.weight', 'ocr_model.decoder.model.decoder.layers.11.fc1.bias', 'ocr_model.decoder.model.decoder.layers.11.fc2.weight', 'ocr_model.decoder.model.decoder.layers.11.fc2.bias', 'ocr_model.decoder.model.decoder.layers.11.final_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.11.final_layer_norm.bias', 'ocr_model.decoder.output_projection.weight', 'crop_embedding_model.embeddings.cls_token', 'crop_embedding_model.embeddings.position_embeddings', 'crop_embedding_model.embeddings.patch_embeddings.projection.weight', 'crop_embedding_model.embeddings.patch_embeddings.projection.bias', 'crop_embedding_model.encoder.layer.0.attention.attention.query.weight', 'crop_embedding_model.encoder.layer.0.attention.attention.query.bias', 'crop_embedding_model.encoder.layer.0.attention.attention.key.weight', 'crop_embedding_model.encoder.layer.0.attention.attention.key.bias', 'crop_embedding_model.encoder.layer.0.attention.attention.value.weight', 'crop_embedding_model.encoder.layer.0.attention.attention.value.bias', 'crop_embedding_model.encoder.layer.0.attention.output.dense.weight', 'crop_embedding_model.encoder.layer.0.attention.output.dense.bias', 'crop_embedding_model.encoder.layer.0.intermediate.dense.weight', 'crop_embedding_model.encoder.layer.0.intermediate.dense.bias', 'crop_embedding_model.encoder.layer.0.output.dense.weight', 'crop_embedding_model.encoder.layer.0.output.dense.bias', 'crop_embedding_model.encoder.layer.0.layernorm_before.weight', 'crop_embedding_model.encoder.layer.0.layernorm_before.bias', 'crop_embedding_model.encoder.layer.0.layernorm_after.weight', 'crop_embedding_model.encoder.layer.0.layernorm_after.bias', 'crop_embedding_model.encoder.layer.1.attention.attention.query.weight', 'crop_embedding_model.encoder.layer.1.attention.attention.query.bias', 'crop_embedding_model.encoder.layer.1.attention.attention.key.weight', 'crop_embedding_model.encoder.layer.1.attention.attention.key.bias', 'crop_embedding_model.encoder.layer.1.attention.attention.value.weight', 'crop_embedding_model.encoder.layer.1.attention.attention.value.bias', 'crop_embedding_model.encoder.layer.1.attention.output.dense.weight', 'crop_embedding_model.encoder.layer.1.attention.output.dense.bias', 'crop_embedding_model.encoder.layer.1.intermediate.dense.weight', 'crop_embedding_model.encoder.layer.1.intermediate.dense.bias', 'crop_embedding_model.encoder.layer.1.output.dense.weight', 'crop_embedding_model.encoder.layer.1.output.dense.bias', 'crop_embedding_model.encoder.layer.1.layernorm_before.weight', 'crop_embedding_model.encoder.layer.1.layernorm_before.bias', 'crop_embedding_model.encoder.layer.1.layernorm_after.weight', 'crop_embedding_model.encoder.layer.1.layernorm_after.bias', 'crop_embedding_model.encoder.layer.2.attention.attention.query.weight', 'crop_embedding_model.encoder.layer.2.attention.attention.query.bias', 'crop_embedding_model.encoder.layer.2.attention.attention.key.weight', 'crop_embedding_model.encoder.layer.2.attention.attention.key.bias', 'crop_embedding_model.encoder.layer.2.attention.attention.value.weight', 'crop_embedding_model.encoder.layer.2.attention.attention.value.bias', 'crop_embedding_model.encoder.layer.2.attention.output.dense.weight', 'crop_embedding_model.encoder.layer.2.attention.output.dense.bias', 'crop_embedding_model.encoder.layer.2.intermediate.dense.weight', 'crop_embedding_model.encoder.layer.2.intermediate.dense.bias', 'crop_embedding_model.encoder.layer.2.output.dense.weight', 'crop_embedding_model.encoder.layer.2.output.dense.bias', 'crop_embedding_model.encoder.layer.2.layernorm_before.weight', 'crop_embedding_model.encoder.layer.2.layernorm_before.bias', 'crop_embedding_model.encoder.layer.2.layernorm_after.weight', 'crop_embedding_model.encoder.layer.2.layernorm_after.bias', 'crop_embedding_model.encoder.layer.3.attention.attention.query.weight', 'crop_embedding_model.encoder.layer.3.attention.attention.query.bias', 'crop_embedding_model.encoder.layer.3.attention.attention.key.weight', 'crop_embedding_model.encoder.layer.3.attention.attention.key.bias', 'crop_embedding_model.encoder.layer.3.attention.attention.value.weight', 'crop_embedding_model.encoder.layer.3.attention.attention.value.bias', 'crop_embedding_model.encoder.layer.3.attention.output.dense.weight', 'crop_embedding_model.encoder.layer.3.attention.output.dense.bias', 'crop_embedding_model.encoder.layer.3.intermediate.dense.weight', 'crop_embedding_model.encoder.layer.3.intermediate.dense.bias', 'crop_embedding_model.encoder.layer.3.output.dense.weight', 'crop_embedding_model.encoder.layer.3.output.dense.bias', 'crop_embedding_model.encoder.layer.3.layernorm_before.weight', 'crop_embedding_model.encoder.layer.3.layernorm_before.bias', 'crop_embedding_model.encoder.layer.3.layernorm_after.weight', 'crop_embedding_model.encoder.layer.3.layernorm_after.bias', 'crop_embedding_model.encoder.layer.4.attention.attention.query.weight', 'crop_embedding_model.encoder.layer.4.attention.attention.query.bias', 'crop_embedding_model.encoder.layer.4.attention.attention.key.weight', 'crop_embedding_model.encoder.layer.4.attention.attention.key.bias', 'crop_embedding_model.encoder.layer.4.attention.attention.value.weight', 'crop_embedding_model.encoder.layer.4.attention.attention.value.bias', 'crop_embedding_model.encoder.layer.4.attention.output.dense.weight', 'crop_embedding_model.encoder.layer.4.attention.output.dense.bias', 'crop_embedding_model.encoder.layer.4.intermediate.dense.weight', 'crop_embedding_model.encoder.layer.4.intermediate.dense.bias', 'crop_embedding_model.encoder.layer.4.output.dense.weight', 'crop_embedding_model.encoder.layer.4.output.dense.bias', 'crop_embedding_model.encoder.layer.4.layernorm_before.weight', 'crop_embedding_model.encoder.layer.4.layernorm_before.bias', 'crop_embedding_model.encoder.layer.4.layernorm_after.weight', 'crop_embedding_model.encoder.layer.4.layernorm_after.bias', 'crop_embedding_model.encoder.layer.5.attention.attention.query.weight', 'crop_embedding_model.encoder.layer.5.attention.attention.query.bias', 'crop_embedding_model.encoder.layer.5.attention.attention.key.weight', 'crop_embedding_model.encoder.layer.5.attention.attention.key.bias', 'crop_embedding_model.encoder.layer.5.attention.attention.value.weight', 'crop_embedding_model.encoder.layer.5.attention.attention.value.bias', 'crop_embedding_model.encoder.layer.5.attention.output.dense.weight', 'crop_embedding_model.encoder.layer.5.attention.output.dense.bias', 'crop_embedding_model.encoder.layer.5.intermediate.dense.weight', 'crop_embedding_model.encoder.layer.5.intermediate.dense.bias', 'crop_embedding_model.encoder.layer.5.output.dense.weight', 'crop_embedding_model.encoder.layer.5.output.dense.bias', 'crop_embedding_model.encoder.layer.5.layernorm_before.weight', 'crop_embedding_model.encoder.layer.5.layernorm_before.bias', 'crop_embedding_model.encoder.layer.5.layernorm_after.weight', 'crop_embedding_model.encoder.layer.5.layernorm_after.bias', 'crop_embedding_model.encoder.layer.6.attention.attention.query.weight', 'crop_embedding_model.encoder.layer.6.attention.attention.query.bias', 'crop_embedding_model.encoder.layer.6.attention.attention.key.weight', 'crop_embedding_model.encoder.layer.6.attention.attention.key.bias', 'crop_embedding_model.encoder.layer.6.attention.attention.value.weight', 'crop_embedding_model.encoder.layer.6.attention.attention.value.bias', 'crop_embedding_model.encoder.layer.6.attention.output.dense.weight', 'crop_embedding_model.encoder.layer.6.attention.output.dense.bias', 'crop_embedding_model.encoder.layer.6.intermediate.dense.weight', 'crop_embedding_model.encoder.layer.6.intermediate.dense.bias', 'crop_embedding_model.encoder.layer.6.output.dense.weight', 'crop_embedding_model.encoder.layer.6.output.dense.bias', 'crop_embedding_model.encoder.layer.6.layernorm_before.weight', 'crop_embedding_model.encoder.layer.6.layernorm_before.bias', 'crop_embedding_model.encoder.layer.6.layernorm_after.weight', 'crop_embedding_model.encoder.layer.6.layernorm_after.bias', 'crop_embedding_model.encoder.layer.7.attention.attention.query.weight', 'crop_embedding_model.encoder.layer.7.attention.attention.query.bias', 'crop_embedding_model.encoder.layer.7.attention.attention.key.weight', 'crop_embedding_model.encoder.layer.7.attention.attention.key.bias', 'crop_embedding_model.encoder.layer.7.attention.attention.value.weight', 'crop_embedding_model.encoder.layer.7.attention.attention.value.bias', 'crop_embedding_model.encoder.layer.7.attention.output.dense.weight', 'crop_embedding_model.encoder.layer.7.attention.output.dense.bias', 'crop_embedding_model.encoder.layer.7.intermediate.dense.weight', 'crop_embedding_model.encoder.layer.7.intermediate.dense.bias', 'crop_embedding_model.encoder.layer.7.output.dense.weight', 'crop_embedding_model.encoder.layer.7.output.dense.bias', 'crop_embedding_model.encoder.layer.7.layernorm_before.weight', 'crop_embedding_model.encoder.layer.7.layernorm_before.bias', 'crop_embedding_model.encoder.layer.7.layernorm_after.weight', 'crop_embedding_model.encoder.layer.7.layernorm_after.bias', 'crop_embedding_model.encoder.layer.8.attention.attention.query.weight', 'crop_embedding_model.encoder.layer.8.attention.attention.query.bias', 'crop_embedding_model.encoder.layer.8.attention.attention.key.weight', 'crop_embedding_model.encoder.layer.8.attention.attention.key.bias', 'crop_embedding_model.encoder.layer.8.attention.attention.value.weight', 'crop_embedding_model.encoder.layer.8.attention.attention.value.bias', 'crop_embedding_model.encoder.layer.8.attention.output.dense.weight', 'crop_embedding_model.encoder.layer.8.attention.output.dense.bias', 'crop_embedding_model.encoder.layer.8.intermediate.dense.weight', 'crop_embedding_model.encoder.layer.8.intermediate.dense.bias', 'crop_embedding_model.encoder.layer.8.output.dense.weight', 'crop_embedding_model.encoder.layer.8.output.dense.bias', 'crop_embedding_model.encoder.layer.8.layernorm_before.weight', 'crop_embedding_model.encoder.layer.8.layernorm_before.bias', 'crop_embedding_model.encoder.layer.8.layernorm_after.weight', 'crop_embedding_model.encoder.layer.8.layernorm_after.bias', 'crop_embedding_model.encoder.layer.9.attention.attention.query.weight', 'crop_embedding_model.encoder.layer.9.attention.attention.query.bias', 'crop_embedding_model.encoder.layer.9.attention.attention.key.weight', 'crop_embedding_model.encoder.layer.9.attention.attention.key.bias', 'crop_embedding_model.encoder.layer.9.attention.attention.value.weight', 'crop_embedding_model.encoder.layer.9.attention.attention.value.bias', 'crop_embedding_model.encoder.layer.9.attention.output.dense.weight', 'crop_embedding_model.encoder.layer.9.attention.output.dense.bias', 'crop_embedding_model.encoder.layer.9.intermediate.dense.weight', 'crop_embedding_model.encoder.layer.9.intermediate.dense.bias', 'crop_embedding_model.encoder.layer.9.output.dense.weight', 'crop_embedding_model.encoder.layer.9.output.dense.bias', 'crop_embedding_model.encoder.layer.9.layernorm_before.weight', 'crop_embedding_model.encoder.layer.9.layernorm_before.bias', 'crop_embedding_model.encoder.layer.9.layernorm_after.weight', 'crop_embedding_model.encoder.layer.9.layernorm_after.bias', 'crop_embedding_model.encoder.layer.10.attention.attention.query.weight', 'crop_embedding_model.encoder.layer.10.attention.attention.query.bias', 'crop_embedding_model.encoder.layer.10.attention.attention.key.weight', 'crop_embedding_model.encoder.layer.10.attention.attention.key.bias', 'crop_embedding_model.encoder.layer.10.attention.attention.value.weight', 'crop_embedding_model.encoder.layer.10.attention.attention.value.bias', 'crop_embedding_model.encoder.layer.10.attention.output.dense.weight', 'crop_embedding_model.encoder.layer.10.attention.output.dense.bias', 'crop_embedding_model.encoder.layer.10.intermediate.dense.weight', 'crop_embedding_model.encoder.layer.10.intermediate.dense.bias', 'crop_embedding_model.encoder.layer.10.output.dense.weight', 'crop_embedding_model.encoder.layer.10.output.dense.bias', 'crop_embedding_model.encoder.layer.10.layernorm_before.weight', 'crop_embedding_model.encoder.layer.10.layernorm_before.bias', 'crop_embedding_model.encoder.layer.10.layernorm_after.weight', 'crop_embedding_model.encoder.layer.10.layernorm_after.bias', 'crop_embedding_model.encoder.layer.11.attention.attention.query.weight', 'crop_embedding_model.encoder.layer.11.attention.attention.query.bias', 'crop_embedding_model.encoder.layer.11.attention.attention.key.weight', 'crop_embedding_model.encoder.layer.11.attention.attention.key.bias', 'crop_embedding_model.encoder.layer.11.attention.attention.value.weight', 'crop_embedding_model.encoder.layer.11.attention.attention.value.bias', 'crop_embedding_model.encoder.layer.11.attention.output.dense.weight', 'crop_embedding_model.encoder.layer.11.attention.output.dense.bias', 'crop_embedding_model.encoder.layer.11.intermediate.dense.weight', 'crop_embedding_model.encoder.layer.11.intermediate.dense.bias', 'crop_embedding_model.encoder.layer.11.output.dense.weight', 'crop_embedding_model.encoder.layer.11.output.dense.bias', 'crop_embedding_model.encoder.layer.11.layernorm_before.weight', 'crop_embedding_model.encoder.layer.11.layernorm_before.bias', 'crop_embedding_model.encoder.layer.11.layernorm_after.weight', 'crop_embedding_model.encoder.layer.11.layernorm_after.bias', 'crop_embedding_model.layernorm.weight', 'crop_embedding_model.layernorm.bias', 'detection_transformer.backbone.conv_encoder.model.conv1.weight', 'detection_transformer.backbone.conv_encoder.model.bn1.weight', 'detection_transformer.backbone.conv_encoder.model.bn1.bias', 'detection_transformer.backbone.conv_encoder.model.bn1.running_mean', 'detection_transformer.backbone.conv_encoder.model.bn1.running_var', 'detection_transformer.backbone.conv_encoder.model.layer1.0.conv1.weight', 'detection_transformer.backbone.conv_encoder.model.layer1.0.bn1.weight', 'detection_transformer.backbone.conv_encoder.model.layer1.0.bn1.bias', 'detection_transformer.backbone.conv_encoder.model.layer1.0.bn1.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer1.0.bn1.running_var', 'detection_transformer.backbone.conv_encoder.model.layer1.0.conv2.weight', 'detection_transformer.backbone.conv_encoder.model.layer1.0.bn2.weight', 'detection_transformer.backbone.conv_encoder.model.layer1.0.bn2.bias', 'detection_transformer.backbone.conv_encoder.model.layer1.0.bn2.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer1.0.bn2.running_var', 'detection_transformer.backbone.conv_encoder.model.layer1.0.conv3.weight', 'detection_transformer.backbone.conv_encoder.model.layer1.0.bn3.weight', 'detection_transformer.backbone.conv_encoder.model.layer1.0.bn3.bias', 'detection_transformer.backbone.conv_encoder.model.layer1.0.bn3.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer1.0.bn3.running_var', 'detection_transformer.backbone.conv_encoder.model.layer1.0.downsample.0.weight', 'detection_transformer.backbone.conv_encoder.model.layer1.0.downsample.1.weight', 'detection_transformer.backbone.conv_encoder.model.layer1.0.downsample.1.bias', 'detection_transformer.backbone.conv_encoder.model.layer1.0.downsample.1.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer1.0.downsample.1.running_var', 'detection_transformer.backbone.conv_encoder.model.layer1.1.conv1.weight', 'detection_transformer.backbone.conv_encoder.model.layer1.1.bn1.weight', 'detection_transformer.backbone.conv_encoder.model.layer1.1.bn1.bias', 'detection_transformer.backbone.conv_encoder.model.layer1.1.bn1.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer1.1.bn1.running_var', 'detection_transformer.backbone.conv_encoder.model.layer1.1.conv2.weight', 'detection_transformer.backbone.conv_encoder.model.layer1.1.bn2.weight', 'detection_transformer.backbone.conv_encoder.model.layer1.1.bn2.bias', 'detection_transformer.backbone.conv_encoder.model.layer1.1.bn2.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer1.1.bn2.running_var', 'detection_transformer.backbone.conv_encoder.model.layer1.1.conv3.weight', 'detection_transformer.backbone.conv_encoder.model.layer1.1.bn3.weight', 'detection_transformer.backbone.conv_encoder.model.layer1.1.bn3.bias', 'detection_transformer.backbone.conv_encoder.model.layer1.1.bn3.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer1.1.bn3.running_var', 'detection_transformer.backbone.conv_encoder.model.layer1.2.conv1.weight', 'detection_transformer.backbone.conv_encoder.model.layer1.2.bn1.weight', 'detection_transformer.backbone.conv_encoder.model.layer1.2.bn1.bias', 'detection_transformer.backbone.conv_encoder.model.layer1.2.bn1.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer1.2.bn1.running_var', 'detection_transformer.backbone.conv_encoder.model.layer1.2.conv2.weight', 'detection_transformer.backbone.conv_encoder.model.layer1.2.bn2.weight', 'detection_transformer.backbone.conv_encoder.model.layer1.2.bn2.bias', 'detection_transformer.backbone.conv_encoder.model.layer1.2.bn2.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer1.2.bn2.running_var', 'detection_transformer.backbone.conv_encoder.model.layer1.2.conv3.weight', 'detection_transformer.backbone.conv_encoder.model.layer1.2.bn3.weight', 'detection_transformer.backbone.conv_encoder.model.layer1.2.bn3.bias', 'detection_transformer.backbone.conv_encoder.model.layer1.2.bn3.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer1.2.bn3.running_var', 'detection_transformer.backbone.conv_encoder.model.layer2.0.conv1.weight', 'detection_transformer.backbone.conv_encoder.model.layer2.0.bn1.weight', 'detection_transformer.backbone.conv_encoder.model.layer2.0.bn1.bias', 'detection_transformer.backbone.conv_encoder.model.layer2.0.bn1.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer2.0.bn1.running_var', 'detection_transformer.backbone.conv_encoder.model.layer2.0.conv2.weight', 'detection_transformer.backbone.conv_encoder.model.layer2.0.bn2.weight', 'detection_transformer.backbone.conv_encoder.model.layer2.0.bn2.bias', 'detection_transformer.backbone.conv_encoder.model.layer2.0.bn2.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer2.0.bn2.running_var', 'detection_transformer.backbone.conv_encoder.model.layer2.0.conv3.weight', 'detection_transformer.backbone.conv_encoder.model.layer2.0.bn3.weight', 'detection_transformer.backbone.conv_encoder.model.layer2.0.bn3.bias', 'detection_transformer.backbone.conv_encoder.model.layer2.0.bn3.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer2.0.bn3.running_var', 'detection_transformer.backbone.conv_encoder.model.layer2.0.downsample.0.weight', 'detection_transformer.backbone.conv_encoder.model.layer2.0.downsample.1.weight', 'detection_transformer.backbone.conv_encoder.model.layer2.0.downsample.1.bias', 'detection_transformer.backbone.conv_encoder.model.layer2.0.downsample.1.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer2.0.downsample.1.running_var', 'detection_transformer.backbone.conv_encoder.model.layer2.1.conv1.weight', 'detection_transformer.backbone.conv_encoder.model.layer2.1.bn1.weight', 'detection_transformer.backbone.conv_encoder.model.layer2.1.bn1.bias', 'detection_transformer.backbone.conv_encoder.model.layer2.1.bn1.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer2.1.bn1.running_var', 'detection_transformer.backbone.conv_encoder.model.layer2.1.conv2.weight', 'detection_transformer.backbone.conv_encoder.model.layer2.1.bn2.weight', 'detection_transformer.backbone.conv_encoder.model.layer2.1.bn2.bias', 'detection_transformer.backbone.conv_encoder.model.layer2.1.bn2.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer2.1.bn2.running_var', 'detection_transformer.backbone.conv_encoder.model.layer2.1.conv3.weight', 'detection_transformer.backbone.conv_encoder.model.layer2.1.bn3.weight', 'detection_transformer.backbone.conv_encoder.model.layer2.1.bn3.bias', 'detection_transformer.backbone.conv_encoder.model.layer2.1.bn3.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer2.1.bn3.running_var', 'detection_transformer.backbone.conv_encoder.model.layer2.2.conv1.weight', 'detection_transformer.backbone.conv_encoder.model.layer2.2.bn1.weight', 'detection_transformer.backbone.conv_encoder.model.layer2.2.bn1.bias', 'detection_transformer.backbone.conv_encoder.model.layer2.2.bn1.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer2.2.bn1.running_var', 'detection_transformer.backbone.conv_encoder.model.layer2.2.conv2.weight', 'detection_transformer.backbone.conv_encoder.model.layer2.2.bn2.weight', 'detection_transformer.backbone.conv_encoder.model.layer2.2.bn2.bias', 'detection_transformer.backbone.conv_encoder.model.layer2.2.bn2.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer2.2.bn2.running_var', 'detection_transformer.backbone.conv_encoder.model.layer2.2.conv3.weight', 'detection_transformer.backbone.conv_encoder.model.layer2.2.bn3.weight', 'detection_transformer.backbone.conv_encoder.model.layer2.2.bn3.bias', 'detection_transformer.backbone.conv_encoder.model.layer2.2.bn3.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer2.2.bn3.running_var', 'detection_transformer.backbone.conv_encoder.model.layer2.3.conv1.weight', 'detection_transformer.backbone.conv_encoder.model.layer2.3.bn1.weight', 'detection_transformer.backbone.conv_encoder.model.layer2.3.bn1.bias', 'detection_transformer.backbone.conv_encoder.model.layer2.3.bn1.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer2.3.bn1.running_var', 'detection_transformer.backbone.conv_encoder.model.layer2.3.conv2.weight', 'detection_transformer.backbone.conv_encoder.model.layer2.3.bn2.weight', 'detection_transformer.backbone.conv_encoder.model.layer2.3.bn2.bias', 'detection_transformer.backbone.conv_encoder.model.layer2.3.bn2.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer2.3.bn2.running_var', 'detection_transformer.backbone.conv_encoder.model.layer2.3.conv3.weight', 'detection_transformer.backbone.conv_encoder.model.layer2.3.bn3.weight', 'detection_transformer.backbone.conv_encoder.model.layer2.3.bn3.bias', 'detection_transformer.backbone.conv_encoder.model.layer2.3.bn3.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer2.3.bn3.running_var', 'detection_transformer.backbone.conv_encoder.model.layer3.0.conv1.weight', 'detection_transformer.backbone.conv_encoder.model.layer3.0.bn1.weight', 'detection_transformer.backbone.conv_encoder.model.layer3.0.bn1.bias', 'detection_transformer.backbone.conv_encoder.model.layer3.0.bn1.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer3.0.bn1.running_var', 'detection_transformer.backbone.conv_encoder.model.layer3.0.conv2.weight', 'detection_transformer.backbone.conv_encoder.model.layer3.0.bn2.weight', 'detection_transformer.backbone.conv_encoder.model.layer3.0.bn2.bias', 'detection_transformer.backbone.conv_encoder.model.layer3.0.bn2.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer3.0.bn2.running_var', 'detection_transformer.backbone.conv_encoder.model.layer3.0.conv3.weight', 'detection_transformer.backbone.conv_encoder.model.layer3.0.bn3.weight', 'detection_transformer.backbone.conv_encoder.model.layer3.0.bn3.bias', 'detection_transformer.backbone.conv_encoder.model.layer3.0.bn3.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer3.0.bn3.running_var', 'detection_transformer.backbone.conv_encoder.model.layer3.0.downsample.0.weight', 'detection_transformer.backbone.conv_encoder.model.layer3.0.downsample.1.weight', 'detection_transformer.backbone.conv_encoder.model.layer3.0.downsample.1.bias', 'detection_transformer.backbone.conv_encoder.model.layer3.0.downsample.1.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer3.0.downsample.1.running_var', 'detection_transformer.backbone.conv_encoder.model.layer3.1.conv1.weight', 'detection_transformer.backbone.conv_encoder.model.layer3.1.bn1.weight', 'detection_transformer.backbone.conv_encoder.model.layer3.1.bn1.bias', 'detection_transformer.backbone.conv_encoder.model.layer3.1.bn1.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer3.1.bn1.running_var', 'detection_transformer.backbone.conv_encoder.model.layer3.1.conv2.weight', 'detection_transformer.backbone.conv_encoder.model.layer3.1.bn2.weight', 'detection_transformer.backbone.conv_encoder.model.layer3.1.bn2.bias', 'detection_transformer.backbone.conv_encoder.model.layer3.1.bn2.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer3.1.bn2.running_var', 'detection_transformer.backbone.conv_encoder.model.layer3.1.conv3.weight', 'detection_transformer.backbone.conv_encoder.model.layer3.1.bn3.weight', 'detection_transformer.backbone.conv_encoder.model.layer3.1.bn3.bias', 'detection_transformer.backbone.conv_encoder.model.layer3.1.bn3.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer3.1.bn3.running_var', 'detection_transformer.backbone.conv_encoder.model.layer3.2.conv1.weight', 'detection_transformer.backbone.conv_encoder.model.layer3.2.bn1.weight', 'detection_transformer.backbone.conv_encoder.model.layer3.2.bn1.bias', 'detection_transformer.backbone.conv_encoder.model.layer3.2.bn1.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer3.2.bn1.running_var', 'detection_transformer.backbone.conv_encoder.model.layer3.2.conv2.weight', 'detection_transformer.backbone.conv_encoder.model.layer3.2.bn2.weight', 'detection_transformer.backbone.conv_encoder.model.layer3.2.bn2.bias', 'detection_transformer.backbone.conv_encoder.model.layer3.2.bn2.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer3.2.bn2.running_var', 'detection_transformer.backbone.conv_encoder.model.layer3.2.conv3.weight', 'detection_transformer.backbone.conv_encoder.model.layer3.2.bn3.weight', 'detection_transformer.backbone.conv_encoder.model.layer3.2.bn3.bias', 'detection_transformer.backbone.conv_encoder.model.layer3.2.bn3.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer3.2.bn3.running_var', 'detection_transformer.backbone.conv_encoder.model.layer3.3.conv1.weight', 'detection_transformer.backbone.conv_encoder.model.layer3.3.bn1.weight', 'detection_transformer.backbone.conv_encoder.model.layer3.3.bn1.bias', 'detection_transformer.backbone.conv_encoder.model.layer3.3.bn1.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer3.3.bn1.running_var', 'detection_transformer.backbone.conv_encoder.model.layer3.3.conv2.weight', 'detection_transformer.backbone.conv_encoder.model.layer3.3.bn2.weight', 'detection_transformer.backbone.conv_encoder.model.layer3.3.bn2.bias', 'detection_transformer.backbone.conv_encoder.model.layer3.3.bn2.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer3.3.bn2.running_var', 'detection_transformer.backbone.conv_encoder.model.layer3.3.conv3.weight', 'detection_transformer.backbone.conv_encoder.model.layer3.3.bn3.weight', 'detection_transformer.backbone.conv_encoder.model.layer3.3.bn3.bias', 'detection_transformer.backbone.conv_encoder.model.layer3.3.bn3.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer3.3.bn3.running_var', 'detection_transformer.backbone.conv_encoder.model.layer3.4.conv1.weight', 'detection_transformer.backbone.conv_encoder.model.layer3.4.bn1.weight', 'detection_transformer.backbone.conv_encoder.model.layer3.4.bn1.bias', 'detection_transformer.backbone.conv_encoder.model.layer3.4.bn1.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer3.4.bn1.running_var', 'detection_transformer.backbone.conv_encoder.model.layer3.4.conv2.weight', 'detection_transformer.backbone.conv_encoder.model.layer3.4.bn2.weight', 'detection_transformer.backbone.conv_encoder.model.layer3.4.bn2.bias', 'detection_transformer.backbone.conv_encoder.model.layer3.4.bn2.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer3.4.bn2.running_var', 'detection_transformer.backbone.conv_encoder.model.layer3.4.conv3.weight', 'detection_transformer.backbone.conv_encoder.model.layer3.4.bn3.weight', 'detection_transformer.backbone.conv_encoder.model.layer3.4.bn3.bias', 'detection_transformer.backbone.conv_encoder.model.layer3.4.bn3.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer3.4.bn3.running_var', 'detection_transformer.backbone.conv_encoder.model.layer3.5.conv1.weight', 'detection_transformer.backbone.conv_encoder.model.layer3.5.bn1.weight', 'detection_transformer.backbone.conv_encoder.model.layer3.5.bn1.bias', 'detection_transformer.backbone.conv_encoder.model.layer3.5.bn1.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer3.5.bn1.running_var', 'detection_transformer.backbone.conv_encoder.model.layer3.5.conv2.weight', 'detection_transformer.backbone.conv_encoder.model.layer3.5.bn2.weight', 'detection_transformer.backbone.conv_encoder.model.layer3.5.bn2.bias', 'detection_transformer.backbone.conv_encoder.model.layer3.5.bn2.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer3.5.bn2.running_var', 'detection_transformer.backbone.conv_encoder.model.layer3.5.conv3.weight', 'detection_transformer.backbone.conv_encoder.model.layer3.5.bn3.weight', 'detection_transformer.backbone.conv_encoder.model.layer3.5.bn3.bias', 'detection_transformer.backbone.conv_encoder.model.layer3.5.bn3.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer3.5.bn3.running_var', 'detection_transformer.backbone.conv_encoder.model.layer4.0.conv1.weight', 'detection_transformer.backbone.conv_encoder.model.layer4.0.bn1.weight', 'detection_transformer.backbone.conv_encoder.model.layer4.0.bn1.bias', 'detection_transformer.backbone.conv_encoder.model.layer4.0.bn1.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer4.0.bn1.running_var', 'detection_transformer.backbone.conv_encoder.model.layer4.0.conv2.weight', 'detection_transformer.backbone.conv_encoder.model.layer4.0.bn2.weight', 'detection_transformer.backbone.conv_encoder.model.layer4.0.bn2.bias', 'detection_transformer.backbone.conv_encoder.model.layer4.0.bn2.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer4.0.bn2.running_var', 'detection_transformer.backbone.conv_encoder.model.layer4.0.conv3.weight', 'detection_transformer.backbone.conv_encoder.model.layer4.0.bn3.weight', 'detection_transformer.backbone.conv_encoder.model.layer4.0.bn3.bias', 'detection_transformer.backbone.conv_encoder.model.layer4.0.bn3.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer4.0.bn3.running_var', 'detection_transformer.backbone.conv_encoder.model.layer4.0.downsample.0.weight', 'detection_transformer.backbone.conv_encoder.model.layer4.0.downsample.1.weight', 'detection_transformer.backbone.conv_encoder.model.layer4.0.downsample.1.bias', 'detection_transformer.backbone.conv_encoder.model.layer4.0.downsample.1.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer4.0.downsample.1.running_var', 'detection_transformer.backbone.conv_encoder.model.layer4.1.conv1.weight', 'detection_transformer.backbone.conv_encoder.model.layer4.1.bn1.weight', 'detection_transformer.backbone.conv_encoder.model.layer4.1.bn1.bias', 'detection_transformer.backbone.conv_encoder.model.layer4.1.bn1.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer4.1.bn1.running_var', 'detection_transformer.backbone.conv_encoder.model.layer4.1.conv2.weight', 'detection_transformer.backbone.conv_encoder.model.layer4.1.bn2.weight', 'detection_transformer.backbone.conv_encoder.model.layer4.1.bn2.bias', 'detection_transformer.backbone.conv_encoder.model.layer4.1.bn2.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer4.1.bn2.running_var', 'detection_transformer.backbone.conv_encoder.model.layer4.1.conv3.weight', 'detection_transformer.backbone.conv_encoder.model.layer4.1.bn3.weight', 'detection_transformer.backbone.conv_encoder.model.layer4.1.bn3.bias', 'detection_transformer.backbone.conv_encoder.model.layer4.1.bn3.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer4.1.bn3.running_var', 'detection_transformer.backbone.conv_encoder.model.layer4.2.conv1.weight', 'detection_transformer.backbone.conv_encoder.model.layer4.2.bn1.weight', 'detection_transformer.backbone.conv_encoder.model.layer4.2.bn1.bias', 'detection_transformer.backbone.conv_encoder.model.layer4.2.bn1.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer4.2.bn1.running_var', 'detection_transformer.backbone.conv_encoder.model.layer4.2.conv2.weight', 'detection_transformer.backbone.conv_encoder.model.layer4.2.bn2.weight', 'detection_transformer.backbone.conv_encoder.model.layer4.2.bn2.bias', 'detection_transformer.backbone.conv_encoder.model.layer4.2.bn2.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer4.2.bn2.running_var', 'detection_transformer.backbone.conv_encoder.model.layer4.2.conv3.weight', 'detection_transformer.backbone.conv_encoder.model.layer4.2.bn3.weight', 'detection_transformer.backbone.conv_encoder.model.layer4.2.bn3.bias', 'detection_transformer.backbone.conv_encoder.model.layer4.2.bn3.running_mean', 'detection_transformer.backbone.conv_encoder.model.layer4.2.bn3.running_var', 'detection_transformer.input_projection.weight', 'detection_transformer.input_projection.bias', 'detection_transformer.query_position_embeddings.weight', 'detection_transformer.encoder.layers.0.self_attn.k_proj.weight', 'detection_transformer.encoder.layers.0.self_attn.k_proj.bias', 'detection_transformer.encoder.layers.0.self_attn.v_proj.weight', 'detection_transformer.encoder.layers.0.self_attn.v_proj.bias', 'detection_transformer.encoder.layers.0.self_attn.q_proj.weight', 'detection_transformer.encoder.layers.0.self_attn.q_proj.bias', 'detection_transformer.encoder.layers.0.self_attn.out_proj.weight', 'detection_transformer.encoder.layers.0.self_attn.out_proj.bias', 'detection_transformer.encoder.layers.0.self_attn_layer_norm.weight', 'detection_transformer.encoder.layers.0.self_attn_layer_norm.bias', 'detection_transformer.encoder.layers.0.fc1.weight', 'detection_transformer.encoder.layers.0.fc1.bias', 'detection_transformer.encoder.layers.0.fc2.weight', 'detection_transformer.encoder.layers.0.fc2.bias', 'detection_transformer.encoder.layers.0.final_layer_norm.weight', 'detection_transformer.encoder.layers.0.final_layer_norm.bias', 'detection_transformer.encoder.layers.1.self_attn.k_proj.weight', 'detection_transformer.encoder.layers.1.self_attn.k_proj.bias', 'detection_transformer.encoder.layers.1.self_attn.v_proj.weight', 'detection_transformer.encoder.layers.1.self_attn.v_proj.bias', 'detection_transformer.encoder.layers.1.self_attn.q_proj.weight', 'detection_transformer.encoder.layers.1.self_attn.q_proj.bias', 'detection_transformer.encoder.layers.1.self_attn.out_proj.weight', 'detection_transformer.encoder.layers.1.self_attn.out_proj.bias', 'detection_transformer.encoder.layers.1.self_attn_layer_norm.weight', 'detection_transformer.encoder.layers.1.self_attn_layer_norm.bias', 'detection_transformer.encoder.layers.1.fc1.weight', 'detection_transformer.encoder.layers.1.fc1.bias', 'detection_transformer.encoder.layers.1.fc2.weight', 'detection_transformer.encoder.layers.1.fc2.bias', 'detection_transformer.encoder.layers.1.final_layer_norm.weight', 'detection_transformer.encoder.layers.1.final_layer_norm.bias', 'detection_transformer.encoder.layers.2.self_attn.k_proj.weight', 'detection_transformer.encoder.layers.2.self_attn.k_proj.bias', 'detection_transformer.encoder.layers.2.self_attn.v_proj.weight', 'detection_transformer.encoder.layers.2.self_attn.v_proj.bias', 'detection_transformer.encoder.layers.2.self_attn.q_proj.weight', 'detection_transformer.encoder.layers.2.self_attn.q_proj.bias', 'detection_transformer.encoder.layers.2.self_attn.out_proj.weight', 'detection_transformer.encoder.layers.2.self_attn.out_proj.bias', 'detection_transformer.encoder.layers.2.self_attn_layer_norm.weight', 'detection_transformer.encoder.layers.2.self_attn_layer_norm.bias', 'detection_transformer.encoder.layers.2.fc1.weight', 'detection_transformer.encoder.layers.2.fc1.bias', 'detection_transformer.encoder.layers.2.fc2.weight', 'detection_transformer.encoder.layers.2.fc2.bias', 'detection_transformer.encoder.layers.2.final_layer_norm.weight', 'detection_transformer.encoder.layers.2.final_layer_norm.bias', 'detection_transformer.encoder.layers.3.self_attn.k_proj.weight', 'detection_transformer.encoder.layers.3.self_attn.k_proj.bias', 'detection_transformer.encoder.layers.3.self_attn.v_proj.weight', 'detection_transformer.encoder.layers.3.self_attn.v_proj.bias', 'detection_transformer.encoder.layers.3.self_attn.q_proj.weight', 'detection_transformer.encoder.layers.3.self_attn.q_proj.bias', 'detection_transformer.encoder.layers.3.self_attn.out_proj.weight', 'detection_transformer.encoder.layers.3.self_attn.out_proj.bias', 'detection_transformer.encoder.layers.3.self_attn_layer_norm.weight', 'detection_transformer.encoder.layers.3.self_attn_layer_norm.bias', 'detection_transformer.encoder.layers.3.fc1.weight', 'detection_transformer.encoder.layers.3.fc1.bias', 'detection_transformer.encoder.layers.3.fc2.weight', 'detection_transformer.encoder.layers.3.fc2.bias', 'detection_transformer.encoder.layers.3.final_layer_norm.weight', 'detection_transformer.encoder.layers.3.final_layer_norm.bias', 'detection_transformer.encoder.layers.4.self_attn.k_proj.weight', 'detection_transformer.encoder.layers.4.self_attn.k_proj.bias', 'detection_transformer.encoder.layers.4.self_attn.v_proj.weight', 'detection_transformer.encoder.layers.4.self_attn.v_proj.bias', 'detection_transformer.encoder.layers.4.self_attn.q_proj.weight', 'detection_transformer.encoder.layers.4.self_attn.q_proj.bias', 'detection_transformer.encoder.layers.4.self_attn.out_proj.weight', 'detection_transformer.encoder.layers.4.self_attn.out_proj.bias', 'detection_transformer.encoder.layers.4.self_attn_layer_norm.weight', 'detection_transformer.encoder.layers.4.self_attn_layer_norm.bias', 'detection_transformer.encoder.layers.4.fc1.weight', 'detection_transformer.encoder.layers.4.fc1.bias', 'detection_transformer.encoder.layers.4.fc2.weight', 'detection_transformer.encoder.layers.4.fc2.bias', 'detection_transformer.encoder.layers.4.final_layer_norm.weight', 'detection_transformer.encoder.layers.4.final_layer_norm.bias', 'detection_transformer.encoder.layers.5.self_attn.k_proj.weight', 'detection_transformer.encoder.layers.5.self_attn.k_proj.bias', 'detection_transformer.encoder.layers.5.self_attn.v_proj.weight', 'detection_transformer.encoder.layers.5.self_attn.v_proj.bias', 'detection_transformer.encoder.layers.5.self_attn.q_proj.weight', 'detection_transformer.encoder.layers.5.self_attn.q_proj.bias', 'detection_transformer.encoder.layers.5.self_attn.out_proj.weight', 'detection_transformer.encoder.layers.5.self_attn.out_proj.bias', 'detection_transformer.encoder.layers.5.self_attn_layer_norm.weight', 'detection_transformer.encoder.layers.5.self_attn_layer_norm.bias', 'detection_transformer.encoder.layers.5.fc1.weight', 'detection_transformer.encoder.layers.5.fc1.bias', 'detection_transformer.encoder.layers.5.fc2.weight', 'detection_transformer.encoder.layers.5.fc2.bias', 'detection_transformer.encoder.layers.5.final_layer_norm.weight', 'detection_transformer.encoder.layers.5.final_layer_norm.bias', 'detection_transformer.decoder.layers.0.sa_qcontent_proj.weight', 'detection_transformer.decoder.layers.0.sa_qcontent_proj.bias', 'detection_transformer.decoder.layers.0.sa_qpos_proj.weight', 'detection_transformer.decoder.layers.0.sa_qpos_proj.bias', 'detection_transformer.decoder.layers.0.sa_kcontent_proj.weight', 'detection_transformer.decoder.layers.0.sa_kcontent_proj.bias', 'detection_transformer.decoder.layers.0.sa_kpos_proj.weight', 'detection_transformer.decoder.layers.0.sa_kpos_proj.bias', 'detection_transformer.decoder.layers.0.sa_v_proj.weight', 'detection_transformer.decoder.layers.0.sa_v_proj.bias', 'detection_transformer.decoder.layers.0.self_attn.out_proj.weight', 'detection_transformer.decoder.layers.0.self_attn.out_proj.bias', 'detection_transformer.decoder.layers.0.self_attn_layer_norm.weight', 'detection_transformer.decoder.layers.0.self_attn_layer_norm.bias', 'detection_transformer.decoder.layers.0.ca_qcontent_proj.weight', 'detection_transformer.decoder.layers.0.ca_qcontent_proj.bias', 'detection_transformer.decoder.layers.0.ca_qpos_proj.weight', 'detection_transformer.decoder.layers.0.ca_qpos_proj.bias', 'detection_transformer.decoder.layers.0.ca_kcontent_proj.weight', 'detection_transformer.decoder.layers.0.ca_kcontent_proj.bias', 'detection_transformer.decoder.layers.0.ca_kpos_proj.weight', 'detection_transformer.decoder.layers.0.ca_kpos_proj.bias', 'detection_transformer.decoder.layers.0.ca_v_proj.weight', 'detection_transformer.decoder.layers.0.ca_v_proj.bias', 'detection_transformer.decoder.layers.0.ca_qpos_sine_proj.weight', 'detection_transformer.decoder.layers.0.ca_qpos_sine_proj.bias', 'detection_transformer.decoder.layers.0.encoder_attn.out_proj.weight', 'detection_transformer.decoder.layers.0.encoder_attn.out_proj.bias', 'detection_transformer.decoder.layers.0.encoder_attn_layer_norm.weight', 'detection_transformer.decoder.layers.0.encoder_attn_layer_norm.bias', 'detection_transformer.decoder.layers.0.fc1.weight', 'detection_transformer.decoder.layers.0.fc1.bias', 'detection_transformer.decoder.layers.0.fc2.weight', 'detection_transformer.decoder.layers.0.fc2.bias', 'detection_transformer.decoder.layers.0.final_layer_norm.weight', 'detection_transformer.decoder.layers.0.final_layer_norm.bias', 'detection_transformer.decoder.layers.1.sa_qcontent_proj.weight', 'detection_transformer.decoder.layers.1.sa_qcontent_proj.bias', 'detection_transformer.decoder.layers.1.sa_qpos_proj.weight', 'detection_transformer.decoder.layers.1.sa_qpos_proj.bias', 'detection_transformer.decoder.layers.1.sa_kcontent_proj.weight', 'detection_transformer.decoder.layers.1.sa_kcontent_proj.bias', 'detection_transformer.decoder.layers.1.sa_kpos_proj.weight', 'detection_transformer.decoder.layers.1.sa_kpos_proj.bias', 'detection_transformer.decoder.layers.1.sa_v_proj.weight', 'detection_transformer.decoder.layers.1.sa_v_proj.bias', 'detection_transformer.decoder.layers.1.self_attn.out_proj.weight', 'detection_transformer.decoder.layers.1.self_attn.out_proj.bias', 'detection_transformer.decoder.layers.1.self_attn_layer_norm.weight', 'detection_transformer.decoder.layers.1.self_attn_layer_norm.bias', 'detection_transformer.decoder.layers.1.ca_qcontent_proj.weight', 'detection_transformer.decoder.layers.1.ca_qcontent_proj.bias', 'detection_transformer.decoder.layers.1.ca_kcontent_proj.weight', 'detection_transformer.decoder.layers.1.ca_kcontent_proj.bias', 'detection_transformer.decoder.layers.1.ca_kpos_proj.weight', 'detection_transformer.decoder.layers.1.ca_kpos_proj.bias', 'detection_transformer.decoder.layers.1.ca_v_proj.weight', 'detection_transformer.decoder.layers.1.ca_v_proj.bias', 'detection_transformer.decoder.layers.1.ca_qpos_sine_proj.weight', 'detection_transformer.decoder.layers.1.ca_qpos_sine_proj.bias', 'detection_transformer.decoder.layers.1.encoder_attn.out_proj.weight', 'detection_transformer.decoder.layers.1.encoder_attn.out_proj.bias', 'detection_transformer.decoder.layers.1.encoder_attn_layer_norm.weight', 'detection_transformer.decoder.layers.1.encoder_attn_layer_norm.bias', 'detection_transformer.decoder.layers.1.fc1.weight', 'detection_transformer.decoder.layers.1.fc1.bias', 'detection_transformer.decoder.layers.1.fc2.weight', 'detection_transformer.decoder.layers.1.fc2.bias', 'detection_transformer.decoder.layers.1.final_layer_norm.weight', 'detection_transformer.decoder.layers.1.final_layer_norm.bias', 'detection_transformer.decoder.layers.2.sa_qcontent_proj.weight', 'detection_transformer.decoder.layers.2.sa_qcontent_proj.bias', 'detection_transformer.decoder.layers.2.sa_qpos_proj.weight', 'detection_transformer.decoder.layers.2.sa_qpos_proj.bias', 'detection_transformer.decoder.layers.2.sa_kcontent_proj.weight', 'detection_transformer.decoder.layers.2.sa_kcontent_proj.bias', 'detection_transformer.decoder.layers.2.sa_kpos_proj.weight', 'detection_transformer.decoder.layers.2.sa_kpos_proj.bias', 'detection_transformer.decoder.layers.2.sa_v_proj.weight', 'detection_transformer.decoder.layers.2.sa_v_proj.bias', 'detection_transformer.decoder.layers.2.self_attn.out_proj.weight', 'detection_transformer.decoder.layers.2.self_attn.out_proj.bias', 'detection_transformer.decoder.layers.2.self_attn_layer_norm.weight', 'detection_transformer.decoder.layers.2.self_attn_layer_norm.bias', 'detection_transformer.decoder.layers.2.ca_qcontent_proj.weight', 'detection_transformer.decoder.layers.2.ca_qcontent_proj.bias', 'detection_transformer.decoder.layers.2.ca_kcontent_proj.weight', 'detection_transformer.decoder.layers.2.ca_kcontent_proj.bias', 'detection_transformer.decoder.layers.2.ca_kpos_proj.weight', 'detection_transformer.decoder.layers.2.ca_kpos_proj.bias', 'detection_transformer.decoder.layers.2.ca_v_proj.weight', 'detection_transformer.decoder.layers.2.ca_v_proj.bias', 'detection_transformer.decoder.layers.2.ca_qpos_sine_proj.weight', 'detection_transformer.decoder.layers.2.ca_qpos_sine_proj.bias', 'detection_transformer.decoder.layers.2.encoder_attn.out_proj.weight', 'detection_transformer.decoder.layers.2.encoder_attn.out_proj.bias', 'detection_transformer.decoder.layers.2.encoder_attn_layer_norm.weight', 'detection_transformer.decoder.layers.2.encoder_attn_layer_norm.bias', 'detection_transformer.decoder.layers.2.fc1.weight', 'detection_transformer.decoder.layers.2.fc1.bias', 'detection_transformer.decoder.layers.2.fc2.weight', 'detection_transformer.decoder.layers.2.fc2.bias', 'detection_transformer.decoder.layers.2.final_layer_norm.weight', 'detection_transformer.decoder.layers.2.final_layer_norm.bias', 'detection_transformer.decoder.layers.3.sa_qcontent_proj.weight', 'detection_transformer.decoder.layers.3.sa_qcontent_proj.bias', 'detection_transformer.decoder.layers.3.sa_qpos_proj.weight', 'detection_transformer.decoder.layers.3.sa_qpos_proj.bias', 'detection_transformer.decoder.layers.3.sa_kcontent_proj.weight', 'detection_transformer.decoder.layers.3.sa_kcontent_proj.bias', 'detection_transformer.decoder.layers.3.sa_kpos_proj.weight', 'detection_transformer.decoder.layers.3.sa_kpos_proj.bias', 'detection_transformer.decoder.layers.3.sa_v_proj.weight', 'detection_transformer.decoder.layers.3.sa_v_proj.bias', 'detection_transformer.decoder.layers.3.self_attn.out_proj.weight', 'detection_transformer.decoder.layers.3.self_attn.out_proj.bias', 'detection_transformer.decoder.layers.3.self_attn_layer_norm.weight', 'detection_transformer.decoder.layers.3.self_attn_layer_norm.bias', 'detection_transformer.decoder.layers.3.ca_qcontent_proj.weight', 'detection_transformer.decoder.layers.3.ca_qcontent_proj.bias', 'detection_transformer.decoder.layers.3.ca_kcontent_proj.weight', 'detection_transformer.decoder.layers.3.ca_kcontent_proj.bias', 'detection_transformer.decoder.layers.3.ca_kpos_proj.weight', 'detection_transformer.decoder.layers.3.ca_kpos_proj.bias', 'detection_transformer.decoder.layers.3.ca_v_proj.weight', 'detection_transformer.decoder.layers.3.ca_v_proj.bias', 'detection_transformer.decoder.layers.3.ca_qpos_sine_proj.weight', 'detection_transformer.decoder.layers.3.ca_qpos_sine_proj.bias', 'detection_transformer.decoder.layers.3.encoder_attn.out_proj.weight', 'detection_transformer.decoder.layers.3.encoder_attn.out_proj.bias', 'detection_transformer.decoder.layers.3.encoder_attn_layer_norm.weight', 'detection_transformer.decoder.layers.3.encoder_attn_layer_norm.bias', 'detection_transformer.decoder.layers.3.fc1.weight', 'detection_transformer.decoder.layers.3.fc1.bias', 'detection_transformer.decoder.layers.3.fc2.weight', 'detection_transformer.decoder.layers.3.fc2.bias', 'detection_transformer.decoder.layers.3.final_layer_norm.weight', 'detection_transformer.decoder.layers.3.final_layer_norm.bias', 'detection_transformer.decoder.layers.4.sa_qcontent_proj.weight', 'detection_transformer.decoder.layers.4.sa_qcontent_proj.bias', 'detection_transformer.decoder.layers.4.sa_qpos_proj.weight', 'detection_transformer.decoder.layers.4.sa_qpos_proj.bias', 'detection_transformer.decoder.layers.4.sa_kcontent_proj.weight', 'detection_transformer.decoder.layers.4.sa_kcontent_proj.bias', 'detection_transformer.decoder.layers.4.sa_kpos_proj.weight', 'detection_transformer.decoder.layers.4.sa_kpos_proj.bias', 'detection_transformer.decoder.layers.4.sa_v_proj.weight', 'detection_transformer.decoder.layers.4.sa_v_proj.bias', 'detection_transformer.decoder.layers.4.self_attn.out_proj.weight', 'detection_transformer.decoder.layers.4.self_attn.out_proj.bias', 'detection_transformer.decoder.layers.4.self_attn_layer_norm.weight', 'detection_transformer.decoder.layers.4.self_attn_layer_norm.bias', 'detection_transformer.decoder.layers.4.ca_qcontent_proj.weight', 'detection_transformer.decoder.layers.4.ca_qcontent_proj.bias', 'detection_transformer.decoder.layers.4.ca_kcontent_proj.weight', 'detection_transformer.decoder.layers.4.ca_kcontent_proj.bias', 'detection_transformer.decoder.layers.4.ca_kpos_proj.weight', 'detection_transformer.decoder.layers.4.ca_kpos_proj.bias', 'detection_transformer.decoder.layers.4.ca_v_proj.weight', 'detection_transformer.decoder.layers.4.ca_v_proj.bias', 'detection_transformer.decoder.layers.4.ca_qpos_sine_proj.weight', 'detection_transformer.decoder.layers.4.ca_qpos_sine_proj.bias', 'detection_transformer.decoder.layers.4.encoder_attn.out_proj.weight', 'detection_transformer.decoder.layers.4.encoder_attn.out_proj.bias', 'detection_transformer.decoder.layers.4.encoder_attn_layer_norm.weight', 'detection_transformer.decoder.layers.4.encoder_attn_layer_norm.bias', 'detection_transformer.decoder.layers.4.fc1.weight', 'detection_transformer.decoder.layers.4.fc1.bias', 'detection_transformer.decoder.layers.4.fc2.weight', 'detection_transformer.decoder.layers.4.fc2.bias', 'detection_transformer.decoder.layers.4.final_layer_norm.weight', 'detection_transformer.decoder.layers.4.final_layer_norm.bias', 'detection_transformer.decoder.layers.5.sa_qcontent_proj.weight', 'detection_transformer.decoder.layers.5.sa_qcontent_proj.bias', 'detection_transformer.decoder.layers.5.sa_qpos_proj.weight', 'detection_transformer.decoder.layers.5.sa_qpos_proj.bias', 'detection_transformer.decoder.layers.5.sa_kcontent_proj.weight', 'detection_transformer.decoder.layers.5.sa_kcontent_proj.bias', 'detection_transformer.decoder.layers.5.sa_kpos_proj.weight', 'detection_transformer.decoder.layers.5.sa_kpos_proj.bias', 'detection_transformer.decoder.layers.5.sa_v_proj.weight', 'detection_transformer.decoder.layers.5.sa_v_proj.bias', 'detection_transformer.decoder.layers.5.self_attn.out_proj.weight', 'detection_transformer.decoder.layers.5.self_attn.out_proj.bias', 'detection_transformer.decoder.layers.5.self_attn_layer_norm.weight', 'detection_transformer.decoder.layers.5.self_attn_layer_norm.bias', 'detection_transformer.decoder.layers.5.ca_qcontent_proj.weight', 'detection_transformer.decoder.layers.5.ca_qcontent_proj.bias', 'detection_transformer.decoder.layers.5.ca_kcontent_proj.weight', 'detection_transformer.decoder.layers.5.ca_kcontent_proj.bias', 'detection_transformer.decoder.layers.5.ca_kpos_proj.weight', 'detection_transformer.decoder.layers.5.ca_kpos_proj.bias', 'detection_transformer.decoder.layers.5.ca_v_proj.weight', 'detection_transformer.decoder.layers.5.ca_v_proj.bias', 'detection_transformer.decoder.layers.5.ca_qpos_sine_proj.weight', 'detection_transformer.decoder.layers.5.ca_qpos_sine_proj.bias', 'detection_transformer.decoder.layers.5.encoder_attn.out_proj.weight', 'detection_transformer.decoder.layers.5.encoder_attn.out_proj.bias', 'detection_transformer.decoder.layers.5.encoder_attn_layer_norm.weight', 'detection_transformer.decoder.layers.5.encoder_attn_layer_norm.bias', 'detection_transformer.decoder.layers.5.fc1.weight', 'detection_transformer.decoder.layers.5.fc1.bias', 'detection_transformer.decoder.layers.5.fc2.weight', 'detection_transformer.decoder.layers.5.fc2.bias', 'detection_transformer.decoder.layers.5.final_layer_norm.weight', 'detection_transformer.decoder.layers.5.final_layer_norm.bias', 'detection_transformer.decoder.layernorm.weight', 'detection_transformer.decoder.layernorm.bias', 'detection_transformer.decoder.query_scale.layers.0.weight', 'detection_transformer.decoder.query_scale.layers.0.bias', 'detection_transformer.decoder.query_scale.layers.1.weight', 'detection_transformer.decoder.query_scale.layers.1.bias', 'detection_transformer.decoder.ref_point_head.layers.0.weight', 'detection_transformer.decoder.ref_point_head.layers.0.bias', 'detection_transformer.decoder.ref_point_head.layers.1.weight', 'detection_transformer.decoder.ref_point_head.layers.1.bias', 'bbox_predictor.layers.0.weight', 'bbox_predictor.layers.0.bias', 'bbox_predictor.layers.1.weight', 'bbox_predictor.layers.1.bias', 'bbox_predictor.layers.2.weight', 'bbox_predictor.layers.2.bias', 'is_this_text_a_dialogue.layers.0.weight', 'is_this_text_a_dialogue.layers.0.bias', 'is_this_text_a_dialogue.layers.1.weight', 'is_this_text_a_dialogue.layers.1.bias', 'is_this_text_a_dialogue.layers.2.weight', 'is_this_text_a_dialogue.layers.2.bias', 'character_character_matching_head.layers.0.weight', 'character_character_matching_head.layers.0.bias', 'character_character_matching_head.layers.1.weight', 'character_character_matching_head.layers.1.bias', 'character_character_matching_head.layers.2.weight', 'character_character_matching_head.layers.2.bias', 'text_character_matching_head.layers.0.weight', 'text_character_matching_head.layers.0.bias', 'text_character_matching_head.layers.1.weight', 'text_character_matching_head.layers.1.bias', 'text_character_matching_head.layers.2.weight', 'text_character_matching_head.layers.2.bias', 'class_labels_classifier.weight', 'class_labels_classifier.bias'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = load_state_dict(str(Path(\"models/magi/pytorch_model.bin\").resolve()))\n",
    "state_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Считываем конфиг из локальной директории и инициализируем нашу модель с ним"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config: MagiConfig = MagiConfig.from_json_file(Path(\"libs/lizi/my_magi/config.json\").resolve())  # type: ignore\n",
    "model = MyMagiModel(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружаем pre-train веса в модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=[], unexpected_keys=['ocr_model.encoder.embeddings.cls_token', 'ocr_model.encoder.embeddings.position_embeddings', 'ocr_model.encoder.embeddings.patch_embeddings.projection.weight', 'ocr_model.encoder.embeddings.patch_embeddings.projection.bias', 'ocr_model.encoder.encoder.layer.0.attention.attention.query.weight', 'ocr_model.encoder.encoder.layer.0.attention.attention.key.weight', 'ocr_model.encoder.encoder.layer.0.attention.attention.value.weight', 'ocr_model.encoder.encoder.layer.0.attention.output.dense.weight', 'ocr_model.encoder.encoder.layer.0.attention.output.dense.bias', 'ocr_model.encoder.encoder.layer.0.intermediate.dense.weight', 'ocr_model.encoder.encoder.layer.0.intermediate.dense.bias', 'ocr_model.encoder.encoder.layer.0.output.dense.weight', 'ocr_model.encoder.encoder.layer.0.output.dense.bias', 'ocr_model.encoder.encoder.layer.0.layernorm_before.weight', 'ocr_model.encoder.encoder.layer.0.layernorm_before.bias', 'ocr_model.encoder.encoder.layer.0.layernorm_after.weight', 'ocr_model.encoder.encoder.layer.0.layernorm_after.bias', 'ocr_model.encoder.encoder.layer.1.attention.attention.query.weight', 'ocr_model.encoder.encoder.layer.1.attention.attention.key.weight', 'ocr_model.encoder.encoder.layer.1.attention.attention.value.weight', 'ocr_model.encoder.encoder.layer.1.attention.output.dense.weight', 'ocr_model.encoder.encoder.layer.1.attention.output.dense.bias', 'ocr_model.encoder.encoder.layer.1.intermediate.dense.weight', 'ocr_model.encoder.encoder.layer.1.intermediate.dense.bias', 'ocr_model.encoder.encoder.layer.1.output.dense.weight', 'ocr_model.encoder.encoder.layer.1.output.dense.bias', 'ocr_model.encoder.encoder.layer.1.layernorm_before.weight', 'ocr_model.encoder.encoder.layer.1.layernorm_before.bias', 'ocr_model.encoder.encoder.layer.1.layernorm_after.weight', 'ocr_model.encoder.encoder.layer.1.layernorm_after.bias', 'ocr_model.encoder.encoder.layer.2.attention.attention.query.weight', 'ocr_model.encoder.encoder.layer.2.attention.attention.key.weight', 'ocr_model.encoder.encoder.layer.2.attention.attention.value.weight', 'ocr_model.encoder.encoder.layer.2.attention.output.dense.weight', 'ocr_model.encoder.encoder.layer.2.attention.output.dense.bias', 'ocr_model.encoder.encoder.layer.2.intermediate.dense.weight', 'ocr_model.encoder.encoder.layer.2.intermediate.dense.bias', 'ocr_model.encoder.encoder.layer.2.output.dense.weight', 'ocr_model.encoder.encoder.layer.2.output.dense.bias', 'ocr_model.encoder.encoder.layer.2.layernorm_before.weight', 'ocr_model.encoder.encoder.layer.2.layernorm_before.bias', 'ocr_model.encoder.encoder.layer.2.layernorm_after.weight', 'ocr_model.encoder.encoder.layer.2.layernorm_after.bias', 'ocr_model.encoder.encoder.layer.3.attention.attention.query.weight', 'ocr_model.encoder.encoder.layer.3.attention.attention.key.weight', 'ocr_model.encoder.encoder.layer.3.attention.attention.value.weight', 'ocr_model.encoder.encoder.layer.3.attention.output.dense.weight', 'ocr_model.encoder.encoder.layer.3.attention.output.dense.bias', 'ocr_model.encoder.encoder.layer.3.intermediate.dense.weight', 'ocr_model.encoder.encoder.layer.3.intermediate.dense.bias', 'ocr_model.encoder.encoder.layer.3.output.dense.weight', 'ocr_model.encoder.encoder.layer.3.output.dense.bias', 'ocr_model.encoder.encoder.layer.3.layernorm_before.weight', 'ocr_model.encoder.encoder.layer.3.layernorm_before.bias', 'ocr_model.encoder.encoder.layer.3.layernorm_after.weight', 'ocr_model.encoder.encoder.layer.3.layernorm_after.bias', 'ocr_model.encoder.encoder.layer.4.attention.attention.query.weight', 'ocr_model.encoder.encoder.layer.4.attention.attention.key.weight', 'ocr_model.encoder.encoder.layer.4.attention.attention.value.weight', 'ocr_model.encoder.encoder.layer.4.attention.output.dense.weight', 'ocr_model.encoder.encoder.layer.4.attention.output.dense.bias', 'ocr_model.encoder.encoder.layer.4.intermediate.dense.weight', 'ocr_model.encoder.encoder.layer.4.intermediate.dense.bias', 'ocr_model.encoder.encoder.layer.4.output.dense.weight', 'ocr_model.encoder.encoder.layer.4.output.dense.bias', 'ocr_model.encoder.encoder.layer.4.layernorm_before.weight', 'ocr_model.encoder.encoder.layer.4.layernorm_before.bias', 'ocr_model.encoder.encoder.layer.4.layernorm_after.weight', 'ocr_model.encoder.encoder.layer.4.layernorm_after.bias', 'ocr_model.encoder.encoder.layer.5.attention.attention.query.weight', 'ocr_model.encoder.encoder.layer.5.attention.attention.key.weight', 'ocr_model.encoder.encoder.layer.5.attention.attention.value.weight', 'ocr_model.encoder.encoder.layer.5.attention.output.dense.weight', 'ocr_model.encoder.encoder.layer.5.attention.output.dense.bias', 'ocr_model.encoder.encoder.layer.5.intermediate.dense.weight', 'ocr_model.encoder.encoder.layer.5.intermediate.dense.bias', 'ocr_model.encoder.encoder.layer.5.output.dense.weight', 'ocr_model.encoder.encoder.layer.5.output.dense.bias', 'ocr_model.encoder.encoder.layer.5.layernorm_before.weight', 'ocr_model.encoder.encoder.layer.5.layernorm_before.bias', 'ocr_model.encoder.encoder.layer.5.layernorm_after.weight', 'ocr_model.encoder.encoder.layer.5.layernorm_after.bias', 'ocr_model.encoder.encoder.layer.6.attention.attention.query.weight', 'ocr_model.encoder.encoder.layer.6.attention.attention.key.weight', 'ocr_model.encoder.encoder.layer.6.attention.attention.value.weight', 'ocr_model.encoder.encoder.layer.6.attention.output.dense.weight', 'ocr_model.encoder.encoder.layer.6.attention.output.dense.bias', 'ocr_model.encoder.encoder.layer.6.intermediate.dense.weight', 'ocr_model.encoder.encoder.layer.6.intermediate.dense.bias', 'ocr_model.encoder.encoder.layer.6.output.dense.weight', 'ocr_model.encoder.encoder.layer.6.output.dense.bias', 'ocr_model.encoder.encoder.layer.6.layernorm_before.weight', 'ocr_model.encoder.encoder.layer.6.layernorm_before.bias', 'ocr_model.encoder.encoder.layer.6.layernorm_after.weight', 'ocr_model.encoder.encoder.layer.6.layernorm_after.bias', 'ocr_model.encoder.encoder.layer.7.attention.attention.query.weight', 'ocr_model.encoder.encoder.layer.7.attention.attention.key.weight', 'ocr_model.encoder.encoder.layer.7.attention.attention.value.weight', 'ocr_model.encoder.encoder.layer.7.attention.output.dense.weight', 'ocr_model.encoder.encoder.layer.7.attention.output.dense.bias', 'ocr_model.encoder.encoder.layer.7.intermediate.dense.weight', 'ocr_model.encoder.encoder.layer.7.intermediate.dense.bias', 'ocr_model.encoder.encoder.layer.7.output.dense.weight', 'ocr_model.encoder.encoder.layer.7.output.dense.bias', 'ocr_model.encoder.encoder.layer.7.layernorm_before.weight', 'ocr_model.encoder.encoder.layer.7.layernorm_before.bias', 'ocr_model.encoder.encoder.layer.7.layernorm_after.weight', 'ocr_model.encoder.encoder.layer.7.layernorm_after.bias', 'ocr_model.encoder.encoder.layer.8.attention.attention.query.weight', 'ocr_model.encoder.encoder.layer.8.attention.attention.key.weight', 'ocr_model.encoder.encoder.layer.8.attention.attention.value.weight', 'ocr_model.encoder.encoder.layer.8.attention.output.dense.weight', 'ocr_model.encoder.encoder.layer.8.attention.output.dense.bias', 'ocr_model.encoder.encoder.layer.8.intermediate.dense.weight', 'ocr_model.encoder.encoder.layer.8.intermediate.dense.bias', 'ocr_model.encoder.encoder.layer.8.output.dense.weight', 'ocr_model.encoder.encoder.layer.8.output.dense.bias', 'ocr_model.encoder.encoder.layer.8.layernorm_before.weight', 'ocr_model.encoder.encoder.layer.8.layernorm_before.bias', 'ocr_model.encoder.encoder.layer.8.layernorm_after.weight', 'ocr_model.encoder.encoder.layer.8.layernorm_after.bias', 'ocr_model.encoder.encoder.layer.9.attention.attention.query.weight', 'ocr_model.encoder.encoder.layer.9.attention.attention.key.weight', 'ocr_model.encoder.encoder.layer.9.attention.attention.value.weight', 'ocr_model.encoder.encoder.layer.9.attention.output.dense.weight', 'ocr_model.encoder.encoder.layer.9.attention.output.dense.bias', 'ocr_model.encoder.encoder.layer.9.intermediate.dense.weight', 'ocr_model.encoder.encoder.layer.9.intermediate.dense.bias', 'ocr_model.encoder.encoder.layer.9.output.dense.weight', 'ocr_model.encoder.encoder.layer.9.output.dense.bias', 'ocr_model.encoder.encoder.layer.9.layernorm_before.weight', 'ocr_model.encoder.encoder.layer.9.layernorm_before.bias', 'ocr_model.encoder.encoder.layer.9.layernorm_after.weight', 'ocr_model.encoder.encoder.layer.9.layernorm_after.bias', 'ocr_model.encoder.encoder.layer.10.attention.attention.query.weight', 'ocr_model.encoder.encoder.layer.10.attention.attention.key.weight', 'ocr_model.encoder.encoder.layer.10.attention.attention.value.weight', 'ocr_model.encoder.encoder.layer.10.attention.output.dense.weight', 'ocr_model.encoder.encoder.layer.10.attention.output.dense.bias', 'ocr_model.encoder.encoder.layer.10.intermediate.dense.weight', 'ocr_model.encoder.encoder.layer.10.intermediate.dense.bias', 'ocr_model.encoder.encoder.layer.10.output.dense.weight', 'ocr_model.encoder.encoder.layer.10.output.dense.bias', 'ocr_model.encoder.encoder.layer.10.layernorm_before.weight', 'ocr_model.encoder.encoder.layer.10.layernorm_before.bias', 'ocr_model.encoder.encoder.layer.10.layernorm_after.weight', 'ocr_model.encoder.encoder.layer.10.layernorm_after.bias', 'ocr_model.encoder.encoder.layer.11.attention.attention.query.weight', 'ocr_model.encoder.encoder.layer.11.attention.attention.key.weight', 'ocr_model.encoder.encoder.layer.11.attention.attention.value.weight', 'ocr_model.encoder.encoder.layer.11.attention.output.dense.weight', 'ocr_model.encoder.encoder.layer.11.attention.output.dense.bias', 'ocr_model.encoder.encoder.layer.11.intermediate.dense.weight', 'ocr_model.encoder.encoder.layer.11.intermediate.dense.bias', 'ocr_model.encoder.encoder.layer.11.output.dense.weight', 'ocr_model.encoder.encoder.layer.11.output.dense.bias', 'ocr_model.encoder.encoder.layer.11.layernorm_before.weight', 'ocr_model.encoder.encoder.layer.11.layernorm_before.bias', 'ocr_model.encoder.encoder.layer.11.layernorm_after.weight', 'ocr_model.encoder.encoder.layer.11.layernorm_after.bias', 'ocr_model.encoder.layernorm.weight', 'ocr_model.encoder.layernorm.bias', 'ocr_model.encoder.pooler.dense.weight', 'ocr_model.encoder.pooler.dense.bias', 'ocr_model.decoder.model.decoder.embed_tokens.weight', 'ocr_model.decoder.model.decoder.embed_positions.weight', 'ocr_model.decoder.model.decoder.layernorm_embedding.weight', 'ocr_model.decoder.model.decoder.layernorm_embedding.bias', 'ocr_model.decoder.model.decoder.layers.0.self_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.0.self_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.0.self_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.0.self_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.0.self_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.0.self_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.0.self_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.0.self_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.0.self_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.0.self_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.0.encoder_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.0.encoder_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.0.encoder_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.0.encoder_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.0.encoder_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.0.encoder_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.0.encoder_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.0.encoder_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.0.encoder_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.0.encoder_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.0.fc1.weight', 'ocr_model.decoder.model.decoder.layers.0.fc1.bias', 'ocr_model.decoder.model.decoder.layers.0.fc2.weight', 'ocr_model.decoder.model.decoder.layers.0.fc2.bias', 'ocr_model.decoder.model.decoder.layers.0.final_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.0.final_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.1.self_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.1.self_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.1.self_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.1.self_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.1.self_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.1.self_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.1.self_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.1.self_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.1.self_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.1.self_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.1.encoder_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.1.encoder_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.1.encoder_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.1.encoder_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.1.encoder_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.1.encoder_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.1.encoder_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.1.encoder_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.1.encoder_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.1.encoder_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.1.fc1.weight', 'ocr_model.decoder.model.decoder.layers.1.fc1.bias', 'ocr_model.decoder.model.decoder.layers.1.fc2.weight', 'ocr_model.decoder.model.decoder.layers.1.fc2.bias', 'ocr_model.decoder.model.decoder.layers.1.final_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.1.final_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.2.self_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.2.self_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.2.self_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.2.self_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.2.self_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.2.self_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.2.self_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.2.self_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.2.self_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.2.self_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.2.encoder_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.2.encoder_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.2.encoder_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.2.encoder_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.2.encoder_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.2.encoder_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.2.encoder_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.2.encoder_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.2.encoder_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.2.encoder_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.2.fc1.weight', 'ocr_model.decoder.model.decoder.layers.2.fc1.bias', 'ocr_model.decoder.model.decoder.layers.2.fc2.weight', 'ocr_model.decoder.model.decoder.layers.2.fc2.bias', 'ocr_model.decoder.model.decoder.layers.2.final_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.2.final_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.3.self_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.3.self_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.3.self_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.3.self_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.3.self_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.3.self_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.3.self_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.3.self_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.3.self_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.3.self_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.3.encoder_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.3.encoder_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.3.encoder_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.3.encoder_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.3.encoder_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.3.encoder_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.3.encoder_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.3.encoder_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.3.encoder_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.3.encoder_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.3.fc1.weight', 'ocr_model.decoder.model.decoder.layers.3.fc1.bias', 'ocr_model.decoder.model.decoder.layers.3.fc2.weight', 'ocr_model.decoder.model.decoder.layers.3.fc2.bias', 'ocr_model.decoder.model.decoder.layers.3.final_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.3.final_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.4.self_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.4.self_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.4.self_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.4.self_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.4.self_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.4.self_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.4.self_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.4.self_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.4.self_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.4.self_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.4.encoder_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.4.encoder_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.4.encoder_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.4.encoder_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.4.encoder_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.4.encoder_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.4.encoder_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.4.encoder_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.4.encoder_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.4.encoder_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.4.fc1.weight', 'ocr_model.decoder.model.decoder.layers.4.fc1.bias', 'ocr_model.decoder.model.decoder.layers.4.fc2.weight', 'ocr_model.decoder.model.decoder.layers.4.fc2.bias', 'ocr_model.decoder.model.decoder.layers.4.final_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.4.final_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.5.self_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.5.self_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.5.self_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.5.self_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.5.self_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.5.self_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.5.self_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.5.self_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.5.self_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.5.self_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.5.encoder_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.5.encoder_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.5.encoder_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.5.encoder_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.5.encoder_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.5.encoder_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.5.encoder_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.5.encoder_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.5.encoder_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.5.encoder_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.5.fc1.weight', 'ocr_model.decoder.model.decoder.layers.5.fc1.bias', 'ocr_model.decoder.model.decoder.layers.5.fc2.weight', 'ocr_model.decoder.model.decoder.layers.5.fc2.bias', 'ocr_model.decoder.model.decoder.layers.5.final_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.5.final_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.6.self_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.6.self_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.6.self_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.6.self_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.6.self_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.6.self_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.6.self_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.6.self_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.6.self_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.6.self_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.6.encoder_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.6.encoder_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.6.encoder_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.6.encoder_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.6.encoder_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.6.encoder_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.6.encoder_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.6.encoder_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.6.encoder_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.6.encoder_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.6.fc1.weight', 'ocr_model.decoder.model.decoder.layers.6.fc1.bias', 'ocr_model.decoder.model.decoder.layers.6.fc2.weight', 'ocr_model.decoder.model.decoder.layers.6.fc2.bias', 'ocr_model.decoder.model.decoder.layers.6.final_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.6.final_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.7.self_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.7.self_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.7.self_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.7.self_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.7.self_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.7.self_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.7.self_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.7.self_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.7.self_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.7.self_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.7.encoder_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.7.encoder_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.7.encoder_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.7.encoder_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.7.encoder_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.7.encoder_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.7.encoder_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.7.encoder_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.7.encoder_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.7.encoder_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.7.fc1.weight', 'ocr_model.decoder.model.decoder.layers.7.fc1.bias', 'ocr_model.decoder.model.decoder.layers.7.fc2.weight', 'ocr_model.decoder.model.decoder.layers.7.fc2.bias', 'ocr_model.decoder.model.decoder.layers.7.final_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.7.final_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.8.self_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.8.self_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.8.self_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.8.self_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.8.self_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.8.self_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.8.self_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.8.self_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.8.self_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.8.self_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.8.encoder_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.8.encoder_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.8.encoder_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.8.encoder_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.8.encoder_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.8.encoder_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.8.encoder_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.8.encoder_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.8.encoder_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.8.encoder_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.8.fc1.weight', 'ocr_model.decoder.model.decoder.layers.8.fc1.bias', 'ocr_model.decoder.model.decoder.layers.8.fc2.weight', 'ocr_model.decoder.model.decoder.layers.8.fc2.bias', 'ocr_model.decoder.model.decoder.layers.8.final_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.8.final_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.9.self_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.9.self_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.9.self_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.9.self_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.9.self_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.9.self_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.9.self_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.9.self_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.9.self_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.9.self_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.9.encoder_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.9.encoder_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.9.encoder_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.9.encoder_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.9.encoder_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.9.encoder_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.9.encoder_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.9.encoder_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.9.encoder_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.9.encoder_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.9.fc1.weight', 'ocr_model.decoder.model.decoder.layers.9.fc1.bias', 'ocr_model.decoder.model.decoder.layers.9.fc2.weight', 'ocr_model.decoder.model.decoder.layers.9.fc2.bias', 'ocr_model.decoder.model.decoder.layers.9.final_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.9.final_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.10.self_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.10.self_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.10.self_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.10.self_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.10.self_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.10.self_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.10.self_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.10.self_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.10.self_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.10.self_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.10.encoder_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.10.encoder_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.10.encoder_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.10.encoder_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.10.encoder_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.10.encoder_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.10.encoder_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.10.encoder_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.10.encoder_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.10.encoder_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.10.fc1.weight', 'ocr_model.decoder.model.decoder.layers.10.fc1.bias', 'ocr_model.decoder.model.decoder.layers.10.fc2.weight', 'ocr_model.decoder.model.decoder.layers.10.fc2.bias', 'ocr_model.decoder.model.decoder.layers.10.final_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.10.final_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.11.self_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.11.self_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.11.self_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.11.self_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.11.self_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.11.self_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.11.self_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.11.self_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.11.self_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.11.self_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.11.encoder_attn.k_proj.weight', 'ocr_model.decoder.model.decoder.layers.11.encoder_attn.k_proj.bias', 'ocr_model.decoder.model.decoder.layers.11.encoder_attn.v_proj.weight', 'ocr_model.decoder.model.decoder.layers.11.encoder_attn.v_proj.bias', 'ocr_model.decoder.model.decoder.layers.11.encoder_attn.q_proj.weight', 'ocr_model.decoder.model.decoder.layers.11.encoder_attn.q_proj.bias', 'ocr_model.decoder.model.decoder.layers.11.encoder_attn.out_proj.weight', 'ocr_model.decoder.model.decoder.layers.11.encoder_attn.out_proj.bias', 'ocr_model.decoder.model.decoder.layers.11.encoder_attn_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.11.encoder_attn_layer_norm.bias', 'ocr_model.decoder.model.decoder.layers.11.fc1.weight', 'ocr_model.decoder.model.decoder.layers.11.fc1.bias', 'ocr_model.decoder.model.decoder.layers.11.fc2.weight', 'ocr_model.decoder.model.decoder.layers.11.fc2.bias', 'ocr_model.decoder.model.decoder.layers.11.final_layer_norm.weight', 'ocr_model.decoder.model.decoder.layers.11.final_layer_norm.bias', 'ocr_model.decoder.output_projection.weight', 'is_this_text_a_dialogue.layers.0.weight', 'is_this_text_a_dialogue.layers.0.bias', 'is_this_text_a_dialogue.layers.1.weight', 'is_this_text_a_dialogue.layers.1.bias', 'is_this_text_a_dialogue.layers.2.weight', 'is_this_text_a_dialogue.layers.2.bias', 'text_character_matching_head.layers.0.weight', 'text_character_matching_head.layers.0.bias', 'text_character_matching_head.layers.1.weight', 'text_character_matching_head.layers.1.bias', 'text_character_matching_head.layers.2.weight', 'text_character_matching_head.layers.2.bias'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyMagiModel(\n",
       "  (crop_embedding_model): ViTMAEModel(\n",
       "    (embeddings): ViTMAEEmbeddings(\n",
       "      (patch_embeddings): ViTMAEPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "    )\n",
       "    (encoder): ViTMAEEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTMAELayer(\n",
       "          (attention): ViTMAEAttention(\n",
       "            (attention): ViTMAESelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTMAESelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTMAEIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTMAEOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (detection_transformer): ConditionalDetrModel(\n",
       "    (backbone): ConditionalDetrConvModel(\n",
       "      (conv_encoder): ConditionalDetrConvEncoder(\n",
       "        (model): FeatureListNet(\n",
       "          (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "          (bn1): ConditionalDetrFrozenBatchNorm2d()\n",
       "          (act1): ReLU(inplace=True)\n",
       "          (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "          (layer1): Sequential(\n",
       "            (0): Bottleneck(\n",
       "              (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "              (downsample): Sequential(\n",
       "                (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): ConditionalDetrFrozenBatchNorm2d()\n",
       "              )\n",
       "            )\n",
       "            (1): Bottleneck(\n",
       "              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): Bottleneck(\n",
       "              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (layer2): Sequential(\n",
       "            (0): Bottleneck(\n",
       "              (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "              (bn2): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "              (downsample): Sequential(\n",
       "                (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                (1): ConditionalDetrFrozenBatchNorm2d()\n",
       "              )\n",
       "            )\n",
       "            (1): Bottleneck(\n",
       "              (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): Bottleneck(\n",
       "              (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (3): Bottleneck(\n",
       "              (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (layer3): Sequential(\n",
       "            (0): Bottleneck(\n",
       "              (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "              (bn2): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "              (downsample): Sequential(\n",
       "                (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                (1): ConditionalDetrFrozenBatchNorm2d()\n",
       "              )\n",
       "            )\n",
       "            (1): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (3): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (4): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (5): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (layer4): Sequential(\n",
       "            (0): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "              (bn2): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "              (downsample): Sequential(\n",
       "                (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                (1): ConditionalDetrFrozenBatchNorm2d()\n",
       "              )\n",
       "            )\n",
       "            (1): Bottleneck(\n",
       "              (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): Bottleneck(\n",
       "              (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): ConditionalDetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (position_embedding): ConditionalDetrSinePositionEmbedding()\n",
       "    )\n",
       "    (input_projection): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (query_position_embeddings): Embedding(305, 256)\n",
       "    (encoder): ConditionalDetrEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x ConditionalDetrEncoderLayer(\n",
       "          (self_attn): DetrAttention(\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): ReLU()\n",
       "          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): ConditionalDetrDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): ConditionalDetrDecoderLayer(\n",
       "          (sa_qcontent_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (sa_qpos_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (sa_kcontent_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (sa_kpos_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (sa_v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (self_attn): ConditionalDetrAttention(\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (ca_qcontent_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (ca_qpos_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (ca_kcontent_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (ca_kpos_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (ca_v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (ca_qpos_sine_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (encoder_attn): ConditionalDetrAttention(\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1-5): 5 x ConditionalDetrDecoderLayer(\n",
       "          (sa_qcontent_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (sa_qpos_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (sa_kcontent_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (sa_kpos_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (sa_v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (self_attn): ConditionalDetrAttention(\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (ca_qcontent_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (ca_qpos_proj): None\n",
       "          (ca_kcontent_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (ca_kpos_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (ca_v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (ca_qpos_sine_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (encoder_attn): ConditionalDetrAttention(\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (query_scale): MLP(\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ref_point_head): MLP(\n",
       "        (layers): ModuleList(\n",
       "          (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (1): Linear(in_features=256, out_features=2, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (bbox_predictor): ConditionalDetrMLPPredictionHead(\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "      (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (character_character_matching_head): ConditionalDetrMLPPredictionHead(\n",
       "    (layers): ModuleList(\n",
       "      (0): Linear(in_features=2304, out_features=256, bias=True)\n",
       "      (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (2): Linear(in_features=256, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (class_labels_classifier): Linear(in_features=256, out_features=3, bias=True)\n",
       "  (matcher): ConditionalDetrHungarianMatcher()\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cuda() # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "image_bboxes - list со вложенными lists, вложенный список - одна страница, где каждый элемент - np.array\n",
    "character_scores - list с тензорами, тензор - одна страница, каждый элемент - для каждого bbox score\n",
    "crop_embeddings_for_batch - list с тензорами, где каждый тензор - страница, а строка - для каждого bbox\n",
    "crop_bboxes - list c тензорами, где каждый тензор - страница, а строка - для каждого bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, asdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/userr/micromamba/envs/manga311/lib/python3.11/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at /opt/conda/conda-bld/pytorch_1712608853085/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    crop_bboxes, crop_embeddings_for_batch, image_bboxes, character_scores = model.get_crops_and_embeddings(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_scores[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(crop_embeddings_for_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(crop_embeddings_for_batch)):\n",
    "    if i == 0:\n",
    "        combined_tensor = crop_embeddings_for_batch[i]\n",
    "    else:\n",
    "        combined_tensor = torch.cat((combined_tensor, crop_embeddings_for_batch[i]), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_tensor.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 768])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_page = combined_tensor.cuda()\n",
    "first_page.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_tensor.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_tensor[7].unsqueeze(0).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_matrix_np = cos_matrix.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_page_np = first_page.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.9297, 0.9122, 0.4220, 0.3838, 0.4331, 0.5009, 0.7580, 0.6124,\n",
       "         0.4703, 0.7207, 0.7264, 0.5305, 0.7301, 0.3716, 0.5543],\n",
       "        [0.9297, 1.0000, 0.9484, 0.4216, 0.3583, 0.4454, 0.4737, 0.7653, 0.4817,\n",
       "         0.4450, 0.7854, 0.6862, 0.5430, 0.7090, 0.3610, 0.5662],\n",
       "        [0.9122, 0.9484, 1.0000, 0.4098, 0.3382, 0.4261, 0.4617, 0.7895, 0.4611,\n",
       "         0.4283, 0.7880, 0.7045, 0.5158, 0.7065, 0.3551, 0.5482],\n",
       "        [0.4220, 0.4216, 0.4098, 1.0000, 0.5134, 0.9528, 0.5269, 0.4975, 0.4503,\n",
       "         0.9116, 0.4256, 0.6700, 0.7774, 0.6608, 0.7918, 0.7940],\n",
       "        [0.3838, 0.3583, 0.3382, 0.5134, 1.0000, 0.5071, 0.9334, 0.5439, 0.5823,\n",
       "         0.6558, 0.5002, 0.6100, 0.7967, 0.5711, 0.5820, 0.7580],\n",
       "        [0.4331, 0.4454, 0.4261, 0.9528, 0.5071, 1.0000, 0.5278, 0.4523, 0.4083,\n",
       "         0.9298, 0.3881, 0.6247, 0.7664, 0.6204, 0.7541, 0.7821],\n",
       "        [0.5009, 0.4737, 0.4617, 0.5269, 0.9334, 0.5278, 1.0000, 0.6235, 0.7013,\n",
       "         0.6565, 0.6223, 0.7158, 0.7806, 0.7019, 0.5577, 0.7678],\n",
       "        [0.7580, 0.7653, 0.7895, 0.4975, 0.5439, 0.4523, 0.6235, 1.0000, 0.5875,\n",
       "         0.5131, 0.8968, 0.8279, 0.6669, 0.8067, 0.4856, 0.6770],\n",
       "        [0.6124, 0.4817, 0.4611, 0.4503, 0.5823, 0.4083, 0.7013, 0.5875, 1.0000,\n",
       "         0.4951, 0.5746, 0.6931, 0.5913, 0.6706, 0.4001, 0.6120],\n",
       "        [0.4703, 0.4450, 0.4283, 0.9116, 0.6558, 0.9298, 0.6565, 0.5131, 0.4951,\n",
       "         1.0000, 0.4169, 0.7078, 0.8422, 0.6909, 0.8391, 0.8468],\n",
       "        [0.7207, 0.7854, 0.7880, 0.4256, 0.5002, 0.3881, 0.6223, 0.8968, 0.5746,\n",
       "         0.4169, 1.0000, 0.7653, 0.6086, 0.7570, 0.3709, 0.6218],\n",
       "        [0.7264, 0.6862, 0.7045, 0.6700, 0.6100, 0.6247, 0.7158, 0.8279, 0.6931,\n",
       "         0.7078, 0.7653, 1.0000, 0.8065, 0.9257, 0.7007, 0.8317],\n",
       "        [0.5305, 0.5430, 0.5158, 0.7774, 0.7967, 0.7664, 0.7806, 0.6669, 0.5913,\n",
       "         0.8422, 0.6086, 0.8065, 1.0000, 0.7748, 0.8288, 0.9835],\n",
       "        [0.7301, 0.7090, 0.7065, 0.6608, 0.5711, 0.6204, 0.7019, 0.8067, 0.6706,\n",
       "         0.6909, 0.7570, 0.9257, 0.7748, 1.0000, 0.6600, 0.8060],\n",
       "        [0.3716, 0.3610, 0.3551, 0.7918, 0.5820, 0.7541, 0.5577, 0.4856, 0.4001,\n",
       "         0.8391, 0.3709, 0.7007, 0.8288, 0.6600, 1.0000, 0.8156],\n",
       "        [0.5543, 0.5662, 0.5482, 0.7940, 0.7580, 0.7821, 0.7678, 0.6770, 0.6120,\n",
       "         0.8468, 0.6218, 0.8317, 0.9835, 0.8060, 0.8156, 1.0000]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_matrix =  pairwise_cosine_similarity(first_page, first_page)\n",
    "cos_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Применяем DBSCAN с косинусной схожестью\n",
    "db = DBSCAN(metric='precomputed', eps=0.5, min_samples=5).fit(matrix)\n",
    "\n",
    "# Проверяем метки кластеров\n",
    "labels = db.labels_\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "\n",
    "print('Estimated number of clusters: %d' % n_clusters_)\n",
    "print('Estimated number of noise points: %d' % n_noise_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_character = torch.cat((combined_tensor[12],combined_tensor[7]), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_character = torch.cat((combined_tensor[0:3],combined_tensor[7].unsqueeze(0)), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_character_new = torch.cat((first_character,combined_tensor[12].unsqueeze(0)), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_character_new.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_character = torch.cat((combined_tensor[5].unsqueeze(0),combined_tensor[3].unsqueeze(0)), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_character.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcs_same_cha = pairwise_cosine_similarity(first_character_new, combined_tensor[4].unsqueeze(0))\n",
    "pcs_same_cha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcs_two_cha = pairwise_cosine_similarity(combined_tensor[4].unsqueeze(0), second_character)\n",
    "pcs_two_cha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_character_matching_threshold = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'character_character_matching_threshold' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Ассоциации между персонажами на основе заданного порога\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m char_i, char_j \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(cos_matrix \u001b[38;5;241m>\u001b[39m \u001b[43mcharacter_character_matching_threshold\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'character_character_matching_threshold' is not defined"
     ]
    }
   ],
   "source": [
    "# Ассоциации между персонажами на основе заданного порога\n",
    "char_i, char_j = torch.where(cos_matrix > character_character_matching_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_character_associations = torch.stack([char_i, char_j], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [0,0,0,1,-,1,-,-,0,-,1,0,1,1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "g = nx.Graph(character_character_associations.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = [list(c) for c in nx.connected_components(g)]\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(g.nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list = [0]*len(g.nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(C)):\n",
    "    for j in C[i]:\n",
    "        index_list[j] = i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw_spring(g, arrows=True, with_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_matrix = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_character_associations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_ufds = UnionFind.from_adj_matrix(\n",
    "    pcs\n",
    "    > character_character_matching_threshold\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_ufds.get_labels_for_connected_components()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_character_associations.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image_bboxes[0][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_embeddings_for_batch[0].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Список со всеми скорами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exl = []\n",
    "exl = np.array(exl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tensor in character_scores:\n",
    "    tensor_cpu = tensor.cpu()\n",
    "    arrays = tensor_cpu.numpy()\n",
    "    exl = np.concatenate((exl, arrays), axis=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Датакласс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CropImage:\n",
    "    image: np.array\n",
    "    bbox: torch.Tensor\n",
    "    embeddings: torch.Tensor\n",
    "    character_score: float\n",
    "    #page_number: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Character:\n",
    "    image_info: CropImage\n",
    "    character_index: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем пустой список для хранения экземпляров класса CropImage\n",
    "crop_images = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проходим по спискам данных и создаем экземпляры класса CropImage\n",
    "for i, image_bbox in enumerate(image_bboxes):\n",
    "    for j, bbox in enumerate(image_bbox):\n",
    "        # Извлекаем соответствующие данные\n",
    "        image = image_bbox[j]\n",
    "        character_score = character_scores[i][j]\n",
    "        embeddings = crop_embeddings_for_batch[i][j]\n",
    "        bbox_tensor = crop_bboxes[i][j]\n",
    "        \n",
    "        # Создаем экземпляр класса CropImage и добавляем его в список\n",
    "        crop_image = CropImage(image=image, bbox=bbox_tensor, embeddings=embeddings, character_score=character_score)\n",
    "        crop_images.append(crop_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(crop_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asdict(crop_images[0]).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = datasets.Features(\n",
    "    {\n",
    "        \"image\": datasets.Image(),\n",
    "        \"bbox\": datasets.Sequence(feature=datasets.Value(dtype=\"float32\")),\n",
    "        \"embeddings\": datasets.Sequence(feature=datasets.Value(dtype=\"float32\")),\n",
    "        \"character_score\": datasets.Value(dtype=\"float32\"),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = datasets.Dataset.from_list(list(map(asdict, crop_images)), features=schema)\n",
    "ds.add_faiss_index(\"embeddings\")\n",
    "# ds.set_format(\"numpy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries: NDArray[np.float64] = np.array(ds[\"embeddings\"])\n",
    "queries.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = ds.num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "np.linalg.norm(normalize(queries)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(queries[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, examples = ds.get_nearest_examples_batch(\"embeddings\", queries=np.array(ds[\"embeddings\"]), k=rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_distances = []\n",
    "\n",
    "for i in range(rows):\n",
    "    all_distances.append(examples[i][\"character_score\"])\n",
    "\n",
    "sc_array = np.array(all_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for score, sample, cha_score in zip(scores[0], examples[0][\"image\"], examples[0][\"character_score\"]):\n",
    "    print(f\"{score=}\")\n",
    "    print(f\"{cha_score=}\")\n",
    "    sample.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_array[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_matrix = np.array(scores)\n",
    "dist_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exl.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Симметричная матрица"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_distance = np.zeros((len(dist_matrix), len(dist_matrix)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dist_matrix)):\n",
    "    for j in range(len(dist_matrix)):\n",
    "        cell_ind = np.where(exl == sc_array[i][j])\n",
    "        cell_ind = cell_ind[0].item()\n",
    "        change_distance[i][cell_ind] = dist_matrix[i][j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_matrix.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaid = np.linalg.inv(diag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(torch.from_numpy(dist_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qw = scores[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zaq = scores[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = np.std(qw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(rows):\n",
    "    scores, examples = ds.get_nearest_examples(\"embeddings\", query=np.array(ds[i][\"embeddings\"]), k=rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_one, examples_one = ds.get_nearest_examples(\"embeddings\", query=np.array(ds[20][\"embeddings\"]), k=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(scores_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_score = scores/me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "me = max(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for score, el in zip(scores, examples[\"image\"]):\n",
    "    pp(score)\n",
    "    el.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp(character_scores[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp(crop_bboxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp(crop_bboxes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp(image_bboxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([  99.3922,  244.3012,  454.1992,  477.5109])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def array_to_image(array, tensor_key):\n",
    "    \"\"\"\n",
    "    Преобразует np.array в изображение и сохраняет его по указанному пути, используя имя тензора в качестве имени файла.\n",
    "    \n",
    "    :param array: np.array, представляющий изображение.\n",
    "    :param tensor_key: Тензор, используемый в качестве имени файла.\n",
    "    \"\"\"\n",
    "    # Убедитесь, что массив имеет правильную форму\n",
    "\n",
    "        \n",
    "    # Преобразуем массив в PIL Image\n",
    "    img = Image.fromarray(array)\n",
    "    \n",
    "    # Генерируем имя файла, используя имя тензора\n",
    "    filename = f\"crops_images/{tensor_key}.png\"\n",
    "    \n",
    "    # Сохраняем изображение\n",
    "    img.save(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(image_bboxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, dict_item in enumerate(image_bboxes):\n",
    "    # Извлекаем тензор и np.array из словаря\n",
    "    for tensor_key, array in dict_item.items():  # Используем.items() для перебора пар ключ-значение\n",
    "        # Преобразуем np.array в изображение и сохраняем его, используя имя тензора в качестве имени файла\n",
    "        array_to_image(array, tensor_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(image_bboxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(first_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp(image_bboxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp(crop_bboxes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp(character_scores[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp(crop_embeddings_for_batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_bboxes[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp(image_bboxes[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_directory = 'crops_images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, array in enumerate(image_bboxes):\n",
    "    # Сохранение изображения напрямую из массива\n",
    "    Image.fromarray(array).save(f'{save_directory}/image_{i}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp(character_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     results = model.predict_detections_and_associations(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox = BBox(\n",
    "    coords=results[0][\"characters\"][0],\n",
    "    score=results[0][\"character_scores\"][0],\n",
    "    char_id=results[0][\"character_cluster_labels\"][0],\n",
    ")\n",
    "bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page0 = Page(chars={0:[bbox1,bbox]}, shape=(1549, 1080), image=images[0])\n",
    "page0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp(results[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Строим картинки на основании выхлопа модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = time.localtime()\n",
    "now_str = f\"{now.tm_year}-{now.tm_mon:02}-{now.tm_mday:02}_{now.tm_hour:02}-{now.tm_min:02}-{now.tm_sec:02}\"\n",
    "out_dir = Path(f\"out/{now_str}\").resolve()\n",
    "out_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.visualise_single_image_prediction(chapter, results[0], filename=f\"{str(out_dir)}/chapter.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_bboxes_for_all_images = [x[\"texts\"] for x in results]\n",
    "\n",
    "# for i in range(len(images)):\n",
    "#     model.visualise_single_image_prediction(images[i], results[i], filename=f\"{str(out_dir)}/image_{i}.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
